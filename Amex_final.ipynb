{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeCYQc6n93b+ZRkMA1GHmG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMRNfRbGiMM2","executionInfo":{"status":"ok","timestamp":1751805519136,"user_tz":-330,"elapsed":3825,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"678c5910-ade0-4718-c9fb-90e743869431"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61WiUSL1iIEO","executionInfo":{"status":"ok","timestamp":1751798372382,"user_tz":-330,"elapsed":33144,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"38333154-1efc-466c-f767-9a9b4144fafc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading datasets...\n","All datasets loaded successfully (using Dask for large files).\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import dask.dataframe as dd\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","print(\"Loading datasets...\")\n","# --- 1. Load Datasets (Using Dask for Large Files) ---\n","# Load smaller files with pandas\n","stratified_train_df = pd.read_parquet('/content/drive/MyDrive/stratified_train_data.parquet')\n","test_df = pd.read_parquet('/content/drive/MyDrive/test_data.parquet')\n","offer_df = pd.read_parquet('/content/drive/MyDrive/offer_metadata.parquet')\n","\n","# Load large event and transaction logs with Dask to prevent memory crashes\n","try:\n","    event_df_dask = dd.read_parquet('/content/drive/MyDrive/add_event.parquet')\n","    trans_df_dask = dd.read_parquet('/content/drive/MyDrive/add_trans.parquet')\n","    print(\"All datasets loaded successfully (using Dask for large files).\")\n","except Exception as e:\n","    print(f\"Error loading supplementary data: {e}\")\n","    # Terminate if supplementary data isn't available\n","    exit()"]},{"cell_type":"code","source":["# --- 2. Feature Engineering from add_event_df (Offer-Level Only) ---\n","print(\"\\nStarting feature engineering on event data for OFFERS only...\")\n","\n","# Create a 'clicked' column based on whether id7 is null\n","event_df_dask['clicked'] = (~event_df_dask['id7'].isnull()).astype(int)\n","\n","# --- Offer-level event features ---\n","# Group by offer (id3) and aggregate its historical performance\n","offer_event_features_dask = event_df_dask.groupby('id3').agg(\n","    offer_total_impressions=('id4', 'count'),\n","    offer_total_clicks=('clicked', 'sum')\n",")\n","# Calculate offer's historical CTR\n","offer_event_features_dask['offer_historical_ctr'] = (\n","    offer_event_features_dask['offer_total_clicks'] / offer_event_features_dask['offer_total_impressions']\n",").fillna(0)\n","\n","# --- Execute Dask computation ---\n","print(\"Computing aggregated offer features...\")\n","offer_event_features = offer_event_features_dask.compute().reset_index()\n","print(\"Offer event features created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VnQTDoyigRW","executionInfo":{"status":"ok","timestamp":1751801929574,"user_tz":-330,"elapsed":8394,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"c9bea562-fa96-4466-a42b-cab2213ac79c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting feature engineering on event data for OFFERS only...\n","Computing aggregated offer features...\n","Offer event features created.\n"]}]},{"cell_type":"code","source":["# --- 3. NEW: Feature Engineering from add_trans_df (Industry-Level) ---\n","print(\"\\nStarting feature engineering on transaction data for INDUSTRIES...\")\n","trans_df_dask['f367'] = dd.to_numeric(trans_df_dask['f367'], errors='coerce')\n","\n","# Group by industry (id8) and aggregate transaction behavior\n","industry_trans_features_dask = trans_df_dask.groupby('id8').agg(\n","    industry_avg_spend=('f367', 'mean'),\n","    industry_total_transactions=('f367', 'count'),\n","    industry_unique_products=('f368', dd.Aggregation('nunique', chunk=lambda s: s.nunique(), agg=lambda s: s.nunique()))\n",")\n","print(\"Computing aggregated industry features...\")\n","industry_trans_features = industry_trans_features_dask.compute().reset_index()\n","print(\"Industry transaction features created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOXfYh68jWaD","executionInfo":{"status":"ok","timestamp":1751803894269,"user_tz":-330,"elapsed":4126,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"13b9d651-6119-4cc9-9ecc-568bd1b8cd81"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting feature engineering on transaction data for INDUSTRIES...\n","Computing aggregated industry features...\n","Industry transaction features created.\n"]}]},{"cell_type":"code","source":["# --- 4. Feature Engineering from offer_metadata_df (Pandas) ---\n","print(\"\\nStarting feature engineering on offer metadata...\")\n","offer_df['id12'] = pd.to_datetime(offer_df['id12'], errors='coerce')\n","offer_df['id13'] = pd.to_datetime(offer_df['id13'], errors='coerce')\n","offer_df['offer_duration_days'] = (offer_df['id13'] - offer_df['id12']).dt.days\n","\n","offer_meta_features = offer_df[['id3', 'f375', 'f376', 'id10', 'id8', 'offer_duration_days']].rename(columns={\n","    'f375': 'offer_redemption_freq',\n","    'f376': 'offer_discount_rate',\n","    'id10': 'offer_type_code' # Renamed to avoid confusion with industry\n","})\n","# Ensure the merge keys are the correct type\n","offer_meta_features['id3'] = offer_meta_features['id3'].astype(str)\n","offer_meta_features['id8'] = offer_meta_features['id8'].astype(str)\n","industry_trans_features['id8'] = industry_trans_features['id8'].astype(str)\n","\n","# --- NEW: Merge industry features into offer metadata ---\n","offer_meta_features = pd.merge(offer_meta_features, industry_trans_features, on='id8', how='left')\n","print(\"Offer metadata enriched with industry transaction data.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXG4b1VaiMuD","executionInfo":{"status":"ok","timestamp":1751803911137,"user_tz":-330,"elapsed":88,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"556bd7e3-f5fd-4f2e-b376-684512a3dd91"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting feature engineering on offer metadata...\n","Offer metadata enriched with industry transaction data.\n"]}]},{"cell_type":"code","source":["# --- 5. Merge All New Features into Main DataFrames ---\n","print(\"\\nMerging all new features into training and test sets...\")\n","def enrich_dataframe(df):\n","    \"\"\"Merges all engineered features into a given dataframe.\"\"\"\n","    df['id3'] = df['id3'].astype(str)\n","\n","    # Now merging small, pre-computed pandas DataFrames\n","    df = pd.merge(df, offer_event_features, on='id3', how='left')\n","    df = pd.merge(df, offer_meta_features, on='id3', how='left')\n","\n","    return df\n","\n","train_enriched = enrich_dataframe(stratified_train_df)\n","test_enriched = enrich_dataframe(test_df)\n","\n","print(f\"Enriched training data shape: {train_enriched.shape}\")\n","print(f\"Enriched test data shape: {test_enriched.shape}\")\n","\n","\n","# --- 6. Save the Enriched Datasets ---\n","print(\"\\nSaving enriched datasets to Parquet files...\")\n","try:\n","    train_enriched.to_parquet('/content/drive/MyDrive/train_enriched.parquet')\n","    test_enriched.to_parquet('/content/drive/MyDrive/test_enriched.parquet')\n","    print(\"Successfully saved enriched data.\")\n","except Exception as e:\n","    print(f\"Error saving enriched data: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqdPB2j8pmsM","executionInfo":{"status":"ok","timestamp":1751803998996,"user_tz":-330,"elapsed":63239,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"0a0808ae-ad46-4eb3-fb2d-c09b76984887"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Merging all new features into training and test sets...\n","Enriched training data shape: (110362, 383)\n","Enriched test data shape: (369301, 382)\n","\n","Saving enriched datasets to Parquet files...\n","Successfully saved enriched data.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import warnings\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Enriched Training Data ---\n","try:\n","    df_train = pd.read_parquet('/content/drive/MyDrive/train_enriched.parquet')\n","    print(\"Successfully loaded train_enriched.parquet.\")\n","    print(f\"Original training data shape: {df_train.shape}\")\n","except FileNotFoundError:\n","    print(\"Error: train_enriched.parquet not found.\")\n","    print(\"Please ensure you have run the feature engineering script successfully.\")\n","    exit()\n","\n","# --- 2. Define Initial Feature Set ---\n","# Identify all potential feature columns\n","feature_cols = [col for col in df_train.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n","\n","# Convert all feature columns to numeric to allow for calculations\n","for col in feature_cols:\n","    if col in df_train.columns:\n","        df_train[col] = pd.to_numeric(df_train[col], errors='coerce')\n","\n","# --- 3. Preprocessing on Training Data ---\n","\n","# --- Step 3a: Drop Columns with High Missing Values ---\n","print(\"\\n--- Preprocessing Step 1: Dropping Sparse Columns from Training Data ---\")\n","missing_threshold = 0.95 # Drop columns with > 95% missing values\n","missing_fractions = df_train[feature_cols].isnull().mean()\n","cols_to_drop_missing = missing_fractions[missing_fractions > missing_threshold].index.tolist()\n","df_train.drop(columns=cols_to_drop_missing, inplace=True)\n","# Update our list of feature columns\n","feature_cols = [col for col in feature_cols if col not in cols_to_drop_missing]\n","print(f\"Dropped {len(cols_to_drop_missing)} columns with more than {missing_threshold*100}% missing values.\")\n","\n","# --- Step 3b: Drop Highly Correlated Features ---\n","print(\"\\n--- Preprocessing Step 2: Dropping Correlated Features from Training Data ---\")\n","# First, fill NaNs to calculate correlation. We'll use the median.\n","df_for_corr = df_train[feature_cols].fillna(df_train[feature_cols].median())\n","corr_matrix = df_for_corr.corr().abs()\n","upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","corr_threshold = 0.95 # Drop one feature from any pair with correlation > 0.95\n","cols_to_drop_corr = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n","df_train.drop(columns=cols_to_drop_corr, inplace=True)\n","# Update our list of feature columns one last time\n","feature_cols = [col for col in feature_cols if col not in cols_to_drop_corr]\n","print(f\"Dropped {len(cols_to_drop_corr)} highly correlated features.\")\n","del df_for_corr, corr_matrix, upper # Clean up memory\n","gc.collect()\n","\n","# --- 4. Save the Refined Training Data ---\n","# The list of columns to keep is implicitly defined by the columns remaining in df_train\n","final_columns_to_keep = df_train.columns.tolist()\n","print(f\"\\nTotal features remaining after preprocessing: {len(feature_cols)}\")\n","\n","try:\n","    df_train.to_parquet('/content/drive/MyDrive/train_enriched_refined.parquet')\n","    print(\"Successfully saved train_enriched_refined.parquet.\")\n","except Exception as e:\n","    print(f\"Error saving refined training data: {e}\")\n","\n","\n","# --- 5. Apply the SAME Preprocessing to the Test Data ---\n","print(\"\\n--- Applying Preprocessing to Test Data ---\")\n","try:\n","    df_test = pd.read_parquet('/content/drive/MyDrive/test_enriched.parquet')\n","    print(\"Successfully loaded test_enriched.parquet.\")\n","    print(f\"Original test data shape: {df_test.shape}\")\n","except FileNotFoundError:\n","    print(\"Error: test_enriched.parquet not found.\")\n","    exit()\n","\n","# The list of columns to keep is defined by the refined training set.\n","# This ensures perfect consistency between train and test.\n","# We need to handle the case where a dummy column might not appear in the test set.\n","final_columns_in_test = [col for col in final_columns_to_keep if col in df_test.columns]\n","df_test_refined = df_test[final_columns_in_test]\n","\n","# Add any columns that were in the refined training set but not the test set\n","# (e.g., a rare category that only appeared in training) and fill with 0.\n","for col in final_columns_to_keep:\n","    if col not in df_test_refined.columns:\n","        df_test_refined[col] = 0\n","\n","# Ensure the column order is identical\n","df_test_refined = df_test_refined[final_columns_to_keep]\n","\n","print(f\"Refined test data shape: {df_test_refined.shape}\")\n","\n","\n","# --- 6. Save the Refined Test Data ---\n","try:\n","    df_test_refined.to_parquet('/content/drive/MyDrive/test_enriched_refined.parquet')\n","    print(\"Successfully saved test_enriched_refined.parquet.\")\n","except Exception as e:\n","    print(f\"Error saving refined test data: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1meqImcXtNbW","executionInfo":{"status":"ok","timestamp":1751805009104,"user_tz":-330,"elapsed":105256,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"294d2646-2692-4f67-e711-445257273810"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train_enriched.parquet.\n","Original training data shape: (110362, 383)\n","\n","--- Preprocessing Step 1: Dropping Sparse Columns from Training Data ---\n","Dropped 41 columns with more than 95.0% missing values.\n","\n","--- Preprocessing Step 2: Dropping Correlated Features from Training Data ---\n","Dropped 23 highly correlated features.\n","\n","Total features remaining after preprocessing: 312\n","Successfully saved train_enriched_refined.parquet.\n","\n","--- Applying Preprocessing to Test Data ---\n","Successfully loaded test_enriched.parquet.\n","Original test data shape: (369301, 382)\n","Refined test data shape: (369301, 319)\n","Successfully saved test_enriched_refined.parquet.\n"]}]},{"cell_type":"code","source":["# prompt: import this and list the columns present in this /content/drive/MyDrive/test_enriched_refined.parquet\n","\n","test_refined_df = pd.read_parquet('/content/drive/MyDrive/test_enriched_refined.parquet')\n","print(\"Columns in /content/drive/MyDrive/test_enriched_refined.parquet:\")\n","print(test_refined_df.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldITkQjuzVAm","executionInfo":{"status":"ok","timestamp":1751806530389,"user_tz":-330,"elapsed":14922,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"649f6dc3-2d54-482c-9f18-ca8143829527"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in /content/drive/MyDrive/test_enriched_refined.parquet:\n","['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f35', 'f38', 'f39', 'f40', 'f41', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f51', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f65', 'f68', 'f69', 'f71', 'f72', 'f73', 'f74', 'f75', 'f77', 'f78', 'f79', 'f81', 'f82', 'f83', 'f85', 'f86', 'f87', 'f89', 'f90', 'f91', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f121', 'f123', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f150', 'f151', 'f152', 'f153', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f203', 'f204', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f332', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f355', 'f356', 'f357', 'f359', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'offer_total_impressions', 'offer_total_clicks', 'offer_historical_ctr', 'offer_redemption_freq', 'offer_discount_rate', 'offer_type_code', 'id8', 'industry_avg_spend', 'industry_total_transactions', 'industry_unique_products']\n"]}]},{"cell_type":"code","source":["# prompt: print nunique values in id8 column in test_enriched_refined\n","\n","print(f\"Number of unique values in 'id8' in test_enriched_refined: {test_refined_df['id8'].nunique()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5JUBli4zzVh","executionInfo":{"status":"ok","timestamp":1751806616357,"user_tz":-330,"elapsed":18,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"33a19aba-fb12-4375-d350-b1cb2c8eea46"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique values in 'id8' in test_enriched_refined: 183\n"]}]},{"cell_type":"code","source":["# prompt: print all the columns inside train_enriched_refined\n","\n","print(df_test_refined.columns.tolist())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"ddV8nJSAuCRI","executionInfo":{"status":"error","timestamp":1751806444958,"user_tz":-330,"elapsed":375,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"9d3ce120-deaa-4560-b731-4efffd4b91eb"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_test_refined' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3-775878349.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: print all the columns inside train_enriched_refined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test_refined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df_test_refined' is not defined"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n","import warnings\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Refined Training Data ---\n","try:\n","    df = pd.read_parquet('/content/drive/MyDrive/train_enriched_refined.parquet')\n","    print(\"Successfully loaded train_enriched_refined.parquet.\")\n","    print(f\"Refined training data shape: {df.shape}\")\n","except FileNotFoundError:\n","    print(\"Error: train_enriched_refined.parquet not found.\")\n","    print(\"Please ensure you have run the preprocessing script successfully.\")\n","    exit()\n","\n","\n","# --- 2. Final Data Preparation ---\n","\n","# Identify the target column\n","target_col = 'y'\n","\n","# Identify all feature columns (all columns except identifiers and the target)\n","feature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n","\n","# Handle the 'offer_type_code' categorical feature using one-hot encoding\n","# This is a safe way to handle it, even though LightGBM can use categoricals directly.\n","if 'offer_type_code' in df.columns:\n","    df = pd.get_dummies(df, columns=['offer_type_code'], dummy_na=True, prefix='offer_type')\n","    # Update feature_cols to include the new dummy columns\n","    if 'offer_type_code' in feature_cols:\n","        feature_cols.remove('offer_type_code')\n","    dummy_cols = [col for col in df.columns if col.startswith('offer_type_')]\n","    feature_cols.extend(dummy_cols)\n","\n","# Final check and imputation for any remaining missing values\n","# Filling with a value like -999 allows the model to treat \"missing\" as a special category.\n","df[feature_cols] = df[feature_cols].fillna(-999)\n","\n","# Define features (X), target (y), and groups for cross-validation\n","X = df[feature_cols]\n","y = df[target_col].astype(int)\n","groups = df['id2']\n","\n","print(f\"\\nPrepared data for training with {len(feature_cols)} features.\")\n","\n","\n","# --- 3. Training with GroupKFold Cross-Validation ---\n","\n","# Define LightGBM parameters\n","# These are robust starting parameters that balance speed and accuracy.\n","params = {\n","    'objective': 'binary',\n","    'metric': 'auc',\n","    'boosting_type': 'gbdt',\n","    'n_estimators': 2000,\n","    'learning_rate': 0.02,\n","    'num_leaves': 40,\n","    'max_depth': -1,\n","    'seed': 42,\n","    'n_jobs': -1,\n","    'verbose': -1,\n","    'colsample_bytree': 0.7,\n","    'subsample': 0.7,\n","    'reg_alpha': 0.1,\n","    'reg_lambda': 0.1,\n","}\n","\n","# Set up GroupKFold\n","N_SPLITS = 5\n","gkf = GroupKFold(n_splits=N_SPLITS)\n","\n","# Initialize arrays to store predictions and feature importances\n","oof_predictions = np.zeros(len(df))\n","feature_importances = pd.DataFrame(index=feature_cols)\n","\n","print(f\"\\nStarting training with {N_SPLITS}-Fold GroupKFold Cross-Validation...\")\n","\n","for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n","    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n","\n","    # Split data into training and validation sets for this fold\n","    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n","    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    # Define and train the model\n","    model = lgb.LGBMClassifier(**params)\n","    model.fit(X_train, y_train,\n","              eval_set=[(X_val, y_val)],\n","              eval_metric='auc',\n","              callbacks=[lgb.early_stopping(100, verbose=False)])\n","\n","    # Make predictions on the validation set\n","    val_preds = model.predict_proba(X_val)[:, 1]\n","\n","    # Store the out-of-fold (OOF) predictions\n","    oof_predictions[val_idx] = val_preds\n","\n","    # Store feature importances and save the model for this fold\n","    feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n","    model_path = f'/content/drive/MyDrive/lgbm_model_fold_{fold+1}.txt'\n","    model.booster_.save_model(model_path)\n","    print(f\"Model for fold {fold+1} saved to {model_path}\")\n","\n","    # Clean up memory\n","    del X_train, y_train, X_val, y_val, model\n","    gc.collect()\n","\n","# --- 4. Evaluate Overall Performance ---\n","overall_auc = roc_auc_score(y, oof_predictions)\n","\n","# --- NEW: Generate Classification Metrics ---\n","# Convert OOF probabilities to binary predictions using a 0.5 threshold\n","oof_binary_preds = (oof_predictions > 0.5).astype(int)\n","overall_accuracy = accuracy_score(y, oof_binary_preds)\n","\n","print(f\"\\n--- Overall Cross-Validation Results ---\")\n","print(f\"Overall Out-of-Fold (OOF) AUC Score: {overall_auc:.5f}\")\n","print(f\"Overall Out-of-Fold (OOF) Accuracy: {overall_accuracy:.5f}\")\n","print(\"\\nOverall Out-of-Fold (OOF) Classification Report:\")\n","print(classification_report(y, oof_binary_preds))\n","\n","\n","# Display top 20 most important features\n","feature_importances['mean'] = feature_importances.mean(axis=1)\n","print(\"\\nTop 20 Most Important Features (averaged across folds):\")\n","print(feature_importances.sort_values('mean', ascending=False).head(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_60lk5Tt32C","executionInfo":{"status":"ok","timestamp":1751805887508,"user_tz":-330,"elapsed":346441,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"1d5906a4-57c5-4952-f79b-cecd059b1fa5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train_enriched_refined.parquet.\n","Refined training data shape: (110362, 319)\n","\n","Prepared data for training with 314 features.\n","\n","Starting training with 5-Fold GroupKFold Cross-Validation...\n","--- Fold 1/5 ---\n","Model for fold 1 saved to /content/drive/MyDrive/lgbm_model_fold_1.txt\n","--- Fold 2/5 ---\n","Model for fold 2 saved to /content/drive/MyDrive/lgbm_model_fold_2.txt\n","--- Fold 3/5 ---\n","Model for fold 3 saved to /content/drive/MyDrive/lgbm_model_fold_3.txt\n","--- Fold 4/5 ---\n","Model for fold 4 saved to /content/drive/MyDrive/lgbm_model_fold_4.txt\n","--- Fold 5/5 ---\n","Model for fold 5 saved to /content/drive/MyDrive/lgbm_model_fold_5.txt\n","\n","--- Overall Cross-Validation Results ---\n","Overall Out-of-Fold (OOF) AUC Score: 0.91820\n","Overall Out-of-Fold (OOF) Accuracy: 0.86947\n","\n","Overall Out-of-Fold (OOF) Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.95      0.91     73311\n","           1       0.87      0.71      0.79     37051\n","\n","    accuracy                           0.87    110362\n","   macro avg       0.87      0.83      0.85    110362\n","weighted avg       0.87      0.87      0.87    110362\n","\n","\n","Top 20 Most Important Features (averaged across folds):\n","                      fold_1  fold_2  fold_3  fold_4  fold_5   mean\n","offer_historical_ctr     710     687     570     672     572  642.2\n","f366                     498     485     438     507     450  475.6\n","f132                     347     317     313     316     277  314.0\n","f206                     348     292     227     325     256  289.6\n","f350                     319     174     153     316     172  226.8\n","f223                     263     210     204     230     185  218.4\n","f363                     219     216     181     237     207  212.0\n","f204                     128     249     157     237     179  190.0\n","f210                     232     172     150     178     158  178.0\n","f95                      168     157     133     160     189  161.4\n","f68                      171     241      98     187      73  154.0\n","f203                     267     130      85     177     106  153.0\n","f207                     169     159     152     140     111  146.2\n","f98                      173     216     104     162      62  143.4\n","offer_total_clicks       156     149     128     154     123  142.0\n","f123                     193     124     114     125     151  141.4\n","f30                      163     171      98     139     132  140.6\n","f130                     166     151     102     129     127  135.0\n","f147                     133     139     139     128     121  132.0\n","f365                     159     106     110     146     134  131.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","import warnings\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Refined Test Data ---\n","try:\n","    df_test = pd.read_parquet('/content/drive/MyDrive/test_enriched_refined.parquet')\n","    print(\"Successfully loaded test_enriched_refined.parquet.\")\n","    print(f\"Refined test data shape: {df_test.shape}\")\n","except FileNotFoundError:\n","    print(\"Error: test_enriched_refined.parquet not found.\")\n","    print(\"Please ensure you have run the preprocessing script successfully.\")\n","    exit()\n","\n","# --- 2. Final Data Preparation for Test Set ---\n","\n","# Keep a copy of the identifier columns for the final submission file\n","submission_ids = df_test[['id1', 'id2', 'id3', 'id5']].copy()\n","\n","# Identify all feature columns (all columns except identifiers and the target)\n","# Note: 'y' is the target, and 'id8' is an identifier we've already used for feature engineering.\n","feature_cols = [col for col in df_test.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n","\n","# Handle the 'offer_type_code' categorical feature using one-hot encoding\n","if 'offer_type_code' in df_test.columns:\n","    df_test = pd.get_dummies(df_test, columns=['offer_type_code'], dummy_na=True, prefix='offer_type')\n","    # Update feature_cols to include the new dummy columns\n","    if 'offer_type_code' in feature_cols:\n","        feature_cols.remove('offer_type_code')\n","    dummy_cols = [col for col in df_test.columns if col.startswith('offer_type_')]\n","    feature_cols.extend(dummy_cols)\n","\n","# MODIFIED: Explicitly convert all feature columns to numeric types.\n","# This is the key fix to prevent the ValueError.\n","print(\"\\nConverting all feature columns to numeric types...\")\n","for col in feature_cols:\n","    if col in df_test.columns:\n","        df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n","\n","# Final check and imputation for any remaining missing values\n","df_test[feature_cols] = df_test[feature_cols].fillna(-999)\n","\n","# Define the final feature set for prediction\n","X_test = df_test[feature_cols]\n","\n","print(f\"\\nPrepared test data for prediction with {len(feature_cols)} features.\")\n","\n","\n","# --- 3. Load Models and Generate Predictions ---\n","\n","N_SPLITS = 5\n","test_predictions = np.zeros(len(df_test))\n","\n","print(f\"\\nLoading {N_SPLITS} models and generating predictions...\")\n","\n","for fold in range(1, N_SPLITS + 1):\n","    print(f\"--- Predicting with Fold {fold}/{N_SPLITS} ---\")\n","    try:\n","        # Load the model saved from the training script\n","        model_path = f'/content/drive/MyDrive/lgbm_model_fold_{fold}.txt'\n","        model = lgb.Booster(model_file=model_path)\n","\n","        # Predict probabilities on the test set\n","        fold_preds = model.predict(X_test)\n","\n","        # Add the predictions for this fold to our total\n","        # We divide by N_SPLITS here to average them as we go\n","        test_predictions += fold_preds / N_SPLITS\n","\n","    except lgb.basic.LightGBMError as e:\n","        print(f\"Error loading model for fold {fold}: {e}\")\n","        print(\"Please ensure all model files were saved correctly during training.\")\n","        continue\n","\n","print(\"All predictions generated and averaged.\")\n","\n","\n","# --- 4. Create and Save the Submission File ---\n","print(\"\\nCreating the final submission file...\")\n","\n","# Create the submission DataFrame\n","submission_df = submission_ids.copy()\n","\n","# Format the 'id5' date column to mm-dd-yyyy as requested\n","submission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\n","\n","# Add the final averaged predictions\n","# Clean predictions to ensure no NaNs and format to prevent scientific notation\n","cleaned_predictions = np.nan_to_num(test_predictions, nan=0.0)\n","submission_df['pred'] = [f\"{p:.10f}\" for p in cleaned_predictions]\n","\n","\n","print(\"\\nFirst 5 rows of the final submission file:\")\n","print(submission_df.head())\n","\n","# Define the output path\n","submission_path = '/content/drive/MyDrive/r2_submission_best_lightgbm.csv'\n","\n","try:\n","    submission_df.to_csv(submission_path, index=False)\n","    print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n","except Exception as e:\n","    print(f\"\\nError saving submission file: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biYpsozI0QYw","executionInfo":{"status":"ok","timestamp":1751807002396,"user_tz":-330,"elapsed":112427,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"774db51c-3a48-4a73-a08c-068ef433fcc8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded test_enriched_refined.parquet.\n","Refined test data shape: (369301, 319)\n","\n","Converting all feature columns to numeric types...\n","\n","Prepared test data for prediction with 314 features.\n","\n","Loading 5 models and generating predictions...\n","--- Predicting with Fold 1/5 ---\n","--- Predicting with Fold 2/5 ---\n","--- Predicting with Fold 3/5 ---\n","--- Predicting with Fold 4/5 ---\n","--- Predicting with Fold 5/5 ---\n","All predictions generated and averaged.\n","\n","Creating the final submission file...\n","\n","First 5 rows of the final submission file:\n","                                               id1      id2     id3  \\\n","0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n","1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n","2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n","3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n","4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n","\n","          id5          pred  \n","0  11-04-2023  0.0420275449  \n","1  11-04-2023  0.1522975077  \n","2  11-05-2023  0.9808929177  \n","3  11-04-2023  0.0497704573  \n","4  11-05-2023  0.0507627029  \n","\n","Submission file successfully saved to: /content/drive/MyDrive/r2_submission_best_lightgbm.csv\n"]}]},{"cell_type":"code","source":["# prompt: now i want to remove all the features/columns from train_enriched and test_enriched whose missing value is greater than 80% in the training dataset. also print their names. don't remove these columns\n","# - customer_total_impressions\n","# - customer_total_clicks\n","# - customer_unique_offers_seen\n","# - customer_historical_ctr\n","# - customer_total_spend\n","# - customer_avg_spend\n","# - customer_transaction_count\n","# - customer_unique_products_purchased\n","# again reload the dataset from drive and do this operation\n","\n","# Reload enriched data\n","train_enriched = pd.read_parquet('/content/drive/MyDrive/train_enriched.parquet')\n","test_enriched = pd.read_parquet('/content/drive/MyDrive/test_enriched.parquet')\n","\n","# Define columns to keep\n","columns_to_keep = [\n","    'customer_total_impressions',\n","    'customer_total_clicks',\n","    'customer_unique_offers_seen',\n","    'customer_historical_ctr',\n","    'customer_total_spend',\n","    'customer_avg_spend',\n","    'customer_transaction_count',\n","    'customer_unique_products_purchased',\n","    'id2', # Assuming 'id2' is the customer ID and should be kept\n","    'id3', # Assuming 'id3' is the offer ID and should be kept\n","    'f377', # Assuming this is the target variable based on previous context\n","]\n","\n","# Calculate missing value percentage for each column in the training data\n","missing_percentage = train_enriched.isnull().sum() / len(train_enriched) * 100\n","\n","# Identify columns with missing percentage > 80%\n","cols_to_drop = missing_percentage[missing_percentage > 85].index.tolist()\n","\n","# Filter out columns from the drop list that are in columns_to_keep\n","cols_to_drop = [col for col in cols_to_drop if col not in columns_to_keep]\n","\n","# Print the names of columns to be removed\n","print(\"Columns to be removed due to >80% missing values in training data:\")\n","for col in cols_to_drop:\n","    print(col)\n","\n","# Remove the identified columns from both training and test datasets\n","train_enriched = train_enriched.drop(columns=cols_to_drop)\n","test_enriched = test_enriched.drop(columns=cols_to_drop)\n","\n","print(f\"\\nTrain enriched shape after removing columns: {train_enriched.shape}\")\n","print(f\"Test enriched shape after removing columns: {test_enriched.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_6jiMhKlwKW","executionInfo":{"status":"ok","timestamp":1751719493718,"user_tz":-330,"elapsed":26393,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"2ad2eb70-0943-4635-f82d-2a9e670f6ab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns to be removed due to >80% missing values in training data:\n","f3\n","f4\n","f13\n","f14\n","f15\n","f16\n","f17\n","f18\n","f19\n","f20\n","f21\n","f33\n","f34\n","f36\n","f37\n","f64\n","f66\n","f70\n","f79\n","f80\n","f81\n","f84\n","f88\n","f92\n","f112\n","f114\n","f117\n","f118\n","f120\n","f121\n","f122\n","f135\n","f136\n","f154\n","f176\n","f189\n","f205\n","f220\n","f221\n","f360\n","\n","Train enriched shape after removing columns: (110362, 347)\n","Test enriched shape after removing columns: (369301, 346)\n"]}]},{"cell_type":"code","source":["train_enriched.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VY0uTMhfog0c","executionInfo":{"status":"ok","timestamp":1751719751143,"user_tz":-330,"elapsed":10,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"8cdcc027-36c0-41a3-bfd9-2324ee985809"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(110362, 347)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","df = train_enriched\n","\n","# --- 2. Data Preparation and Memory Reduction ---\n","\n","# Identify the target column\n","target_col = 'y'\n","\n","# Identify all feature columns: original 'f' columns + new engineered columns\n","# Exclude identifier columns and the target variable\n","original_f_cols = [col for col in df.columns if col.startswith('f')]\n","new_feature_cols = [\n","    'customer_total_impressions', 'customer_total_clicks', 'customer_unique_offers_seen',\n","    'customer_historical_ctr', 'offer_total_impressions', 'offer_total_clicks',\n","    'offer_historical_ctr', 'customer_total_spend', 'customer_avg_spend',\n","    'customer_transaction_count', 'customer_unique_products_purchased',\n","    'offer_redemption_freq', 'offer_discount_rate', 'offer_industry_code',\n","    'offer_duration_days'\n","]\n","feature_cols = original_f_cols + new_feature_cols\n","\n","# Handle categorical features (e.g., offer_industry_code) using one-hot encoding\n","# This converts categorical columns into separate binary (0/1) columns\n","df = pd.get_dummies(df, columns=['offer_industry_code'], dummy_na=True)\n","\n","# Update feature_cols to include the new dummy columns and remove the original\n","feature_cols.remove('offer_industry_code')\n","dummy_cols = [col for col in df.columns if col.startswith('offer_industry_code_')]\n","feature_cols.extend(dummy_cols)\n","\n","# Convert all feature columns to numeric, coercing errors\n","for col in feature_cols:\n","    if col in df.columns:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","# Simple imputation: fill any remaining missing values with 0\n","# This is a safe baseline. More advanced imputation could be explored later.\n","df[feature_cols] = df[feature_cols].fillna(0)\n","\n","# Memory Reduction Step\n","print(f\"\\nOriginal memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","for col in df.select_dtypes(include=['float64']).columns:\n","    df[col] = df[col].astype(np.float32)\n","for col in df.select_dtypes(include=['int64']).columns:\n","    df[col] = df[col].astype(np.int32)\n","print(f\"Reduced memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","\n","# Define features (X) and target (y)\n","X = df[feature_cols]\n","y = df[target_col].astype(int)\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","print(f\"\\nTraining set size: {X_train.shape}\")\n","print(f\"Validation set size: {X_val.shape}\")\n","\n","\n","# --- 3. Hyperparameter Tuning with RandomizedSearchCV ---\n","\n","# Calculate scale_pos_weight for XGBoost\n","scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n","print(f\"\\nCalculated scale_pos_weight: {scale_pos_weight:.2f}\")\n","\n","# Define the parameter grid\n","param_grid = {\n","    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","    'n_estimators': [200, 300, 500],\n","    'max_depth': [5, 7, 9, 11],\n","    'subsample': [0.7, 0.8, 0.9],\n","    'colsample_bytree': [0.7, 0.8, 0.9],\n","    'gamma': [0, 0.1, 0.2],\n","    'min_child_weight': [1, 3, 5]\n","}\n","\n","# Instantiate the XGBoost classifier\n","xgb_clf = xgb.XGBClassifier(\n","    objective='binary:logistic',\n","    eval_metric='logloss',\n","    scale_pos_weight=scale_pos_weight,\n","    use_label_encoder=False,\n","    random_state=42\n",")\n","\n","# Set up RandomizedSearchCV with memory-safe parameters\n","# MODIFIED: cv is now 3 as requested.\n","random_search = RandomizedSearchCV(\n","    estimator=xgb_clf,\n","    param_distributions=param_grid,\n","    n_iter=15,\n","    scoring='roc_auc',\n","    n_jobs=1,  # Use 1 core to be memory-safe\n","    cv=3,      # 3-fold cross-validation\n","    verbose=2,\n","    random_state=42\n",")\n","\n","print(\"\\nStarting RandomizedSearch on enriched data...\")\n","random_search.fit(X_train, y_train)\n","\n","print(\"\\nSearch complete.\")\n","print(f\"Best ROC AUC score from search: {random_search.best_score_:.4f}\")\n","print(\"Best parameters found:\")\n","print(random_search.best_params_)\n","\n","\n","# --- 4. Evaluate the Best Model on the Validation Set ---\n","\n","best_clf = random_search.best_estimator_\n","y_pred_proba = best_clf.predict_proba(X_val)[:, 1]\n","y_pred = best_clf.predict(X_val)\n","\n","print(\"\\n--- Validation Set Performance on Enriched Data ---\")\n","print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n","print(f\"ROC AUC Score: {roc_auc_score(y_val, y_pred_proba):.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_val, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwRW-2UIoiwk","executionInfo":{"status":"ok","timestamp":1751724889662,"user_tz":-330,"elapsed":4694189,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"f33fa0c7-db56-4b96-8b5e-9cae21dcdf05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Original memory usage: 360.75 MB\n","Reduced memory usage: 182.88 MB\n","\n","Training set size: (88289, 343)\n","Validation set size: (22073, 343)\n","\n","Calculated scale_pos_weight: 1.98\n","\n","Starting RandomizedSearch on enriched data...\n","Fitting 3 folds for each of 15 candidates, totalling 45 fits\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 2.0min\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 2.0min\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 2.0min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.2min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.3min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.4min\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.8min\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.7min\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.8min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=500, subsample=0.8; total time= 2.3min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=500, subsample=0.8; total time= 2.3min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=11, min_child_weight=5, n_estimators=500, subsample=0.8; total time= 2.3min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.1min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.0min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.1min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_depth=11, min_child_weight=5, n_estimators=200, subsample=0.8; total time= 1.6min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_depth=11, min_child_weight=5, n_estimators=200, subsample=0.8; total time= 1.6min\n","[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_depth=11, min_child_weight=5, n_estimators=200, subsample=0.8; total time= 1.6min\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=300, subsample=0.9; total time= 2.8min\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=300, subsample=0.9; total time= 2.8min\n","[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=300, subsample=0.9; total time= 2.7min\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=200, subsample=0.7; total time=  29.2s\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=200, subsample=0.7; total time=  29.8s\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=200, subsample=0.7; total time=  27.7s\n","[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time=  34.7s\n","[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time=  32.7s\n","[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time=  34.6s\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.8; total time= 2.3min\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.8; total time= 2.3min\n","[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.8; total time= 2.2min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.0min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time= 1.1min\n","[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_depth=7, min_child_weight=5, n_estimators=300, subsample=0.9; total time=  58.9s\n","[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.2min\n","[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.2min\n","[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.6min\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.7min\n","[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=500, subsample=0.7; total time= 2.6min\n","[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=1, n_estimators=200, subsample=0.9; total time= 1.9min\n","[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=1, n_estimators=200, subsample=0.9; total time= 2.0min\n","[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=11, min_child_weight=1, n_estimators=200, subsample=0.9; total time= 1.9min\n","\n","Search complete.\n","Best ROC AUC score from search: 0.9544\n","Best parameters found:\n","{'subsample': 0.7, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.9}\n","\n","--- Validation Set Performance on Enriched Data ---\n","Accuracy: 0.9071\n","ROC AUC Score: 0.9575\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.94      0.93     14663\n","           1       0.87      0.85      0.86      7410\n","\n","    accuracy                           0.91     22073\n","   macro avg       0.90      0.89      0.90     22073\n","weighted avg       0.91      0.91      0.91     22073\n","\n"]}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/xgb_best_model.json'\n","print(f\"\\nSaving the best model to: {model_path}\")\n","try:\n","    best_clf.save_model(model_path)\n","    print(\"Model saved successfully.\")\n","except Exception as e:\n","    print(f\"Error saving model: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UJyCW0d-Kz-","executionInfo":{"status":"ok","timestamp":1751725423241,"user_tz":-330,"elapsed":243,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"4920f2a1-3eed-4220-c415-4078d9de15ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Saving the best model to: /content/drive/MyDrive/xgb_best_model.json\n","Model saved successfully.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","import dask.dataframe as dd\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Saved Model ---\n","print(\"Loading the saved XGBoost model...\")\n","model_path = '/content/drive/MyDrive/xgb_best_model.json'\n","try:\n","    # Instantiate a new classifier and load the saved model into it\n","    best_clf = xgb.XGBClassifier()\n","    best_clf.load_model(model_path)\n","    print(\"Model loaded successfully.\")\n","except Exception as e:\n","    print(f\"\\nError loading model: {e}\")\n","    print(\"Please ensure the model was saved correctly from the training script.\")\n","    exit()\n","\n","\n","# --- 2. Load and Prepare Test Data with Dask ---\n","print(\"\\nLoading and preparing test data using Dask...\")\n","try:\n","    # Use Dask to read the large enriched test file lazily\n","    test_dd = dd.read_parquet('/content/drive/MyDrive/test_enriched.parquet')\n","    print(\"Successfully loaded test_enriched.parquet as a Dask DataFrame.\")\n","except FileNotFoundError:\n","    print(\"Error: test_enriched.parquet not found.\")\n","    exit()\n","\n","# Keep a copy of the identifier columns for the final submission file\n","# Reading just a few columns with pandas is memory-safe\n","submission_ids = pd.read_parquet('/content/drive/MyDrive/test_enriched.parquet', columns=['id1', 'id2', 'id3', 'id5'])\n","\n","# --- Apply the same column dropping logic as training data ---\n","cols_to_drop = [\n","    'f3', 'f4', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21',\n","    'f33', 'f34', 'f36', 'f37', 'f64', 'f66', 'f70', 'f79', 'f80', 'f81', 'f84',\n","    'f88', 'f92', 'f112', 'f114', 'f117', 'f118', 'f120', 'f121', 'f122',\n","    'f135', 'f136', 'f154', 'f176', 'f189', 'f205', 'f220', 'f221', 'f360'\n","]\n","\n","# Drop the identified columns from the test Dask DataFrame\n","print(f\"Dropping {len(cols_to_drop)} columns from test data...\")\n","# Ensure columns exist before dropping\n","cols_to_drop_exist = [col for col in cols_to_drop if col in test_dd.columns]\n","test_dd = test_dd.drop(columns=cols_to_drop_exist)\n","print(f\"Test data shape after dropping columns: ({test_dd.shape[0].compute()}, {test_dd.shape[1]})\")\n","\n","\n","# --- Replicate Preprocessing Steps using Dask ---\n","# MODIFIED: Explicitly convert the column to 'category' dtype before one-hot encoding.\n","test_dd['offer_industry_code'] = test_dd['offer_industry_code'].astype('category')\n","\n","# Add this step to make categories known to Dask and align with training categories\n","# We need the categories from the training data to ensure consistency\n","try:\n","    train_enriched_for_col_check = pd.read_parquet('/content/drive/MyDrive/train_enriched.parquet', columns=['offer_industry_code'])\n","    train_categories = train_enriched_for_col_check['offer_industry_code'].astype('category').cat.categories\n","    # Use set_categories with rename=False to ensure all training categories are present\n","    test_dd['offer_industry_code'] = test_dd['offer_industry_code'].cat.set_categories(train_categories, rename=False).cat.as_known()\n","    print(\"Offer industry code categories aligned.\")\n","except FileNotFoundError:\n","     print(\"Error: train_enriched.parquet not found. Cannot align offer industry code categories.\")\n","     # If training data is not found, proceed without aligning categories, but this might lead to errors if test data has unseen categories.\n","     test_dd['offer_industry_code'] = test_dd['offer_industry_code'].cat.as_known() # Still make known whatever categories are present in test data.\n","\n","\n","# Handle categorical features using Dask's get_dummies\n","test_dd = dd.get_dummies(test_dd, columns=['offer_industry_code'], dummy_na=True)\n","\n","\n","# Align columns with the training set\n","# Get training columns from the loaded model's feature names\n","try:\n","    training_columns = best_clf.get_booster().feature_names\n","    print(f\"Expected training columns from model: {len(training_columns)}\")\n","except Exception as e:\n","    print(f\"Error getting feature names from model: {e}\")\n","    # Fallback: try to load training data and process to get column names\n","    try:\n","        train_processed_for_cols = pd.read_parquet('/content/drive/MyDrive/train_enriched.parquet')\n","        train_processed_for_cols = train_processed_for_cols.drop(columns=cols_to_drop_exist)\n","        train_processed_for_cols['offer_industry_code'] = train_processed_for_cols['offer_industry_code'].astype('category').cat.as_known()\n","        train_processed_for_cols = pd.get_dummies(train_processed_for_cols, columns=['offer_industry_code'], dummy_na=True)\n","        training_columns = train_processed_for_cols.columns.tolist()\n","        if 'f377' in training_columns:\n","            training_columns.remove('f377')\n","        print(f\"Expected training columns from processing training data: {len(training_columns)}\")\n","    except FileNotFoundError:\n","        print(\"Error: train_enriched.parquet not found. Cannot determine training columns.\")\n","        exit()\n","\n","\n","# Align columns: Add missing training columns to test_dd with fill_value=0\n","# and then select only the training columns in the correct order.\n","test_columns = test_dd.columns\n","missing_cols = set(training_columns) - set(test_columns)\n","\n","# Create dummy columns for missing ones in test_dd\n","for col in missing_cols:\n","    test_dd[col] = 0\n","\n","# Select columns in the training order\n","test_dd = test_dd[training_columns]\n","\n","# Simple imputation: fill any remaining missing values with 0\n","test_dd = test_dd.fillna(0)\n","\n","print(\"Test data preprocessing plan created.\")\n","\n","\n","# --- 3. Generate Predictions in Chunks ---\n","print(\"\\nGenerating predictions on the test data (in chunks)...\")\n","# Dask will automatically handle predicting on the data in chunks\n","# to avoid loading it all into memory.\n","test_predictions_proba_dask = test_dd.map_partitions(\n","    best_clf.predict_proba,\n","    meta=pd.DataFrame(columns=[0, 1], dtype=np.float32)\n",")\n","\n","# We only need the probability of the positive class (1)\n","positive_class_proba_dask = test_predictions_proba_dask[1]\n","\n","# Execute the computation to get the predictions as a pandas Series\n","print(\"Computing predictions...\")\n","test_predictions_proba = positive_class_proba_dask.compute()\n","print(\"Predictions generated successfully.\")\n","\n","\n","# --- 4. Create and Save the Submission File ---\n","print(\"\\nCreating the final submission file...\")\n","\n","# Create the submission DataFrame\n","submission_df = submission_ids.copy()\n","\n","# Format the 'id5' date column to mmddyy\n","submission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m%d%y')\n","\n","# Add the predictions\n","submission_df['pred'] = test_predictions_proba\n","\n","print(\"\\nFirst 5 rows of the final submission file:\")\n","print(submission_df.head())\n","\n","# Define the output path\n","submission_path = '/content/drive/MyDrive/r2_submission_file_myteam111.csv'\n","\n","try:\n","    submission_df.to_csv(submission_path, index=False)\n","    print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n","except Exception as e:\n","    print(f\"\\nError saving submission file: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":842},"id":"91wcXzD4qPOo","executionInfo":{"status":"error","timestamp":1751729265868,"user_tz":-330,"elapsed":19335,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"c1aa7ae7-c4ab-4076-ecc6-d0cbdb0a052b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the saved XGBoost model...\n","Model loaded successfully.\n","\n","Loading and preparing test data using Dask...\n","Successfully loaded test_enriched.parquet as a Dask DataFrame.\n","Dropping 40 columns from test data...\n","Test data shape after dropping columns: (369301, 346)\n","Offer industry code categories aligned.\n","Expected training columns from model: 343\n","Test data preprocessing plan created.\n","\n","Generating predictions on the test data (in chunks)...\n","Computing predictions...\n"]},{"output_type":"error","ename":"TypeError","evalue":"Invalid value '0' for dtype string","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pa_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36m_box_pa\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/string_arrow.py\u001b[0m in \u001b[0;36m_box_pa_scalar\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mpa_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_scalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpa_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36m_box_pa_scalar\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mpa_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpa_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/scalar.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.scalar\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mArrowTypeError\u001b[0m: Expected bytes, got a 'int' object","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m                 new_values = self.values.fillna(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid value '{str(value)}' for dtype {self.dtype}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Invalid value '0' for dtype string","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pa_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36m_box_pa\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/string_arrow.py\u001b[0m in \u001b[0;36m_box_pa_scalar\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mpa_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_pa_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_scalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpa_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36m_box_pa_scalar\u001b[0;34m(cls, value, pa_type)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mpa_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpa_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/scalar.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.scalar\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mArrowTypeError\u001b[0m: Expected bytes, got a 'int' object","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-12-1068963264.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Execute the computation to get the predictions as a pandas Series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computing predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mtest_predictions_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_class_proba_dask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictions generated successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, fuse, concatenate, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDaskMethodsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mshorten_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/array.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid value '{str(value)}' for dtype {self.dtype}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Invalid value '0' for dtype string"]}]},{"cell_type":"code","source":["# prompt: find number of missing values column wise in /content/drive/MyDrive/r2_submission_file_myteam11.csv after imprting it from drive\n","import pandas as pd\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/r2_submission_file Team Pioneers.csv')\n","\n","# Calculate the number of missing values per column\n","# missing_values_count = df.isnull().sum()\n","\n","# Print the column-wise missing value count\n","# print(\"Number of missing values per column:\")\n","# missing_values_count"],"metadata":{"id":"KMHpCn2YBqV3","executionInfo":{"status":"ok","timestamp":1751740023712,"user_tz":-330,"elapsed":3082,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6cc9ac7-c3b8-4531-d98f-fcf3ca3cd027"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-20-1960715525.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('/content/drive/MyDrive/r2_submission_file Team Pioneers.csv')\n"]}]},{"cell_type":"code","source":["# prompt: print the data type of first 5 columns in df\n","\n","print(\"Data types of the first 5 columns:\")\n","print(df.iloc[:, :5].dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhHdqQrouTtn","executionInfo":{"status":"ok","timestamp":1751740055697,"user_tz":-330,"elapsed":58,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"8a2735a7-52da-4b7f-f4e0-093aa565260b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data types of the first 5 columns:\n","id1     object\n","id2     object\n","id3      int64\n","id5     object\n","pred    object\n","dtype: object\n"]}]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04PX8rNVpjTn","executionInfo":{"status":"ok","timestamp":1751740075147,"user_tz":-330,"elapsed":54,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"b32bb7bf-5958-42ce-9690-fcccc88658d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 369301 entries, 0 to 369300\n","Data columns (total 5 columns):\n"," #   Column  Non-Null Count   Dtype \n","---  ------  --------------   ----- \n"," 0   id1     369301 non-null  object\n"," 1   id2     369301 non-null  object\n"," 2   id3     369301 non-null  int64 \n"," 3   id5     369301 non-null  object\n"," 4   pred    369301 non-null  object\n","dtypes: int64(1), object(4)\n","memory usage: 14.1+ MB\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Path to the file you want to check\n","submission_path = '/content/drive/MyDrive/r2_submission_file Team Pioneers.csv'\n","\n","print(f\"Checking file: {submission_path}\")\n","\n","try:\n","    # Load the submission file\n","    df = pd.read_csv(submission_path)\n","\n","    # Check the data type of the 'pred' column as pandas reads it\n","    print(f\"\\nPandas inferred dtype for 'pred' column: {df['pred'].dtype}\")\n","\n","    problem_found = False\n","    # Iterate through each value and try to convert it to a float\n","    for index, value in df['pred'].items():\n","        try:\n","            # This is the core check\n","            float(value)\n","        except (ValueError, TypeError):\n","            print(f\"--> Problem found at row index {index}: value='{value}'\")\n","            problem_found = True\n","\n","    if not problem_found:\n","        print(\"\\nSuccess! All values in the 'pred' column can be read as numbers.\")\n","        print(\"The issue is likely with the submission platform's parser.\")\n","\n","except FileNotFoundError:\n","    print(f\"\\nError: Could not find the file at {submission_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7y3U1YBopzGu","executionInfo":{"status":"ok","timestamp":1751740109385,"user_tz":-330,"elapsed":2504,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"5b524bee-368f-4b6d-fe7a-909d5e62fb30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking file: /content/drive/MyDrive/r2_submission_file Team Pioneers.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-23-2560950774.py:10: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(submission_path)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Pandas inferred dtype for 'pred' column: object\n","--> Problem found at row index 41806: value='0.011-04-202335'\n","--> Problem found at row index 70886: value='0.11-04-202304'\n","--> Problem found at row index 90939: value='0.011-04-202371'\n","--> Problem found at row index 105756: value='0.00811-04-2023'\n","--> Problem found at row index 187554: value='0.11-05-202305'\n"]}]},{"cell_type":"code","source":["print(df['pred'].min())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbBgdtwdqnIo","executionInfo":{"status":"ok","timestamp":1751737118279,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"9cd26675-7588-4f0b-d571-f1dba9e0b199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.00022452489\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import GroupKFold, RandomizedSearchCV, train_test_split\n","from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n","import warnings\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Refined Training Data ---\n","try:\n","    df = pd.read_parquet('/content/drive/MyDrive/train_enriched_refined.parquet')\n","    print(\"Successfully loaded train_enriched_refined.parquet.\")\n","    print(f\"Refined training data shape: {df.shape}\")\n","except FileNotFoundError:\n","    print(\"Error: train_enriched_refined.parquet not found.\")\n","    print(\"Please ensure you have run the preprocessing script successfully.\")\n","    exit()\n","\n","\n","# --- 2. Final Data Preparation ---\n","\n","# Identify the target column\n","target_col = 'y'\n","\n","# Identify all feature columns (all columns except identifiers and the target)\n","feature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n","\n","# Handle the 'offer_type_code' categorical feature using one-hot encoding\n","if 'offer_type_code' in df.columns:\n","    df = pd.get_dummies(df, columns=['offer_type_code'], dummy_na=True, prefix='offer_type')\n","    # Update feature_cols to include the new dummy columns\n","    if 'offer_type_code' in feature_cols:\n","        feature_cols.remove('offer_type_code')\n","    dummy_cols = [col for col in df.columns if col.startswith('offer_type_')]\n","    feature_cols.extend(dummy_cols)\n","\n","# Final check and imputation for any remaining missing values\n","df[feature_cols] = df[feature_cols].fillna(-999)\n","\n","# Define features (X), target (y), and groups\n","X = df[feature_cols]\n","y = df[target_col].astype(int)\n","groups = df['id2']\n","\n","print(f\"\\nPrepared data for training with {len(feature_cols)} features.\")\n","\n","\n","# --- 3. Hyperparameter Tuning with RandomizedSearchCV and GroupKFold ---\n","\n","# Define XGBoost parameters\n","# We don't need n_estimators here as it will be part of the search grid.\n","# We also don't need scale_pos_weight yet, as it's better to calculate it on the specific training fold.\n","params = {\n","    'objective': 'binary:logistic',\n","    'eval_metric': 'auc',\n","    'use_label_encoder': False,\n","    'seed': 42,\n","    'n_jobs': -1,\n","}\n","\n","# Define the parameter grid for the search\n","param_grid = {\n","    'learning_rate': [0.02, 0.05, 0.1],\n","    'n_estimators': [500, 1000, 2000],\n","    'max_depth': [5, 7, 9],\n","    'subsample': [0.7, 0.8],\n","    'colsample_bytree': [0.7, 0.8],\n","    'min_child_weight': [1, 3, 5]\n","}\n","\n","# Instantiate the XGBoost classifier\n","xgb_clf = xgb.XGBClassifier(**params)\n","\n","# Set up GroupKFold as the cross-validation strategy\n","N_SPLITS = 5\n","gkf = GroupKFold(n_splits=N_SPLITS)\n","\n","# Set up RandomizedSearchCV\n","# We pass the GroupKFold object to the 'cv' parameter.\n","random_search = RandomizedSearchCV(\n","    estimator=xgb_clf,\n","    param_distributions=param_grid,\n","    n_iter=10, # Number of parameter settings that are sampled.\n","    scoring='roc_auc',\n","    n_jobs=1,  # Use 1 core to be memory-safe\n","    cv=gkf,    # CRITICAL: Use GroupKFold for cross-validation\n","    verbose=2,\n","    random_state=42\n",")\n","\n","print(f\"\\nStarting RandomizedSearch with {N_SPLITS}-Fold GroupKFold...\")\n","# The .fit() method now requires the 'groups' parameter to pass to GroupKFold\n","random_search.fit(X, y, groups=groups)\n","\n","print(\"\\nSearch complete.\")\n","print(f\"Best cross-validated ROC AUC score from search: {random_search.best_score_:.5f}\")\n","print(\"Best parameters found:\")\n","print(random_search.best_params_)\n","\n","# --- 4. Save the Single Best Model ---\n","best_clf = random_search.best_estimator_\n","model_path = '/content/drive/MyDrive/xgb_best_model_from_search.json'\n","print(f\"\\nSaving the best model to: {model_path}\")\n","best_clf.save_model(model_path)\n","print(\"Model saved successfully.\")\n","\n","\n","# --- 5. Final Evaluation on a Hold-Out Set ---\n","# It's good practice to have a final, unseen validation set to confirm performance.\n","# We'll do a simple train/val split here for a final check.\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","best_clf.fit(X_train, y_train) # Refit on this training portion\n","y_pred_proba = best_clf.predict_proba(X_val)[:, 1]\n","y_pred = best_clf.predict(X_val)\n","\n","print(\"\\n--- Final Performance on a Hold-Out Validation Set ---\")\n","print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n","print(f\"ROC AUC Score: {roc_auc_score(y_val, y_pred_proba):.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_val, y_pred))"],"metadata":{"id":"CbFIGjKDqyx9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751818175593,"user_tz":-330,"elapsed":9125058,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"b6f05a7b-5da7-47a3-c9ae-808a3c8a12f9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train_enriched_refined.parquet.\n","Refined training data shape: (110362, 319)\n","\n","Prepared data for training with 314 features.\n","\n","Starting RandomizedSearch with 5-Fold GroupKFold...\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.8min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, subsample=0.7; total time= 1.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=2000, subsample=0.8; total time= 6.0min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=2000, subsample=0.8; total time= 6.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=2000, subsample=0.8; total time= 6.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=2000, subsample=0.8; total time= 5.9min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=2000, subsample=0.8; total time= 6.0min\n","[CV] END colsample_bytree=0.7, learning_rate=0.02, max_depth=5, min_child_weight=3, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.02, max_depth=5, min_child_weight=3, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.02, max_depth=5, min_child_weight=3, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.02, max_depth=5, min_child_weight=3, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.7, learning_rate=0.02, max_depth=5, min_child_weight=3, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.9min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.9min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.9min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.9min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.9min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.7min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.8min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.7min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.7min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 1.7min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=1000, subsample=0.8; total time= 2.1min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=1000, subsample=0.8; total time= 2.2min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=1000, subsample=0.8; total time= 2.2min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=5, n_estimators=2000, subsample=0.7; total time= 6.0min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=5, n_estimators=2000, subsample=0.7; total time= 6.1min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=5, n_estimators=2000, subsample=0.7; total time= 6.0min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=5, n_estimators=2000, subsample=0.7; total time= 6.2min\n","[CV] END colsample_bytree=0.8, learning_rate=0.02, max_depth=7, min_child_weight=5, n_estimators=2000, subsample=0.7; total time= 6.0min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=1000, subsample=0.7; total time= 4.3min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=1000, subsample=0.7; total time= 4.3min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=1000, subsample=0.7; total time= 4.2min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=1000, subsample=0.7; total time= 4.3min\n","[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, min_child_weight=5, n_estimators=1000, subsample=0.7; total time= 4.2min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 2.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 2.7min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 2.6min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 2.7min\n","[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, subsample=0.7; total time= 2.7min\n","\n","Search complete.\n","Best cross-validated ROC AUC score from search: 0.91841\n","Best parameters found:\n","{'subsample': 0.7, 'n_estimators': 500, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n","\n","Saving the best model to: /content/drive/MyDrive/xgb_best_model_from_search.json\n","Model saved successfully.\n","\n","--- Final Performance on a Hold-Out Validation Set ---\n","Accuracy: 0.9076\n","ROC AUC Score: 0.9543\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.96      0.93     14663\n","           1       0.91      0.80      0.85      7410\n","\n","    accuracy                           0.91     22073\n","   macro avg       0.91      0.88      0.89     22073\n","weighted avg       0.91      0.91      0.91     22073\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","import dask.dataframe as dd\n","import warnings\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","# --- 1. Load the Saved Best Model ---\n","print(\"Loading the saved best XGBoost model...\")\n","model_path = '/content/drive/MyDrive/xgb_best_model_from_search.json'\n","try:\n","    # Instantiate a new classifier and load the saved model into it\n","    best_clf = xgb.XGBClassifier()\n","    best_clf.load_model(model_path)\n","    print(\"Model loaded successfully.\")\n","except Exception as e:\n","    print(f\"\\nError loading model: {e}\")\n","    print(\"Please ensure the model was saved correctly from the tuning script.\")\n","    exit()\n","\n","\n","# --- 2. Load Test Data Identifiers and Set Up for Prediction ---\n","print(\"\\nLoading test data identifiers...\")\n","try:\n","    # We only need the ID columns for the final submission file.\n","    # The features will be loaded partition by partition.\n","    submission_ids = pd.read_parquet(\n","        '/content/drive/MyDrive/test_enriched_refined.parquet',\n","        columns=['id1', 'id2', 'id3', 'id5']\n","    )\n","    # Use Dask to create a reference to the test data for iteration\n","    test_dd = dd.read_parquet('/content/drive/MyDrive/test_enriched_refined.parquet')\n","    print(\"Successfully loaded test data identifiers and created Dask DataFrame.\")\n","except FileNotFoundError:\n","    print(\"Error: test_enriched_refined.parquet not found.\")\n","    print(\"Please ensure you have run the preprocessing script successfully.\")\n","    exit()\n","\n","# Get the list of feature columns the model was trained on\n","training_columns = best_clf.get_booster().feature_names\n","\n","\n","# --- 3. Generate Predictions by Iterating Through Partitions (Memory-Safe) ---\n","print(\"\\nGenerating predictions by processing data in chunks...\")\n","all_predictions = []\n","\n","# Iterate over each partition of the Dask DataFrame\n","for i, partition in enumerate(test_dd.partitions):\n","    print(f\"Processing partition {i+1}/{test_dd.npartitions}...\")\n","    # Compute the current partition to get a small pandas DataFrame\n","    partition_pd = partition.compute()\n","\n","    # --- Apply the EXACT SAME preprocessing steps as the training script ---\n","    # a. Handle categorical features\n","    if 'offer_type_code' in partition_pd.columns:\n","        partition_pd = pd.get_dummies(partition_pd, columns=['offer_type_code'], dummy_na=True, prefix='offer_type')\n","\n","    # b. Align columns with the training set\n","    partition_pd = partition_pd.reindex(columns=training_columns, fill_value=0)\n","\n","    # c. Explicitly convert all feature columns to numeric types\n","    for col in partition_pd.columns:\n","        if col in training_columns:\n","            partition_pd[col] = pd.to_numeric(partition_pd[col], errors='coerce')\n","\n","    # d. Simple imputation: fill any remaining missing values\n","    partition_pd = partition_pd.fillna(-999)\n","\n","    # e. Generate predictions for this chunk\n","    chunk_predictions = best_clf.predict_proba(partition_pd)[:, 1]\n","    all_predictions.append(chunk_predictions)\n","\n","# Concatenate the results from all chunks into a single numpy array\n","test_predictions_proba = np.concatenate(all_predictions)\n","print(\"Predictions generated successfully.\")\n","\n","\n","# --- 4. Create and Save the Submission File ---\n","print(\"\\nCreating the final submission file...\")\n","\n","# Create the submission DataFrame\n","submission_df = submission_ids.copy()\n","\n","# Format the 'id5' date column to mm-dd-yyyy as requested\n","submission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\n","\n","# Add the final predictions\n","# Clean predictions to ensure no NaNs and format to prevent scientific notation\n","cleaned_predictions = np.nan_to_num(test_predictions_proba, nan=0.0)\n","submission_df['pred'] = [f\"{p:.10f}\" for p in cleaned_predictions]\n","\n","print(\"\\nFirst 5 rows of the final submission file:\")\n","print(submission_df.head())\n","\n","# Define the output path\n","submission_path = '/content/drive/MyDrive/r2_submission_file_xgb_gkf_ran.csv'\n","\n","try:\n","    submission_df.to_csv(submission_path, index=False)\n","    print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n","except Exception as e:\n","    print(f\"\\nError saving submission file: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sog2O2nQ9uuW","executionInfo":{"status":"ok","timestamp":1751818381047,"user_tz":-330,"elapsed":109076,"user":{"displayName":"Shubham Kumar","userId":"07217116354773701877"}},"outputId":"5d076188-47a1-4dc4-e61c-355f08b462f4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the saved best XGBoost model...\n","Model loaded successfully.\n","\n","Loading test data identifiers...\n","Successfully loaded test data identifiers and created Dask DataFrame.\n","\n","Generating predictions by processing data in chunks...\n","Processing partition 1/1...\n","Predictions generated successfully.\n","\n","Creating the final submission file...\n","\n","First 5 rows of the final submission file:\n","                                               id1      id2     id3  \\\n","0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n","1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n","2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n","3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n","4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n","\n","          id5          pred  \n","0  11-04-2023  0.0553781241  \n","1  11-04-2023  0.1124634817  \n","2  11-05-2023  0.9906606078  \n","3  11-04-2023  0.0513244905  \n","4  11-05-2023  0.0308717992  \n","\n","Submission file successfully saved to: /content/drive/MyDrive/r2_submission_file_xgb_gkf_ran.csv\n"]}]}]}