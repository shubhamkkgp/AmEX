{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12516693,"sourceType":"datasetVersion","datasetId":7851172},{"sourceId":12522023,"sourceType":"datasetVersion","datasetId":7904190}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:23:15.412263Z","iopub.execute_input":"2025-08-31T17:23:15.412534Z","iopub.status.idle":"2025-08-31T17:23:15.671205Z","shell.execute_reply.started":"2025-08-31T17:23:15.412514Z","shell.execute_reply":"2025-08-31T17:23:15.670640Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amex-v15/test_data_v15.parquet\n/kaggle/input/amex-v15/train_data_v15.parquet\n/kaggle/input/amexkkgp/test_data.parquet\n/kaggle/input/amexkkgp/add_event.parquet\n/kaggle/input/amexkkgp/data_dictionary.csv\n/kaggle/input/amexkkgp/offer_metadata.parquet\n/kaggle/input/amexkkgp/add_trans.parquet\n/kaggle/input/amexkkgp/test_enriched.parquet\n/kaggle/input/amexkkgp/train_enriched.parquet\n/kaggle/input/amexkkgp/train_data.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import dask.dataframe as dd\n# import warnings\n\n# warnings.filterwarnings('ignore')\n\n# print(\"Loading datasets...\")\n# # --- 1. Load Datasets (Using Dask for Large Files) ---\n# # Load smaller files with pandas\n# stratified_train_df = pd.read_parquet('/kaggle/input/amexkkgp/train_data.parquet')\n# test_df = pd.read_parquet('/kaggle/input/amexkkgp/test_data.parquet')\n# offer_df = pd.read_parquet('/kaggle/input/amexkkgp/offer_metadata.parquet')\n\n# # Load large event and transaction logs with Dask to prevent memory crashes\n# try:\n#     event_df_dask = dd.read_parquet('/kaggle/input/amexkkgp/add_event.parquet')\n#     trans_df_dask = dd.read_parquet('/kaggle/input/amexkkgp/add_trans.parquet')\n#     print(\"All datasets loaded successfully (using Dask for large files).\")\n# except Exception as e:\n#     print(f\"Error loading supplementary data: {e}\")\n#     # Terminate if supplementary data isn't available\n#     exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.050039Z","iopub.execute_input":"2025-08-31T16:20:25.050553Z","iopub.status.idle":"2025-08-31T16:20:25.054799Z","shell.execute_reply.started":"2025-08-31T16:20:25.050522Z","shell.execute_reply":"2025-08-31T16:20:25.053982Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# stratified_train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.057206Z","iopub.execute_input":"2025-08-31T16:20:25.057429Z","iopub.status.idle":"2025-08-31T16:20:25.089413Z","shell.execute_reply.started":"2025-08-31T16:20:25.057411Z","shell.execute_reply":"2025-08-31T16:20:25.088635Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # --- 2. Feature Engineering from add_event_df (Offer-Level Only) ---\n# print(\"\\nStarting feature engineering on event data for OFFERS only...\")\n\n# # Create a 'clicked' column based on whether id7 is null\n# event_df_dask['clicked'] = (~event_df_dask['id7'].isnull()).astype(int)\n\n# # --- Offer-level event features ---\n# # Group by offer (id3) and aggregate its historical performance\n# offer_event_features_dask = event_df_dask.groupby('id3').agg(\n#     offer_total_impressions=('id4', 'count'),\n#     offer_total_clicks=('clicked', 'sum')\n# )\n# # Calculate offer's historical CTR\n# offer_event_features_dask['offer_historical_ctr'] = (\n#     offer_event_features_dask['offer_total_clicks'] / offer_event_features_dask['offer_total_impressions']\n# ).fillna(0)\n\n# # --- Execute Dask computation ---\n# print(\"Computing aggregated offer features...\")\n# offer_event_features = offer_event_features_dask.compute().reset_index()\n# print(\"Offer event features created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.090302Z","iopub.execute_input":"2025-08-31T16:20:25.090554Z","iopub.status.idle":"2025-08-31T16:20:25.104826Z","shell.execute_reply.started":"2025-08-31T16:20:25.090506Z","shell.execute_reply":"2025-08-31T16:20:25.104181Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# # --- 3. NEW: Feature Engineering from add_trans_df (Industry-Level) ---\n# print(\"\\nStarting feature engineering on transaction data for INDUSTRIES...\")\n# trans_df_dask['f367'] = dd.to_numeric(trans_df_dask['f367'], errors='coerce')\n\n# # Group by industry (id8) and aggregate transaction behavior\n# industry_trans_features_dask = trans_df_dask.groupby('id8').agg(\n#     industry_avg_spend=('f367', 'mean'),\n#     industry_total_transactions=('f367', 'count'),\n#     industry_unique_products=('f368', dd.Aggregation('nunique', chunk=lambda s: s.nunique(), agg=lambda s: s.nunique()))\n# )\n# print(\"Computing aggregated industry features...\")\n# industry_trans_features = industry_trans_features_dask.compute().reset_index()\n# print(\"Industry transaction features created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.105621Z","iopub.execute_input":"2025-08-31T16:20:25.105843Z","iopub.status.idle":"2025-08-31T16:20:25.123622Z","shell.execute_reply.started":"2025-08-31T16:20:25.105818Z","shell.execute_reply":"2025-08-31T16:20:25.122854Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # --- 4. Feature Engineering from offer_metadata_df (Pandas) ---\n# print(\"\\nStarting feature engineering on offer metadata...\")\n# offer_df['id12'] = pd.to_datetime(offer_df['id12'], errors='coerce')\n# offer_df['id13'] = pd.to_datetime(offer_df['id13'], errors='coerce')\n# offer_df['offer_duration_days'] = (offer_df['id13'] - offer_df['id12']).dt.days\n\n# offer_meta_features = offer_df[['id3', 'f375', 'f376', 'id10', 'id8', 'offer_duration_days']].rename(columns={\n#     'f375': 'offer_redemption_freq',\n#     'f376': 'offer_discount_rate',\n#     'id10': 'offer_type_code' # Renamed to avoid confusion with industry\n# })\n# # Ensure the merge keys are the correct type\n# offer_meta_features['id3'] = offer_meta_features['id3'].astype(str)\n# offer_meta_features['id8'] = offer_meta_features['id8'].astype(str)\n# industry_trans_features['id8'] = industry_trans_features['id8'].astype(str)\n\n# # --- NEW: Merge industry features into offer metadata ---\n# offer_meta_features = pd.merge(offer_meta_features, industry_trans_features, on='id8', how='left')\n# print(\"Offer metadata enriched with industry transaction data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.124560Z","iopub.execute_input":"2025-08-31T16:20:25.124765Z","iopub.status.idle":"2025-08-31T16:20:25.139150Z","shell.execute_reply.started":"2025-08-31T16:20:25.124746Z","shell.execute_reply":"2025-08-31T16:20:25.138341Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# # --- 5. Merge All New Features into Main DataFrames ---\n# print(\"\\nMerging all new features into training and test sets...\")\n# def enrich_dataframe(df):\n#     \"\"\"Merges all engineered features into a given dataframe.\"\"\"\n#     df['id3'] = df['id3'].astype(str)\n\n#     # Now merging small, pre-computed pandas DataFrames\n#     df = pd.merge(df, offer_event_features, on='id3', how='left')\n#     df = pd.merge(df, offer_meta_features, on='id3', how='left')\n\n#     return df\n\n# train_enriched = enrich_dataframe(stratified_train_df)\n# test_enriched = enrich_dataframe(test_df)\n\n# print(f\"Enriched training data shape: {train_enriched.shape}\")\n# print(f\"Enriched test data shape: {test_enriched.shape}\")\n\n\n# # --- 6. Save the Enriched Datasets ---\n# print(\"\\nSaving enriched datasets to Parquet files...\")\n# try:\n#     train_enriched.to_parquet('/kaggle/working/train_enriched_v10.parquet')\n#     test_enriched.to_parquet('/kaggle/working/test_enriched_v10.parquet')\n#     print(\"Successfully saved enriched data.\")\n# except Exception as e:\n#     print(f\"Error saving enriched data: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:20:25.140010Z","iopub.execute_input":"2025-08-31T16:20:25.140318Z","iopub.status.idle":"2025-08-31T16:20:25.162018Z","shell.execute_reply.started":"2025-08-31T16:20:25.140290Z","shell.execute_reply":"2025-08-31T16:20:25.160060Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings('ignore')\n\nprint(\"--- STAGE 1: Loading Datasets ---\")\n# --- Load Datasets (Using Dask for Large Files) ---\n# Define file paths\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# Load smaller files with pandas\nfull_train_df = pd.read_parquet(f'{INPUT_DIR}train_data.parquet')\n# This is the competition's test set, which we will use for the final prediction\n# prediction_df = pd.read_parquet(f'{INPUT_DIR}test_data.parquet') \noffer_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n\n# Load large event and transaction logs with Dask to prevent memory crashes\ntry:\n    event_df_dask = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    trans_df_dask = dd.read_parquet(f'{INPUT_DIR}add_trans.parquet')\n    print(\"All datasets loaded successfully (using Dask for large files).\")\nexcept Exception as e:\n    print(f\"Error loading supplementary data: {e}\")\n    # Terminate if supplementary data isn't available\n    exit()\n\n# =============================================================================\n# STAGE 2: INTELLIGENT SAMPLING AND DATA SPLITTING\n# =============================================================================\nprint(\"\\n--- STAGE 2: Applying Intelligent Sampling (5:1 Ratio) and Splitting Data ---\")\n\n# Ensure the target column 'y' is numeric\nfull_train_df['y'] = pd.to_numeric(full_train_df['y'], errors='coerce')\n\n# Separate positive (clicks) and negative (non-clicks) samples from the full training data\ndf_positive = full_train_df[full_train_df['y'] == 1].copy()\ndf_negative = full_train_df[full_train_df['y'] == 0].copy()\n\n# Calculate the number of negative samples to keep (5 times the number of positives)\nnum_positive_samples = len(df_positive)\nnum_negative_samples_to_keep = num_positive_samples * 5\n\nprint(f\"Original data: {num_positive_samples} positive samples, {len(df_negative)} negative samples.\")\nprint(f\"Sampling {num_negative_samples_to_keep} negative samples for a 5:1 ratio...\")\n\n# Randomly sample the negative instances\ndf_negative_sampled = df_negative.sample(n=num_negative_samples_to_keep, random_state=42)\n\n# Combine the positive samples with the sampled negative samples to create our master sampled dataset\ndf_sampled = pd.concat([df_positive, df_negative_sampled])\n\nprint(\"\\nSampling complete.\")\nprint(f\"Total size of sampled data before splitting: {df_sampled.shape}\")\n\n# --- Create a stratified train/test split from the sampled data ---\n# This will be our new training set and a reliable, labeled hold-out test set\nprint(\"Creating a 75/25 stratified split for training and hold-out testing...\")\ny = df_sampled['y']\nX = df_sampled.drop(columns=['y'])\n\n# stratify=y ensures the 5:1 ratio is maintained in both the train and test sets\ntrain_df, holdout_test_df = train_test_split(df_sampled, test_size=0.25, random_state=42, stratify=y)\n\nprint(\"\\nSplitting complete.\")\nprint(f\"New training data shape: {train_df.shape}\")\nprint(f\"New hold-out test data shape: {holdout_test_df.shape}\")\nprint(\"\\nNew training data class distribution:\")\nprint(train_df['y'].value_counts())\nprint(\"\\nHold-out test data class distribution:\")\nprint(holdout_test_df['y'].value_counts())\n\n\n# =============================================================================\n# STAGE 3: FOUNDATIONAL FEATURE ENGINEERING\n# =============================================================================\nprint(\"\\n--- STAGE 3: Starting Foundational Feature Engineering ---\")\n\n# --- Feature Engineering from add_event_df (Offer-Level) ---\nprint(\"Engineering offer-level features from event data...\")\nevent_df_dask['clicked'] = (~event_df_dask['id7'].isnull()).astype(int)\noffer_event_features_dask = event_df_dask.groupby('id3').agg(\n    offer_total_impressions=('id4', 'count'),\n    offer_total_clicks=('clicked', 'sum')\n)\noffer_event_features_dask['offer_historical_ctr'] = (\n    offer_event_features_dask['offer_total_clicks'] / offer_event_features_dask['offer_total_impressions']\n).fillna(0)\noffer_event_features = offer_event_features_dask.compute().reset_index()\nprint(\"Offer event features created.\")\n\n# --- Feature Engineering from add_trans_df (Industry-Level) ---\nprint(\"Engineering industry-level features from transaction data...\")\ntrans_df_dask['f367'] = dd.to_numeric(trans_df_dask['f367'], errors='coerce')\nindustry_trans_features_dask = trans_df_dask.groupby('id8').agg(\n    industry_avg_spend=('f367', 'mean'),\n    industry_total_transactions=('f367', 'count'),\n    industry_unique_products=('f368', dd.Aggregation('nunique', chunk=lambda s: s.nunique(), agg=lambda s: s.nunique()))\n)\nindustry_trans_features = industry_trans_features_dask.compute().reset_index()\nprint(\"Industry transaction features created.\")\n\n# --- Feature Engineering from offer_metadata_df ---\nprint(\"Engineering features from offer metadata...\")\noffer_df['id12'] = pd.to_datetime(offer_df['id12'], errors='coerce')\noffer_df['id13'] = pd.to_datetime(offer_df['id13'], errors='coerce')\noffer_df['offer_duration_days'] = (offer_df['id13'] - offer_df['id12']).dt.days\n\noffer_meta_features = offer_df[['id3', 'f375', 'f376', 'id10', 'id8', 'offer_duration_days']].rename(columns={\n    'f375': 'offer_redemption_freq',\n    'f376': 'offer_discount_rate',\n    'id10': 'offer_type_code'\n})\noffer_meta_features['id3'] = offer_meta_features['id3'].astype(str)\noffer_meta_features['id8'] = offer_meta_features['id8'].astype(str)\nindustry_trans_features['id8'] = industry_trans_features['id8'].astype(str)\noffer_meta_features = pd.merge(offer_meta_features, industry_trans_features, on='id8', how='left')\nprint(\"Offer metadata enriched with industry data.\")\n\n# --- Clean up Dask DataFrames ---\ndel event_df_dask, trans_df_dask\ngc.collect()\n\n# =============================================================================\n# STAGE 4: MERGE FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 4: Merging all new features and saving data ---\")\ndef enrich_dataframe(df):\n    \"\"\"Merges all engineered features into a given dataframe.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    df = pd.merge(df, offer_event_features, on='id3', how='left')\n    df = pd.merge(df, offer_meta_features, on='id3', how='left')\n    return df\n\n# Enrich all three dataframes\ntrain_enriched = enrich_dataframe(train_df)\ntest_enriched = enrich_dataframe(holdout_test_df)\n# prediction_enriched = enrich_dataframe(prediction_df)\n\n\nprint(f\"Enriched training data shape: {train_enriched.shape}\")\nprint(f\"Enriched holdout-test data shape: {test_enriched.shape}\")\n# print(f\"Enriched prediction data shape: {prediction_enriched.shape}\")\n\n# --- Save the Enriched Datasets ---\nprint(\"Saving final datasets to Parquet files...\")\ntry:\n    train_enriched.to_parquet(f'{WORKING_DIR}train_final.parquet')\n    test_enriched.to_parquet(f'{WORKING_DIR}test_final.parquet')\n    # prediction_enriched.to_parquet(f'{WORKING_DIR}prediction_final.parquet')\n    print(\"Successfully saved all three final datasets.\")\nexcept Exception as e:\n    print(f\"Error saving data: {e}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:24:05.348872Z","iopub.execute_input":"2025-08-31T17:24:05.349651Z","iopub.status.idle":"2025-08-31T17:25:08.175669Z","shell.execute_reply.started":"2025-08-31T17:24:05.349623Z","shell.execute_reply":"2025-08-31T17:25:08.174846Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Datasets ---\nAll datasets loaded successfully (using Dask for large files).\n\n--- STAGE 2: Applying Intelligent Sampling (5:1 Ratio) and Splitting Data ---\nOriginal data: 37051 positive samples, 733113 negative samples.\nSampling 185255 negative samples for a 5:1 ratio...\n\nSampling complete.\nTotal size of sampled data before splitting: (222306, 372)\nCreating a 75/25 stratified split for training and hold-out testing...\n\nSplitting complete.\nNew training data shape: (166729, 372)\nNew hold-out test data shape: (55577, 372)\n\nNew training data class distribution:\ny\n0    138941\n1     27788\nName: count, dtype: int64\n\nHold-out test data class distribution:\ny\n0    46314\n1     9263\nName: count, dtype: int64\n\n--- STAGE 3: Starting Foundational Feature Engineering ---\nEngineering offer-level features from event data...\nOffer event features created.\nEngineering industry-level features from transaction data...\nIndustry transaction features created.\nEngineering features from offer metadata...\nOffer metadata enriched with industry data.\n\n--- STAGE 4: Merging all new features and saving data ---\nEnriched training data shape: (166729, 383)\nEnriched holdout-test data shape: (55577, 383)\nSaving final datasets to Parquet files...\nSuccessfully saved all three final datasets.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_enriched.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:25:12.095910Z","iopub.execute_input":"2025-08-31T17:25:12.096432Z","iopub.status.idle":"2025-08-31T17:25:12.101835Z","shell.execute_reply.started":"2025-08-31T17:25:12.096386Z","shell.execute_reply":"2025-08-31T17:25:12.101043Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(166729, 383)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_enriched.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:25:12.683865Z","iopub.execute_input":"2025-08-31T17:25:12.684467Z","iopub.status.idle":"2025-08-31T17:25:12.711323Z","shell.execute_reply.started":"2025-08-31T17:25:12.684436Z","shell.execute_reply":"2025-08-31T17:25:12.710760Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              id1      id2       id3  \\\n0    1309455_654480_16-23_2023-11-01 05:29:18.734  1309455    654480   \n1  1277975_66290365_16-23_2023-11-01 10:00:06.073  1277975  66290365   \n2    1756831_783808_16-23_2023-11-02 07:45:41.794  1756831    783808   \n3     1839978_80893_16-23_2023-11-02 07:03:16.575  1839978     80893   \n4    1382370_331980_16-23_2023-11-03 07:18:32.167  1382370    331980   \n\n                       id4         id5  y    f1    f2    f3    f4  ...  \\\n0  2023-11-01 05:29:18.734  2023-11-01  0  None  None  None  None  ...   \n1  2023-11-01 10:00:06.073  2023-11-01  0   5.0  None  13.0  88.0  ...   \n2  2023-11-02 07:45:41.794  2023-11-02  0  None  None  None  None  ...   \n3  2023-11-02 07:03:16.575  2023-11-02  0  48.0  25.0  22.0  29.0  ...   \n4  2023-11-03 07:18:32.167  2023-11-03  0  None  40.0  None  None  ...   \n\n  offer_total_clicks offer_historical_ctr offer_redemption_freq  \\\n0               1098             0.016595                   1.0   \n1                719             0.054272                   2.0   \n2                358             0.008301                   1.0   \n3                852             0.058545                   1.0   \n4               4360             0.071267                   1.0   \n\n  offer_discount_rate offer_type_code       id8 offer_duration_days  \\\n0                 NaN               1  59991300               165.0   \n1                 2.0               1  56610000                29.0   \n2                 NaN               1  54990000               108.0   \n3                 NaN               1  59630000                90.0   \n4                 NaN               1  56990300                69.0   \n\n  industry_avg_spend industry_total_transactions industry_unique_products  \n0         167.946222                     14005.0                      1.0  \n1         373.318764                     11470.0                      1.0  \n2          81.011179                     61048.0                      1.0  \n3         316.704859                     11749.0                      1.0  \n4         178.032686                     13629.0                      1.0  \n\n[5 rows x 383 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id1</th>\n      <th>id2</th>\n      <th>id3</th>\n      <th>id4</th>\n      <th>id5</th>\n      <th>y</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>...</th>\n      <th>offer_total_clicks</th>\n      <th>offer_historical_ctr</th>\n      <th>offer_redemption_freq</th>\n      <th>offer_discount_rate</th>\n      <th>offer_type_code</th>\n      <th>id8</th>\n      <th>offer_duration_days</th>\n      <th>industry_avg_spend</th>\n      <th>industry_total_transactions</th>\n      <th>industry_unique_products</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1309455_654480_16-23_2023-11-01 05:29:18.734</td>\n      <td>1309455</td>\n      <td>654480</td>\n      <td>2023-11-01 05:29:18.734</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>1098</td>\n      <td>0.016595</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>59991300</td>\n      <td>165.0</td>\n      <td>167.946222</td>\n      <td>14005.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1277975_66290365_16-23_2023-11-01 10:00:06.073</td>\n      <td>1277975</td>\n      <td>66290365</td>\n      <td>2023-11-01 10:00:06.073</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>5.0</td>\n      <td>None</td>\n      <td>13.0</td>\n      <td>88.0</td>\n      <td>...</td>\n      <td>719</td>\n      <td>0.054272</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>56610000</td>\n      <td>29.0</td>\n      <td>373.318764</td>\n      <td>11470.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1756831_783808_16-23_2023-11-02 07:45:41.794</td>\n      <td>1756831</td>\n      <td>783808</td>\n      <td>2023-11-02 07:45:41.794</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>358</td>\n      <td>0.008301</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>54990000</td>\n      <td>108.0</td>\n      <td>81.011179</td>\n      <td>61048.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1839978_80893_16-23_2023-11-02 07:03:16.575</td>\n      <td>1839978</td>\n      <td>80893</td>\n      <td>2023-11-02 07:03:16.575</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>48.0</td>\n      <td>25.0</td>\n      <td>22.0</td>\n      <td>29.0</td>\n      <td>...</td>\n      <td>852</td>\n      <td>0.058545</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>59630000</td>\n      <td>90.0</td>\n      <td>316.704859</td>\n      <td>11749.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1382370_331980_16-23_2023-11-03 07:18:32.167</td>\n      <td>1382370</td>\n      <td>331980</td>\n      <td>2023-11-03 07:18:32.167</td>\n      <td>2023-11-03</td>\n      <td>0</td>\n      <td>None</td>\n      <td>40.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>4360</td>\n      <td>0.071267</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>56990300</td>\n      <td>69.0</td>\n      <td>178.032686</td>\n      <td>13629.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 383 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Modeling libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA AND DATA DICTIONARY\n# =============================================================================\nprint(\"--- STAGE 1: Loading Data ---\")\ntry:\n    # Load the datasets created by the previous sampling script\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_final.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}test_final.parquet')\n    \n    # Load the data dictionary\n    data_dict = pd.read_csv(f'{INPUT_DIR}data_dictionary.csv')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Training data shape: {df_train.shape}\")\n    print(f\"Hold-out test data shape: {df_holdout_test.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please run the sampling_and_feature_engineering.py script first.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: DEFINE THE REUSABLE CLUSTERING FUNCTION\n# =============================================================================\nprint(\"\\n--- STAGE 2: Defining the Clustering Pipeline Function ---\")\n\ndef create_cluster_features(df_train, df_holdout, data_dict, keywords, cluster_col_name, n_clusters=5):\n    \"\"\"\n    Performs a full PCA + K-Means clustering pipeline.\n    It FITS on the training data and TRANSFORMS both train and holdout datasets.\n    \"\"\"\n    print(f\"\\n--- Creating cluster feature: '{cluster_col_name}' ---\")\n    \n    # --- a. Select Features ---\n    print(f\"Selecting features based on keywords: {keywords}...\")\n    try:\n        if keywords == 'all':\n            group_features_in_df = [col for col in df_train.columns if col.startswith('f')]\n        else:\n            feature_name_col = 'masked_column'\n            description_col = 'Description'\n            group_mask = data_dict[description_col].str.contains('|'.join(keywords), case=False, na=False)\n            group_features = data_dict[group_mask][feature_name_col].tolist()\n            group_features_in_df = [col for col in group_features if col in df_train.columns]\n        \n        if not group_features_in_df:\n            print(f\"Warning: No features found for group '{cluster_col_name}'. Skipping.\")\n            return df_train, df_holdout\n            \n        print(f\"Found {len(group_features_in_df)} features for this group.\")\n        # Ensure consistent column order\n        X_train_cluster = df_train[group_features_in_df].copy()\n        X_holdout_cluster = df_holdout[group_features_in_df].copy()\n\n    except Exception as e:\n        print(f\"Error selecting features: {e}. Skipping this cluster.\")\n        return df_train, df_holdout\n\n    # --- b. Preprocess Data ---\n    print(\"Preprocessing data (impute, scale)...\")\n    for col in X_train_cluster.columns:\n        X_train_cluster[col] = pd.to_numeric(X_train_cluster[col], errors='coerce')\n        X_holdout_cluster[col] = pd.to_numeric(X_holdout_cluster[col], errors='coerce')\n\n    imputer = SimpleImputer(strategy='median')\n    scaler = StandardScaler()\n    \n    # Fit ONLY on training data, then transform both\n    X_train_imputed = imputer.fit_transform(X_train_cluster)\n    X_holdout_imputed = imputer.transform(X_holdout_cluster)\n\n    X_train_scaled = scaler.fit_transform(X_train_imputed)\n    X_holdout_scaled = scaler.transform(X_holdout_imputed)\n\n    # --- c. PCA ---\n    print(\"Applying PCA...\")\n    pca = PCA(n_components=0.90) # Keep components explaining 90% of variance\n    pca.fit(X_train_scaled)\n    \n    X_train_pca = pca.transform(X_train_scaled)\n    X_holdout_pca = pca.transform(X_holdout_scaled)\n    print(f\"PCA resulted in {pca.n_components_} components.\")\n\n    # --- d. K-Means Clustering ---\n    print(\"Applying K-Means...\")\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n    \n    # Fit ONLY on training data, then predict for both\n    df_train[cluster_col_name] = kmeans.fit_predict(X_train_pca)\n    df_holdout[cluster_col_name] = kmeans.predict(X_holdout_pca)\n    \n    print(f\"Successfully created and added '{cluster_col_name}' feature to both datasets.\")\n    \n    return df_train, df_holdout\n\n\n# =============================================================================\n# STAGE 3: DEFINE FEATURE GROUPS AND RUN THE PIPELINE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Running the Clustering Pipeline for Each Feature Group ---\")\n\n# --- Define Keyword Groups ---\n# Group 1: Customer Spending Behavior\nspending_keywords = ['debit amount', 'debit transaction', 'spend in']\ndf_train, df_holdout_test = create_cluster_features(df_train, df_holdout_test, data_dict, spending_keywords, 'spending_cluster')\n\n# Group 2: Customer Click/Impression History\nhistory_keywords = ['ctr in last', 'clicks in last', 'impressions in last']\ndf_train, df_holdout_test = create_cluster_features(df_train, df_holdout_test, data_dict, history_keywords, 'history_cluster')\n\n# Group 3: Customer Profile & Loyalty\nprofile_keywords = ['membership level', 'account age', 'interest score', 'miles', 'segments']\ndf_train, df_holdout_test = create_cluster_features(df_train, df_holdout_test, data_dict, profile_keywords, 'profile_cluster')\n\n# Group 4: Customer Engagement on Amex Platforms\nengagement_keywords = ['time spent', 'pages viewed', 'visited', 'logon']\ndf_train, df_holdout_test = create_cluster_features(df_train, df_holdout_test, data_dict, engagement_keywords, 'engagement_cluster')\n\n# Group 5: All 'f' Features (Holistic)\ndf_train, df_holdout_test = create_cluster_features(df_train, df_holdout_test, data_dict, 'all', 'holistic_cluster')\n\n\n# =============================================================================\n# STAGE 4: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Final Datasets with New Cluster Features ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = ['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train.columns])\nprint(\"\\nColumns added to the holdout-test data:\")\nprint([col for col in new_cols if col in df_holdout_test.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_clustered.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_clustered.parquet'\n    \n    df_train.to_parquet(train_output_path)\n    df_holdout_test.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- CUSTOMER CLUSTER FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:25:17.311944Z","iopub.execute_input":"2025-08-31T17:25:17.312528Z","iopub.status.idle":"2025-08-31T17:26:20.024330Z","shell.execute_reply.started":"2025-08-31T17:25:17.312507Z","shell.execute_reply":"2025-08-31T17:26:20.023676Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Data ---\nSuccessfully loaded all required data.\nTraining data shape: (166729, 383)\nHold-out test data shape: (55577, 383)\n\n--- STAGE 2: Defining the Clustering Pipeline Function ---\n\n--- STAGE 3: Running the Clustering Pipeline for Each Feature Group ---\n\n--- Creating cluster feature: 'spending_cluster' ---\nSelecting features based on keywords: ['debit amount', 'debit transaction', 'spend in']...\nFound 47 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 31 components.\nApplying K-Means...\nSuccessfully created and added 'spending_cluster' feature to both datasets.\n\n--- Creating cluster feature: 'history_cluster' ---\nSelecting features based on keywords: ['ctr in last', 'clicks in last', 'impressions in last']...\nFound 49 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 28 components.\nApplying K-Means...\nSuccessfully created and added 'history_cluster' feature to both datasets.\n\n--- Creating cluster feature: 'profile_cluster' ---\nSelecting features based on keywords: ['membership level', 'account age', 'interest score', 'miles', 'segments']...\nFound 18 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 13 components.\nApplying K-Means...\nSuccessfully created and added 'profile_cluster' feature to both datasets.\n\n--- Creating cluster feature: 'engagement_cluster' ---\nSelecting features based on keywords: ['time spent', 'pages viewed', 'visited', 'logon']...\nFound 45 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 21 components.\nApplying K-Means...\nSuccessfully created and added 'engagement_cluster' feature to both datasets.\n\n--- Creating cluster feature: 'holistic_cluster' ---\nSelecting features based on keywords: all...\nFound 366 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 165 components.\nApplying K-Means...\nSuccessfully created and added 'holistic_cluster' feature to both datasets.\n\n--- STAGE 4: Saving Final Datasets with New Cluster Features ---\n\nColumns added to the training data:\n['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\n\nColumns added to the holdout-test data:\n['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\n\nSuccessfully saved final training data to: /kaggle/working/train_clustered.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_clustered.parquet\n\n--- CUSTOMER CLUSTER FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD LATEST DATASETS\n# =============================================================================\nprint(\"--- STAGE 1: Loading Latest Datasets ---\")\ntry:\n    # Load your latest training and holdout test sets with cluster features\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_clustered.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_clustered.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Input training data shape: {df_train.shape}\")\n    print(f\"Input hold-out test data shape: {df_holdout_test.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Required clustered data not found. Please run the customer_clustering.py script first.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: CREATE TIME-BASED AND INTERACTION FEATURES\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating Time-Based and Interaction Features ---\")\n\ndef create_advanced_features(df):\n    \"\"\"A reusable function to create time-based and interaction features.\"\"\"\n    \n    # --- Time-Based Features ---\n    df['impression_time'] = pd.to_datetime(df['id4'], errors='coerce')\n    df['hour_of_day'] = df['impression_time'].dt.hour\n    df['day_of_week'] = df['impression_time'].dt.dayofweek # Monday=0, Sunday=6\n    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n    \n    # --- Cyclical Time Features ---\n    # This helps the model understand the cyclical nature of time (e.g., hour 23 is close to hour 0)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day']/24.0)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day']/24.0)\n    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week']/7.0)\n    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week']/7.0)\n\n    # --- Interaction Features ---\n    # We create these based on hypotheses from our EDA.\n    # For example, is a certain offer type more effective on the weekend?\n    if 'offer_type_code' in df.columns:\n        # Convert to string to handle potential mixed types and ensure concatenation works\n        df['offer_type_x_weekend'] = df['offer_type_code'].astype(str) + \"_\" + df['is_weekend'].astype(str)\n    \n    # How does an offer's historical popularity compare to the average for its type?\n    if 'offer_historical_ctr' in df.columns and 'offer_type_code' in df.columns:\n        # Use transform to broadcast the mean back to the original dataframe shape\n        avg_ctr_by_type = df.groupby('offer_type_code')['offer_historical_ctr'].transform('mean')\n        df['offer_ctr_vs_type_avg'] = df['offer_historical_ctr'] / (avg_ctr_by_type + 1e-6) # Add epsilon to avoid division by zero\n\n    # This column is no longer needed after feature extraction\n    df = df.drop(columns=['impression_time'])\n    \n    return df\n\n# --- Add features to both the training and holdout test dataframes ---\nprint(\"Enriching training data with new features...\")\ndf_train_final = create_advanced_features(df_train)\nprint(\"Enriching hold-out test data with new features...\")\ndf_holdout_test_final = create_advanced_features(df_holdout_test)\n\n\n# =============================================================================\n# STAGE 3: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Saving Final Datasets with All New Features ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = [\n    'hour_of_day', 'day_of_week', 'is_weekend',\n    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n    'offer_type_x_weekend', 'offer_ctr_vs_type_avg'\n]\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train_final.columns])\nprint(\"\\nColumns added to the holdout-test data:\")\nprint([col for col in new_cols if col in df_holdout_test_final.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_time_features.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_time_features.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_holdout_test_final.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- TIME-BASED FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:26:20.025610Z","iopub.execute_input":"2025-08-31T17:26:20.025827Z","iopub.status.idle":"2025-08-31T17:26:32.575099Z","shell.execute_reply.started":"2025-08-31T17:26:20.025810Z","shell.execute_reply":"2025-08-31T17:26:32.574271Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Latest Datasets ---\nSuccessfully loaded all required data.\nInput training data shape: (166729, 388)\nInput hold-out test data shape: (55577, 388)\n\n--- STAGE 2: Creating Time-Based and Interaction Features ---\nEnriching training data with new features...\nEnriching hold-out test data with new features...\n\n--- STAGE 3: Saving Final Datasets with All New Features ---\n\nColumns added to the training data:\n['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'offer_type_x_weekend', 'offer_ctr_vs_type_avg']\n\nColumns added to the holdout-test data:\n['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'offer_type_x_weekend', 'offer_ctr_vs_type_avg']\n\nSuccessfully saved final training data to: /kaggle/working/train_time_features.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_time_features.parquet\n\n--- TIME-BASED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df_train_final.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:26:32.575799Z","iopub.execute_input":"2025-08-31T17:26:32.576038Z","iopub.status.idle":"2025-08-31T17:26:32.580855Z","shell.execute_reply.started":"2025-08-31T17:26:32.576020Z","shell.execute_reply":"2025-08-31T17:26:32.580124Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(166729, 397)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD ALL NECESSARY DATA\n# =============================================================================\nprint(\"--- STAGE 1: Loading All Necessary Data ---\")\ntry:\n    # Load the datasets with cluster features\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_clustered.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_clustered.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    trans_dd = dd.read_parquet(f'{INPUT_DIR}add_trans.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Input training data shape: {df_train.shape}\")\n    print(f\"Input holdout-test data shape: {df_holdout_test.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure the customer_clustering.py script ran successfully.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: CREATE NEW ADVANCED FEATURES\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating New Advanced Features ---\")\n\n# --- Feature 1: Average Time-to-Click per Offer ---\nprint(\"Creating 'Average Time-to-Click' feature...\")\nevent_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\nevent_dd['click_time'] = dd.to_datetime(event_dd['id7'], errors='coerce')\nevent_dd['time_to_click_seconds'] = (event_dd['click_time'] - event_dd['impression_time']).dt.total_seconds()\navg_time_to_click = event_dd.groupby('id3')['time_to_click_seconds'].mean().compute().reset_index()\navg_time_to_click = avg_time_to_click.rename(columns={'time_to_click_seconds': 'avg_offer_time_to_click_seconds'})\nprint(\"'Average Time-to-Click' feature created.\")\n\n# --- Feature 2 & 3: Debit/Credit and Transaction Time Patterns per Industry ---\nprint(\"Creating industry-level transaction pattern features...\")\ntrans_dd['f371_hour'] = dd.to_datetime(trans_dd['f371'], format='%H:%M:%S', errors='coerce').dt.hour\ndebit_credit_counts = trans_dd.groupby(['id8', 'f369']).size().compute().unstack(fill_value=0)\nif 'C' not in debit_credit_counts.columns: debit_credit_counts['C'] = 0\nif 'D' not in debit_credit_counts.columns: debit_credit_counts['D'] = 0\ndebit_credit_counts.columns = ['credit_trans_count', 'debit_trans_count']\navg_trans_hour = trans_dd.groupby('id8')['f371_hour'].mean().compute().reset_index(name='avg_trans_hour')\n\nindustry_pattern_features = pd.merge(debit_credit_counts.reset_index(), avg_trans_hour, on='id8', how='outer')\nprint(\"Industry pattern features created.\")\n\n# --- Feature 4: Offer Body Length ---\nprint(\"Creating 'Offer Body Length' feature...\")\noffer_meta_df['offer_body_length'] = offer_meta_df['f378'].str.len()\noffer_body_feature = offer_meta_df[['id3', 'offer_body_length']]\nprint(\"'Offer Body Length' feature created.\")\n\n# Clean up memory\ndel event_dd, trans_dd\ngc.collect()\n\n# =============================================================================\n# STAGE 3: MERGE ALL NEW FEATURES INTO MAIN DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging All New Features ---\")\n\ndef enrich_dataframe(df, avg_time_to_click, industry_pattern_features, offer_body_feature):\n    \"\"\"A reusable function to enrich a dataframe with all new features.\"\"\"\n    \n    # Ensure merge keys are the correct type\n    df['id3'] = df['id3'].astype(str)\n    df['id8'] = df['id8'].astype(str)\n    avg_time_to_click['id3'] = avg_time_to_click['id3'].astype(str)\n    industry_pattern_features['id8'] = industry_pattern_features['id8'].astype(str)\n    offer_body_feature['id3'] = offer_body_feature['id3'].astype(str)\n\n    # Merge Time-to-Click\n    df = pd.merge(df, avg_time_to_click, on='id3', how='left')\n    \n    # Merge Industry Patterns directly using the 'id8' column already in the dataframe\n    df = pd.merge(df, industry_pattern_features, on='id8', how='left')\n\n    # Merge Offer Body Length\n    df = pd.merge(df, offer_body_feature, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and holdout test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, avg_time_to_click, industry_pattern_features, offer_body_feature)\nprint(\"Enriching hold-out test data...\")\ndf_holdout_test_final = enrich_dataframe(df_holdout_test, avg_time_to_click, industry_pattern_features, offer_body_feature)\n\n# =============================================================================\n# STAGE 4: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Final Datasets ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = [\n    'avg_offer_time_to_click_seconds', 'credit_trans_count', 'debit_trans_count',\n    'avg_trans_hour', 'offer_body_length'\n]\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train_final.columns])\nprint(\"\\nColumns added to the holdout-test data:\")\nprint([col for col in new_cols if col in df_holdout_test_final.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_advanced_features.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_advanced_features.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_holdout_test_final.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED HISTORICAL FEATURE ENGINEERING COMPLETE ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:26:32.582329Z","iopub.execute_input":"2025-08-31T17:26:32.582614Z","iopub.status.idle":"2025-08-31T17:27:20.850228Z","shell.execute_reply.started":"2025-08-31T17:26:32.582581Z","shell.execute_reply":"2025-08-31T17:27:20.849465Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading All Necessary Data ---\nSuccessfully loaded all required data.\nInput training data shape: (166729, 388)\nInput holdout-test data shape: (55577, 388)\n\n--- STAGE 2: Creating New Advanced Features ---\nCreating 'Average Time-to-Click' feature...\n'Average Time-to-Click' feature created.\nCreating industry-level transaction pattern features...\nIndustry pattern features created.\nCreating 'Offer Body Length' feature...\n'Offer Body Length' feature created.\n\n--- STAGE 3: Merging All New Features ---\nEnriching training data...\nEnriching hold-out test data...\n\n--- STAGE 4: Saving Final Datasets ---\n\nColumns added to the training data:\n['avg_offer_time_to_click_seconds', 'credit_trans_count', 'debit_trans_count', 'avg_trans_hour', 'offer_body_length']\n\nColumns added to the holdout-test data:\n['avg_offer_time_to_click_seconds', 'credit_trans_count', 'debit_trans_count', 'avg_trans_hour', 'offer_body_length']\n\nSuccessfully saved final training data to: /kaggle/working/train_advanced_features.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_advanced_features.parquet\n\n--- ADVANCED HISTORICAL FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD THE LATEST TRAINING DATA AND IDENTIFY ZERO-VARIANCE COLUMNS\n# =============================================================================\nprint(\"--- STAGE 1: Analyzing Training Data for Zero-Variance Features ---\")\ntry:\n    # Load your latest training set with advanced features\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_advanced_features.parquet')\n    print(\"Successfully loaded train_advanced_features.parquet.\")\n    print(f\"Original training data shape: {df_train.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: train_advanced_features.parquet not found. Please ensure the previous feature engineering script ran successfully.\")\n    exit()\n\n# --- Identify columns with zero variance based on the training data ---\n# A column is considered zero-variance if it has only one unique value and no NaNs.\ncols_to_drop = []\nfor col in df_train.columns:\n    # Skip identifier columns and the target variable\n    if col in ['id1', 'id2', 'id3', 'id4', 'id5', 'y']:\n        continue\n    \n    # Condition: exactly one unique value AND no missing values.\n    if df_train[col].nunique() == 1 and df_train[col].isnull().sum() == 0:\n        cols_to_drop.append(col)\n\nprint(f\"\\nFound {len(cols_to_drop)} columns with zero variance to remove.\")\nif cols_to_drop:\n    print(\"Columns to be removed:\", cols_to_drop)\n\n# --- Drop the identified columns from the training set ---\ndf_train_cleaned = df_train.drop(columns=cols_to_drop)\nprint(f\"\\nNew training data shape after removing zero-variance columns: {df_train_cleaned.shape}\")\n\n\n# =============================================================================\n# STAGE 2: APPLY THE SAME CLEANING TO THE HOLD-OUT TEST DATA\n# =============================================================================\nprint(\"\\n--- STAGE 2: Applying the Same Cleaning to the Hold-out Test Data ---\")\ntry:\n    # Load your latest hold-out test set\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_advanced_features.parquet')\n    print(\"Successfully loaded holdout_test_advanced_features.parquet.\")\n    print(f\"Original hold-out test data shape: {df_holdout_test.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: holdout_test_advanced_features.parquet not found.\")\n    exit()\n\n# --- Drop the same columns from the test set for consistency ---\n# Identify which of the columns to drop actually exist in the test set\ncols_to_drop_in_test = [col for col in cols_to_drop if col in df_holdout_test.columns]\ndf_holdout_test_cleaned = df_holdout_test.drop(columns=cols_to_drop_in_test)\nprint(f\"\\nRemoved {len(cols_to_drop_in_test)} columns from the hold-out test set.\")\nprint(f\"New hold-out test data shape: {df_holdout_test_cleaned.shape}\")\n\n\n# =============================================================================\n# STAGE 3: SAVE THE FINAL CLEANED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Saving Final Cleaned Datasets ---\")\ntry:\n    train_output_path = f'{WORKING_DIR}train_cleaned.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_cleaned.parquet'\n    \n    df_train_cleaned.to_parquet(train_output_path)\n    df_holdout_test_cleaned.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- DATA CLEANING COMPLETE ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:27:20.851184Z","iopub.execute_input":"2025-08-31T17:27:20.851804Z","iopub.status.idle":"2025-08-31T17:27:34.996865Z","shell.execute_reply.started":"2025-08-31T17:27:20.851783Z","shell.execute_reply":"2025-08-31T17:27:34.996046Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Analyzing Training Data for Zero-Variance Features ---\nSuccessfully loaded train_advanced_features.parquet.\nOriginal training data shape: (166729, 393)\n\nFound 0 columns with zero variance to remove.\n\nNew training data shape after removing zero-variance columns: (166729, 393)\n\n--- STAGE 2: Applying the Same Cleaning to the Hold-out Test Data ---\nSuccessfully loaded holdout_test_advanced_features.parquet.\nOriginal hold-out test data shape: (55577, 393)\n\nRemoved 0 columns from the hold-out test set.\nNew hold-out test data shape: (55577, 393)\n\n--- STAGE 3: Saving Final Cleaned Datasets ---\n\nSuccessfully saved final training data to: /kaggle/working/train_cleaned.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_cleaned.parquet\n\n--- DATA CLEANING COMPLETE ---\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Modeling libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"--- STAGE 1: Loading Data ---\")\ntry:\n    # Load your latest cleaned training and holdout test sets\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_cleaned.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_cleaned.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    # Use pandas for these as they are needed for clustering and should fit in memory\n    event_df = pd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    trans_df = pd.read_parquet(f'{INPUT_DIR}add_trans.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Input training data shape: {df_train.shape}\")\n    print(f\"Input hold-out test data shape: {df_holdout_test.shape}\")\n    \nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure the data_cleaning.py script ran successfully.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: OFFER CLUSTERING\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating Offer Clusters ---\")\n\n# --- a. Create features for each offer from historical event data ---\nprint(\"Aggregating historical features for each offer...\")\nevent_df['impression_time'] = pd.to_datetime(event_df['id4'], errors='coerce')\nevent_df['hour_of_day'] = event_df['impression_time'].dt.hour\nevent_df['is_weekend'] = (event_df['impression_time'].dt.dayofweek >= 5).astype(int)\nevent_df['clicked'] = event_df['id7'].notna().astype(int)\n\n# Aggregate stats for each offer\noffer_features = event_df.groupby('id3').agg(\n    offer_historical_ctr=('clicked', 'mean'),\n    offer_total_impressions=('id4', 'count'),\n    avg_impression_hour=('hour_of_day', 'mean'),\n    weekend_impression_ratio=('is_weekend', 'mean')\n).reset_index()\n\n# Merge with static offer metadata\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\noffer_features['id3'] = offer_features['id3'].astype(str)\noffer_features = pd.merge(offer_features, offer_meta_df[['id3', 'f375', 'f376']], on='id3', how='left')\noffer_features = offer_features.rename(columns={'f375': 'offer_redemption_freq', 'f376': 'offer_discount_rate'})\n\n# --- b. Preprocess and Cluster Offers ---\nprint(\"Preprocessing and clustering offers...\")\noffer_feature_cols = [col for col in offer_features.columns if col != 'id3']\nX_offer = offer_features[offer_feature_cols].copy()\n\nimputer = SimpleImputer(strategy='median')\nX_offer_imputed = imputer.fit_transform(X_offer)\nscaler = StandardScaler()\nX_offer_scaled = scaler.fit_transform(X_offer_imputed)\n\npca = PCA(n_components=0.95) # Explain 95% of variance\nX_offer_pca = pca.fit_transform(X_offer_scaled)\nprint(f\"Offer PCA resulted in {pca.n_components_} components.\")\n\nkmeans_offer = KMeans(n_clusters=5, random_state=42, n_init='auto')\noffer_features['offer_cluster'] = kmeans_offer.fit_predict(X_offer_pca)\nprint(\"Offer clusters created successfully.\")\n\n\n# =============================================================================\n# STAGE 3: INDUSTRY CLUSTERING\n# =============================================================================\nprint(\"\\n--- STAGE 3: Creating Industry Clusters ---\")\n\n# --- a. Create features for each industry from historical transaction data ---\nprint(\"Aggregating historical features for each industry...\")\ntrans_df['f367'] = pd.to_numeric(trans_df['f367'], errors='coerce')\nindustry_features = trans_df.groupby('id8').agg(\n    industry_avg_spend=('f367', 'mean'),\n    industry_std_spend=('f367', 'std'),\n    industry_total_transactions=('f367', 'count'),\n    industry_unique_products=('f368', 'nunique')\n).reset_index()\n\n# --- b. Preprocess and Cluster Industries ---\nprint(\"Preprocessing and clustering industries...\")\nindustry_feature_cols = [col for col in industry_features.columns if col != 'id8']\nX_industry = industry_features[industry_feature_cols].copy()\n\nimputer = SimpleImputer(strategy='median')\nX_industry_imputed = imputer.fit_transform(X_industry)\nscaler = StandardScaler()\nX_industry_scaled = scaler.fit_transform(X_industry_imputed)\n\npca = PCA(n_components=0.95)\nX_industry_pca = pca.fit_transform(X_industry_scaled)\nprint(f\"Industry PCA resulted in {pca.n_components_} components.\")\n\nkmeans_industry = KMeans(n_clusters=5, random_state=42, n_init='auto')\nindustry_features['industry_cluster'] = kmeans_industry.fit_predict(X_industry_pca)\nprint(\"Industry clusters created successfully.\")\n\n\n# =============================================================================\n# STAGE 4: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 4: Merging New Cluster Features and Saving ---\")\n\n# --- Create a mapping from offer_id to industry_cluster ---\noffer_meta_df['id8'] = offer_meta_df['id8'].astype(str)\nindustry_features['id8'] = industry_features['id8'].astype(str)\noffer_to_industry_cluster = pd.merge(offer_meta_df[['id3', 'id8']], industry_features[['id8', 'industry_cluster']], on='id8', how='left')\n\n# --- Merge all new features into train and holdout test sets ---\ndef enrich_with_entity_clusters(df, offer_features, offer_to_industry_cluster):\n    df['id3'] = df['id3'].astype(str)\n    # Merge offer clusters\n    df = pd.merge(df, offer_features[['id3', 'offer_cluster']], on='id3', how='left')\n    # Merge industry clusters (via the offer_id to industry cluster mapping)\n    df = pd.merge(df, offer_to_industry_cluster[['id3', 'industry_cluster']], on='id3', how='left')\n    return df\n\nprint(\"Enriching training data...\")\ndf_train_final = enrich_with_entity_clusters(df_train, offer_features, offer_to_industry_cluster)\nprint(\"Enriching hold-out test data...\")\ndf_holdout_test_final = enrich_with_entity_clusters(df_holdout_test, offer_features, offer_to_industry_cluster)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_entity_clusters.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_entity_clusters.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_holdout_test_final.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ENTITY CLUSTER FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:27:34.998485Z","iopub.execute_input":"2025-08-31T17:27:34.998704Z","iopub.status.idle":"2025-08-31T17:28:19.898762Z","shell.execute_reply.started":"2025-08-31T17:27:34.998688Z","shell.execute_reply":"2025-08-31T17:28:19.898141Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Data ---\nSuccessfully loaded all required data.\nInput training data shape: (166729, 393)\nInput hold-out test data shape: (55577, 393)\n\n--- STAGE 2: Creating Offer Clusters ---\nAggregating historical features for each offer...\nPreprocessing and clustering offers...\nOffer PCA resulted in 6 components.\nOffer clusters created successfully.\n\n--- STAGE 3: Creating Industry Clusters ---\nAggregating historical features for each industry...\nPreprocessing and clustering industries...\nIndustry PCA resulted in 4 components.\nIndustry clusters created successfully.\n\n--- STAGE 4: Merging New Cluster Features and Saving ---\nEnriching training data...\nEnriching hold-out test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_entity_clusters.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_entity_clusters.parquet\n\n--- ENTITY CLUSTER FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# df_train_final.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:28:19.899441Z","iopub.execute_input":"2025-08-31T17:28:19.899717Z","iopub.status.idle":"2025-08-31T17:28:19.904578Z","shell.execute_reply.started":"2025-08-31T17:28:19.899698Z","shell.execute_reply":"2025-08-31T17:28:19.903886Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(166729, 395)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# df_train_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:28:19.905302Z","iopub.execute_input":"2025-08-31T17:28:19.905622Z","iopub.status.idle":"2025-08-31T17:28:19.928872Z","shell.execute_reply.started":"2025-08-31T17:28:19.905584Z","shell.execute_reply":"2025-08-31T17:28:19.928146Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                              id1      id2       id3  \\\n0    1309455_654480_16-23_2023-11-01 05:29:18.734  1309455    654480   \n1  1277975_66290365_16-23_2023-11-01 10:00:06.073  1277975  66290365   \n2    1756831_783808_16-23_2023-11-02 07:45:41.794  1756831    783808   \n3     1839978_80893_16-23_2023-11-02 07:03:16.575  1839978     80893   \n4    1382370_331980_16-23_2023-11-03 07:18:32.167  1382370    331980   \n\n                       id4         id5  y    f1    f2    f3    f4  ...  \\\n0  2023-11-01 05:29:18.734  2023-11-01  0  None  None  None  None  ...   \n1  2023-11-01 10:00:06.073  2023-11-01  0   5.0  None  13.0  88.0  ...   \n2  2023-11-02 07:45:41.794  2023-11-02  0  None  None  None  None  ...   \n3  2023-11-02 07:03:16.575  2023-11-02  0  48.0  25.0  22.0  29.0  ...   \n4  2023-11-03 07:18:32.167  2023-11-03  0  None  40.0  None  None  ...   \n\n  profile_cluster engagement_cluster holistic_cluster  \\\n0               3                  1                1   \n1               3                  1                1   \n2               3                  0                0   \n3               2                  1                1   \n4               2                  0                0   \n\n  avg_offer_time_to_click_seconds credit_trans_count debit_trans_count  \\\n0                       45.174270            13298.0             707.0   \n1                       50.294750             9329.0            2141.0   \n2                       31.747560            60357.0             691.0   \n3                       28.859316            10744.0            1005.0   \n4                       19.150331            11439.0            2190.0   \n\n  avg_trans_hour offer_body_length offer_cluster industry_cluster  \n0       9.883684              30.0             2              1.0  \n1       8.055623              33.0             4              1.0  \n2      10.632175              30.0             2              1.0  \n3       8.604137              30.0             1              1.0  \n4       7.712818              34.0             2              1.0  \n\n[5 rows x 395 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id1</th>\n      <th>id2</th>\n      <th>id3</th>\n      <th>id4</th>\n      <th>id5</th>\n      <th>y</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>...</th>\n      <th>profile_cluster</th>\n      <th>engagement_cluster</th>\n      <th>holistic_cluster</th>\n      <th>avg_offer_time_to_click_seconds</th>\n      <th>credit_trans_count</th>\n      <th>debit_trans_count</th>\n      <th>avg_trans_hour</th>\n      <th>offer_body_length</th>\n      <th>offer_cluster</th>\n      <th>industry_cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1309455_654480_16-23_2023-11-01 05:29:18.734</td>\n      <td>1309455</td>\n      <td>654480</td>\n      <td>2023-11-01 05:29:18.734</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>45.174270</td>\n      <td>13298.0</td>\n      <td>707.0</td>\n      <td>9.883684</td>\n      <td>30.0</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1277975_66290365_16-23_2023-11-01 10:00:06.073</td>\n      <td>1277975</td>\n      <td>66290365</td>\n      <td>2023-11-01 10:00:06.073</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>5.0</td>\n      <td>None</td>\n      <td>13.0</td>\n      <td>88.0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>50.294750</td>\n      <td>9329.0</td>\n      <td>2141.0</td>\n      <td>8.055623</td>\n      <td>33.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1756831_783808_16-23_2023-11-02 07:45:41.794</td>\n      <td>1756831</td>\n      <td>783808</td>\n      <td>2023-11-02 07:45:41.794</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.747560</td>\n      <td>60357.0</td>\n      <td>691.0</td>\n      <td>10.632175</td>\n      <td>30.0</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1839978_80893_16-23_2023-11-02 07:03:16.575</td>\n      <td>1839978</td>\n      <td>80893</td>\n      <td>2023-11-02 07:03:16.575</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>48.0</td>\n      <td>25.0</td>\n      <td>22.0</td>\n      <td>29.0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>28.859316</td>\n      <td>10744.0</td>\n      <td>1005.0</td>\n      <td>8.604137</td>\n      <td>30.0</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1382370_331980_16-23_2023-11-03 07:18:32.167</td>\n      <td>1382370</td>\n      <td>331980</td>\n      <td>2023-11-03 07:18:32.167</td>\n      <td>2023-11-03</td>\n      <td>0</td>\n      <td>None</td>\n      <td>40.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19.150331</td>\n      <td>11439.0</td>\n      <td>2190.0</td>\n      <td>7.712818</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 395 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# import gc\n\n# # List of common large objects we created in previous steps\n# # Add any other large variables you might have created\n# variables_to_delete = [\n#     'df', 'df_test', 'X', 'y', 'groups', 'df_train', 'df_test_refined',\n#     'df_sample', 'X_cluster', 'X_scaled', 'X_pca', 'df_cluster', 'df_train_v9', 'df_test_v9',\n#     'rfm_df', 'trans_dd', 'event_dd', 'offer_meta_df', 'data_dict', 'df_train_final', 'df_test_final'\n# ]\n\n# print(\"--- Clearing RAM before training ---\")\n# for var_name in variables_to_delete:\n#     if var_name in globals():\n#         print(f\"Deleting: {var_name}\")\n#         del globals()[var_name]\n\n# # Call the garbage collector to release the memory\n# gc.collect()\n# print(\"RAM cleared successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T16:37:44.783263Z","iopub.execute_input":"2025-08-31T16:37:44.783994Z","iopub.status.idle":"2025-08-31T16:37:44.787471Z","shell.execute_reply.started":"2025-08-31T16:37:44.783970Z","shell.execute_reply":"2025-08-31T16:37:44.786657Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import xgboost as xgb\n# from sklearn.model_selection import GroupKFold\n# from sklearn.metrics import roc_auc_score\n# import warnings\n# import gc\n# import os\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n# # =============================================================================\n# # STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# # =============================================================================\n# print(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\n# try:\n#     # Load your final, pre-cleaned training and test data\n#     df = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n#     df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n#     print(\"Successfully loaded final training and test data.\")\n# except FileNotFoundError:\n#     print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n#     exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:04:25.268831Z","iopub.execute_input":"2025-07-20T06:04:25.269403Z","iopub.status.idle":"2025-07-20T06:04:44.967987Z","shell.execute_reply.started":"2025-07-20T06:04:25.269375Z","shell.execute_reply":"2025-07-20T06:04:44.967215Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\nSuccessfully loaded final training and test data.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# df = df.drop(columns=['time_to_click_seconds'], errors='ignore')\n# df_test = df_test.drop(columns=['time_to_click_seconds'], errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:11:40.082245Z","iopub.execute_input":"2025-07-20T06:11:40.082860Z","iopub.status.idle":"2025-07-20T06:11:45.157227Z","shell.execute_reply.started":"2025-07-20T06:11:40.082834Z","shell.execute_reply":"2025-07-20T06:11:45.156603Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import xgboost as xgb\n# from sklearn.model_selection import GroupKFold\n# from sklearn.metrics import roc_auc_score\n# import warnings\n# import gc\n# import os\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 0: DEFINE THE MAP@7 EVALUATION METRIC\n# # =============================================================================\n# def map_at_k(y_true, y_pred, group_ids, k=7):\n#     \"\"\"\n#     Calculates the Mean Average Precision at k.\n    \n#     Args:\n#         y_true (array-like): The true relevance labels (0 or 1).\n#         y_pred (array-like): The predicted ranking scores.\n#         group_ids (array-like): The group/query IDs for each sample.\n#         k (int): The cutoff for the precision calculation.\n        \n#     Returns:\n#         float: The Mean Average Precision @ k score.\n#     \"\"\"\n#     df = pd.DataFrame({'group': group_ids, 'y_true': y_true, 'y_pred': y_pred})\n    \n#     average_precisions = []\n#     for group in df['group'].unique():\n#         group_df = df[df['group'] == group].sort_values('y_pred', ascending=False).reset_index(drop=True)\n        \n#         # Get the top k predictions\n#         group_df = group_df.head(k)\n        \n#         if group_df['y_true'].sum() == 0:\n#             average_precisions.append(0)\n#             continue\n            \n#         relevant_hits = 0\n#         precision_at_i = []\n#         for i, row in group_df.iterrows():\n#             if row['y_true'] == 1:\n#                 relevant_hits += 1\n#                 precision_at_i.append(relevant_hits / (i + 1))\n        \n#         if not precision_at_i:\n#             average_precisions.append(0)\n#         else:\n#             average_precisions.append(np.mean(precision_at_i))\n            \n#     return np.mean(average_precisions)\n\n\n# # # =============================================================================\n# # # STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# # # =============================================================================\n# # print(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\n# # try:\n# #     # Load your final, pre-cleaned training and test data\n# #     df = pd.read_parquet(f'{WORKING_DIR}train_data_v92.parquet')\n# #     df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v92.parquet')\n# #     print(\"Successfully loaded final training (v92) and test (v92) data.\")\n# # except FileNotFoundError:\n# #     print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n# #     exit()\n\n# # --- Prepare Feature and Target Names ---\n# target_col = 'y'\n# # All columns are features except for identifiers and the target\n# feature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n# # Identify all categorical features to be one-hot encoded\n# categorical_features = [col for col in df.columns if 'cluster' in col or col == 'offer_type_code' or col == 'offer_type_x_weekend']\n\n# # --- Preprocessing ---\n# print(\"Preprocessing data for training...\")\n# # Handle one-hot encoding for categorical features\n# df = pd.get_dummies(df, columns=categorical_features, dummy_na=True)\n# df_test = pd.get_dummies(df_test, columns=categorical_features, dummy_na=True)\n\n# # Update feature list after one-hot encoding\n# for cat_col in categorical_features:\n#     if cat_col in feature_cols:\n#         feature_cols.remove(cat_col)\n# dummy_cols = [col for col in df.columns if any(cat_col in col for cat_col in categorical_features)]\n# feature_cols.extend(dummy_cols)\n# # Remove any duplicates that might have been added\n# feature_cols = list(dict.fromkeys(feature_cols)) \n\n# # Convert all feature columns to numeric before imputation\n# print(\"Converting all feature columns to numeric types...\")\n# for col in feature_cols:\n#     if col in df.columns:\n#         df[col] = pd.to_numeric(df[col], errors='coerce')\n#     if col in df_test.columns:\n#         df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n\n# # Final imputation for training data\n# df[feature_cols] = df[feature_cols].fillna(-999)\n\n# # --- Create Groups for XGBRanker ---\n# # A \"query\" or \"group\" is a single ranking task (all offers for one customer on one day)\n# df['group_id'] = df['id2'].astype(str) + '_' + df['id5'].astype(str)\n# df = df.sort_values('group_id').reset_index(drop=True)\n# group_sizes = df.groupby('group_id')['id1'].count().to_numpy()\n\n# # Define features (X) and target (y)\n# X = df[feature_cols]\n# y = df[target_col].astype(int)\n# print(f\"Prepared training data with {len(feature_cols)} features and {len(group_sizes)} groups.\")\n\n\n# # =============================================================================\n# # STAGE 2: TRAINING FINAL ENSEMBLE WITH XGBRANKER\n# # =============================================================================\n# print(\"\\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\")\n\n# # Define XGBRanker parameters. Note the 'rank:ndcg' objective.\n# final_params = {\n#     'objective': 'rank:ndcg', # Learning-to-rank objective\n#     'eval_metric': 'ndcg@7',  # Evaluate using a metric similar to the competition\n#     'use_label_encoder': False, 'seed': 42,\n#     'tree_method': 'gpu_hist', 'gpu_id': 0,\n#     'n_estimators': 1000, \n#     'learning_rate': 0.05, \n#     'max_depth': 7, \n#     'subsample': 0.8, \n#     'colsample_bytree': 0.8\n# }\n\n# N_SPLITS = 5 # Using 5 folds for a balance of stability and speed\n# gkf_train = GroupKFold(n_splits=N_SPLITS)\n# oof_predictions = np.zeros(len(df))\n# oof_map_scores = []\n\n# print(f\"Starting final training with {N_SPLITS}-Fold GroupKFold on GPU...\")\n# # For XGBRanker, we must use the customer ID for the GroupKFold split\n# groups_for_split = df['id2']\n\n# # MODIFIED: Corrected the variable name from 'for_split' to 'groups_for_split'\n# for fold, (train_idx, val_idx) in enumerate(gkf_train.split(X, y, groups=groups_for_split)):\n#     print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n#     X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n#     X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n    \n#     # Get the group sizes for the training and validation sets\n#     train_groups = df.iloc[train_idx].groupby('group_id')['id1'].count().to_numpy()\n#     val_groups = df.iloc[val_idx].groupby('group_id')['id1'].count().to_numpy()\n    \n#     model = xgb.XGBRanker(**final_params)\n#     model.fit(X_train_fold, y_train_fold, group=train_groups,\n#               eval_set=[(X_val_fold, y_val_fold)], eval_group=[val_groups],\n#               early_stopping_rounds=50, verbose=False)\n    \n#     # XGBRanker outputs scores, not probabilities\n#     val_preds = model.predict(X_val_fold)\n#     oof_predictions[val_idx] = val_preds\n\n#     # --- Calculate and store MAP@7 for this fold ---\n#     fold_map_score = map_at_k(y_val_fold, val_preds, df.iloc[val_idx]['group_id'], k=7)\n#     oof_map_scores.append(fold_map_score)\n#     print(f\"Fold {fold+1} MAP@7 Score: {fold_map_score:.5f}\")\n\n#     model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold+1}.json'\n#     model.save_model(model_path)\n#     print(f\"Model for fold {fold+1} saved to {model_path}\")\n#     del X_train_fold, y_train_fold, X_val_fold, y_val_fold, model\n#     gc.collect()\n\n# # --- Evaluate Overall Performance ---\n# print(f\"\\n--- Overall Cross-Validation Results ---\")\n# print(f\"Mean Out-of-Fold (OOF) MAP@7 Score: {np.mean(oof_map_scores):.5f}\")\n# print(\"This score is the most reliable estimate of your final leaderboard performance.\")\n\n\n# # =============================================================================\n# # STAGE 3: PREDICTION AND SUBMISSION FILE CREATION\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Generating Final Predictions ---\")\n# # Prepare test data\n# submission_ids = df_test[['id1', 'id2', 'id3', 'id5']].copy()\n# # Align test columns with the training columns\n# X_test = df_test.reindex(columns=X.columns, fill_value=0)\n# # Final imputation for test data\n# X_test = X_test.fillna(-999)\n\n# # Load models and predict\n# test_predictions = np.zeros(len(X_test))\n# for fold in range(1, N_SPLITS + 1):\n#     print(f\"Predicting with Fold {fold}/{N_SPLITS}...\")\n#     model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold}.json'\n#     model = xgb.XGBRanker()\n#     model.load_model(model_path)\n#     # Predict outputs ranking scores\n#     test_predictions += model.predict(X_test) / N_SPLITS\n\n# # Create submission file\n# print(\"\\nCreating submission file...\")\n# submission_df = submission_ids.copy()\n# submission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\n# cleaned_predictions = np.nan_to_num(test_predictions, nan=-999) # Use a low score for NaNs\n# submission_df['pred'] = cleaned_predictions\n\n# submission_path = f'{WORKING_DIR}r2_submission_final_ranker.csv'\n# submission_df.to_csv(submission_path, index=False)\n# print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n# print(\"\\nFirst 5 rows of the submission file:\")\n# print(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:15:41.002604Z","iopub.execute_input":"2025-07-20T06:15:41.002900Z","iopub.status.idle":"2025-07-20T06:32:41.484281Z","shell.execute_reply.started":"2025-07-20T06:15:41.002877Z","shell.execute_reply":"2025-07-20T06:32:41.483457Z"}},"outputs":[{"name":"stdout","text":"\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\nStarting final training with 5-Fold GroupKFold on GPU...\n--- Fold 1/5 ---\nFold 1 MAP@7 Score: 0.06066\nModel for fold 1 saved to /kaggle/working/final_ranker_model_fold_1.json\n--- Fold 2/5 ---\nFold 2 MAP@7 Score: 0.06649\nModel for fold 2 saved to /kaggle/working/final_ranker_model_fold_2.json\n--- Fold 3/5 ---\nFold 3 MAP@7 Score: 0.06375\nModel for fold 3 saved to /kaggle/working/final_ranker_model_fold_3.json\n--- Fold 4/5 ---\nFold 4 MAP@7 Score: 0.06472\nModel for fold 4 saved to /kaggle/working/final_ranker_model_fold_4.json\n--- Fold 5/5 ---\nFold 5 MAP@7 Score: 0.06350\nModel for fold 5 saved to /kaggle/working/final_ranker_model_fold_5.json\n\n--- Overall Cross-Validation Results ---\nMean Out-of-Fold (OOF) MAP@7 Score: 0.06382\nThis score is the most reliable estimate of your final leaderboard performance.\n\n--- STAGE 3: Generating Final Predictions ---\nPredicting with Fold 1/5...\nPredicting with Fold 2/5...\nPredicting with Fold 3/5...\nPredicting with Fold 4/5...\nPredicting with Fold 5/5...\n\nCreating submission file...\n\nSubmission file successfully saved to: /kaggle/working/r2_submission_final_ranker.csv\n\nFirst 5 rows of the submission file:\n                                               id1      id2     id3  \\\n0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n\n          id5      pred  \n0  11-04-2023 -1.690269  \n1  11-04-2023 -1.326085  \n2  11-05-2023  0.407397  \n3  11-04-2023 -2.410089  \n4  11-05-2023 -2.573299  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # =============================================================================\n# # SECTION 0: SETUP AND IMPORTS\n# # =============================================================================\n# print(\"--- Section 0: Setting up the environment ---\")\n# # Install the necessary libraries for graph and sequential embeddings\n# !pip install -q node2vec gensim\n\n# import pandas as pd\n# import numpy as np\n# import dask.dataframe as dd\n# import warnings\n# import gc\n# import os\n# import networkx as nx\n# from node2vec import Node2Vec\n# from gensim.models import Word2Vec\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 1: LOAD DATA\n# # =============================================================================\n# print(\"\\n--- Section 1: Loading Data ---\")\n# try:\n#     # Load your latest training and test sets\n#     df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n#     df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n#     # Use Dask for the large event log\n#     event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet', columns=['id2', 'id3', 'id4'])\n    \n#     print(\"Successfully loaded all required data.\")\n# except FileNotFoundError:\n#     print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n#     exit()\n\n# # =============================================================================\n# # TECHNIQUE 1: GRAPH EMBEDDINGS (NODEVEC)\n# # =============================================================================\n# print(\"\\n--- Technique 1: Creating Graph Embedding Features ---\")\n\n# # --- Step 1a: Build the Interaction Graph ---\n# print(\"Step 1a: Building the customer-offer interaction graph...\")\n# # Compute the edge list (customer, offer pairs) from the Dask DataFrame\n# # This is the most memory-intensive part of this stage\n# edge_list = event_dd[['id2', 'id3']].compute()\n# # Create a graph from the edge list\n# G = nx.from_pandas_edgelist(edge_list, 'id2', 'id3')\n# print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n\n# # --- Step 1b: Train the Node2Vec Model ---\n# print(\"Step 1b: Training the Node2Vec model (this will take a long time)...\")\n# # Initialize Node2Vec. Parameters can be tuned.\n# node2vec = Node2Vec(G, dimensions=32, walk_length=20, num_walks=10, workers=4, quiet=True)\n# # Train the model\n# model_n2v = node2vec.fit(window=5, min_count=1, batch_words=4)\n# print(\"Node2Vec model trained successfully.\")\n\n# # --- Step 1c: Extract Offer Embeddings ---\n# print(\"Step 1c: Extracting offer embeddings...\")\n# offer_ids = df_train['id3'].unique().tolist() + df_test['id3'].unique().tolist()\n# offer_ids = list(set(offer_ids)) # Get unique offer IDs across train and test\n\n# graph_embeddings = []\n# for offer_id in offer_ids:\n#     try:\n#         vector = model_n2v.wv[offer_id]\n#         graph_embeddings.append([offer_id] + vector.tolist())\n#     except KeyError:\n#         # This offer was not in the historical graph\n#         continue\n\n# graph_embedding_features = pd.DataFrame(graph_embeddings, columns=['id3'] + [f'graph_emb_{i}' for i in range(32)])\n# print(\"Graph embedding features created successfully.\")\n\n# # Clean up memory\n# del G, edge_list, model_n2v, node2vec\n# gc.collect()\n\n# # =============================================================================\n# # TECHNIQUE 2: SEQUENTIAL EMBEDDINGS (WORD2VEC)\n# # =============================================================================\n# print(\"\\n--- Technique 2: Creating Sequential Embedding Features ---\")\n\n# # --- Step 2a: Create Customer Sessions ---\n# print(\"Step 2a: Creating customer sessions...\")\n# event_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\n\n# # MODIFIED: Use a robust, Dask-native method to create time-based sessions\n# # Sort by customer and time\n# event_dd = event_dd.set_index('impression_time').sort_index()\n# event_dd = event_dd.reset_index()\n# event_dd = event_dd.set_index('id2').sort_index()\n\n# # Calculate time difference between consecutive events for each customer\n# event_dd['time_diff'] = event_dd.groupby('id2')['impression_time'].diff().dt.total_seconds()\n\n# # A new session starts if the time difference is > 30 minutes (1800 seconds)\n# event_dd['new_session'] = (event_dd['time_diff'] > 1800).fillna(True)\n# event_dd['session_id'] = event_dd.groupby('id2')['new_session'].cumsum()\n\n# # Create the list of sessions (sequences of offers)\n# sessions = event_dd.groupby(['id2', 'session_id'])['id3'].apply(list, meta=('id3', 'object')).compute().tolist()\n# print(f\"Created {len(sessions)} customer sessions.\")\n\n# # --- Step 2b: Train the Word2Vec Model ---\n# print(\"Step 2b: Training the Word2Vec model...\")\n# # Train the model on the sessions. Treats offers as \"words\" and sessions as \"sentences\".\n# model_w2v = Word2Vec(sentences=sessions, vector_size=32, window=5, min_count=1, workers=4)\n# print(\"Word2Vec model trained successfully.\")\n\n# # --- Step 2c: Extract Offer Embeddings ---\n# print(\"Step 2c: Extracting sequential embeddings...\")\n# sequential_embeddings = []\n# for offer_id in offer_ids:\n#     try:\n#         vector = model_w2v.wv[offer_id]\n#         sequential_embeddings.append([offer_id] + vector.tolist())\n#     except KeyError:\n#         continue\n\n# sequential_embedding_features = pd.DataFrame(sequential_embeddings, columns=['id3'] + [f'seq_emb_{i}' for i in range(32)])\n# print(\"Sequential embedding features created successfully.\")\n\n# # Clean up memory\n# del sessions, model_w2v\n# gc.collect()\n\n# # =============================================================================\n# # STAGE 3: MERGE NEW FEATURES AND SAVE\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v11) ---\")\n\n# def enrich_dataframe(df, graph_embedding_features, sequential_embedding_features):\n#     \"\"\"A reusable function to enrich a dataframe with new embedding features.\"\"\"\n#     df['id3'] = df['id3'].astype(str)\n#     graph_embedding_features['id3'] = graph_embedding_features['id3'].astype(str)\n#     sequential_embedding_features['id3'] = sequential_embedding_features['id3'].astype(str)\n    \n#     # Merge graph embedding features\n#     df = pd.merge(df, graph_embedding_features, on='id3', how='left')\n#     # Merge sequential embedding features\n#     df = pd.merge(df, sequential_embedding_features, on='id3', how='left')\n    \n#     return df\n\n# # --- Enrich both the training and test dataframes ---\n# print(\"Enriching training data...\")\n# df_train_final = enrich_dataframe(df_train, graph_embedding_features, sequential_embedding_features)\n# print(\"Enriching test data...\")\n# df_test_final = enrich_dataframe(df_test, graph_embedding_features, sequential_embedding_features)\n\n# # --- Save the final DataFrames ---\n# try:\n#     train_output_path = f'{WORKING_DIR}train_data_v16.parquet'\n#     test_output_path = f'{WORKING_DIR}test_data_v16.parquet'\n    \n#     df_train_final.to_parquet(train_output_path)\n#     df_test_final.to_parquet(test_output_path)\n    \n#     print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n#     print(f\"Successfully saved final test data to: {test_output_path}\")\n# except Exception as e:\n#     print(f\"\\nError saving the final files: {e}\")\n\n# print(\"\\n--- ADVANCED EMBEDDING FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T12:42:52.605810Z","iopub.execute_input":"2025-07-20T12:42:52.606100Z","execution_failed":"2025-07-20T14:00:04.747Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Graph Embedding Features ---\nStep 1a: Building the customer-offer interaction graph...\nGraph created with 429118 nodes and 12793562 edges.\nStep 1b: Training the Node2Vec model (this will take a long time)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # =============================================================================\n# # SECTION 0: SETUP AND IMPORTS\n# # =============================================================================\n# print(\"--- Section 0: Setting up the environment ---\")\n# # Install the necessary library for text embeddings\n# !pip install -q sentence-transformers\n\n# import pandas as pd\n# import numpy as np\n# import dask.dataframe as dd\n# import warnings\n# import gc\n# import os\n\n# from sentence_transformers import SentenceTransformer\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 1: LOAD DATA\n# # =============================================================================\n# print(\"\\n--- Section 1: Loading Data ---\")\n# try:\n#     # Load your latest training and test sets\n#     df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n#     df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n#     # Load supplementary data for feature creation\n#     offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n#     event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    \n#     print(\"Successfully loaded all required data.\")\n# except FileNotFoundError:\n#     print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n#     exit()\n\n# # =============================================================================\n# # TECHNIQUE 1: CO-CLICK & SEQUENTIAL FEATURES (MEMORY-EFFICIENT)\n# # =============================================================================\n# print(\"\\n--- Technique 1: Creating Co-Click Features ---\")\n\n# # --- Step 1a: Create Customer Sessions on a Sample ---\n# print(\"Step 1a: Creating customer sessions on a sample of the data...\")\n# # Use a large sample of the event data to make the process much faster\n# event_dd_sample = event_dd.sample(frac=0.15, random_state=42)\n# event_dd_sample['impression_time'] = dd.to_datetime(event_dd_sample['id4'], errors='coerce')\n\n# # Use a robust, Dask-native method to create time-based sessions\n# print(\"Sorting events by customer and time...\")\n# event_dd_sample = event_dd_sample.sort_values(['id2', 'impression_time'])\n\n# def calculate_time_diff(df):\n#     df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n#     return df\n\n# meta = event_dd_sample._meta.copy()\n# meta['time_diff'] = pd.Series(dtype='float64')\n# event_dd_sample = event_dd_sample.map_partitions(calculate_time_diff, meta=meta)\n\n# event_dd_sample['new_session'] = (event_dd_sample['time_diff'] > 1800).fillna(True)\n# event_dd_sample['session_id'] = event_dd_sample.groupby('id2')['new_session'].cumsum()\n# print(\"Customer sessions created.\")\n\n\n# # --- Step 1b: Calculate Co-occurrence (Fully with Dask) ---\n# print(\"Step 1b: Calculating offer co-occurrence within sessions using Dask...\")\n# # MODIFIED: Perform the entire co-occurrence calculation within Dask's lazy framework\n# session_offers_dd = event_dd_sample[['session_id', 'id2', 'id3']]\n# # Perform a Dask self-merge\n# merged_sessions_dd = dd.merge(session_offers_dd, session_offers_dd, on=['session_id', 'id2'])\n# # Filter out self-pairs\n# co_occurrence_dd = merged_sessions_dd[merged_sessions_dd['id3_x'] != merged_sessions_dd['id3_y']]\n# # Count how many times each pair appears and compute the final small result\n# co_occurrence_counts = co_occurrence_dd.groupby(['id3_x', 'id3_y']).size().compute().reset_index(name='count')\n# print(\"Co-occurrence calculated successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:18:38.776722Z","iopub.execute_input":"2025-07-20T14:18:38.777450Z","iopub.status.idle":"2025-07-20T14:20:09.798402Z","shell.execute_reply.started":"2025-07-20T14:18:38.777419Z","shell.execute_reply":"2025-07-20T14:20:09.797337Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n","output_type":"stream"},{"name":"stderr","text":"2025-07-20 14:18:58.503146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753021138.720674     210 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753021138.784423     210 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Co-Click Features ---\nStep 1a: Creating customer sessions on a sample of the data...\nSorting events by customer and time...\nCustomer sessions created.\nStep 1b: Calculating offer co-occurrence within sessions using Dask...\nCo-occurrence calculated successfully.\nStep 1c: Creating final co-click features...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Projection' object has no attribute 'notna'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;31m# (Making sure to convert to/from Expr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"{err}\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Projection' object has no attribute 'notna'\n\nThis often means that you are attempting to use an unsupported API function. Current API coverage is documented here: https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_210/3417018196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 1c: Creating final co-click features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Calculate CTR more efficiently using Dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mevent_dd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clicked'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_dd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0moffer_ctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_dd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clicked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# Raise original error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;31m# Prioritize `FrameBase` attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'notna'"],"ename":"AttributeError","evalue":"'Series' object has no attribute 'notna'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# # --- Step 1c: Create Co-Click Features ---\n# print(\"Step 1c: Creating final co-click features...\")\n# # MODIFIED: Changed .notna() to the Dask-compatible ~.isnull() to fix the AttributeError\n# event_dd['clicked'] = (~event_dd['id7'].isnull()).astype(int)\n# offer_ctr = event_dd.groupby('id3')['clicked'].mean().compute().to_dict()\n\n# # For each offer, find its most frequent partner\n# co_occurrence_counts = co_occurrence_counts.sort_values('count', ascending=False)\n# most_frequent_partner = co_occurrence_counts.drop_duplicates(subset=['id3_x'], keep='first')\n\n# # Get the CTR of that most frequent partner\n# most_frequent_partner['partner_ctr'] = most_frequent_partner['id3_y'].map(offer_ctr)\n# co_click_features = most_frequent_partner[['id3_x', 'partner_ctr']].rename(columns={'id3_x': 'id3', 'partner_ctr': 'most_frequent_partner_ctr'})\n# print(\"Co-click features created successfully.\")\n\n# # Clean up memory\n# del event_dd, event_dd_sample, co_occurrence_counts\n# gc.collect()\n\n# # =============================================================================\n# # TECHNIQUE 2: TEXT EMBEDDINGS\n# # =============================================================================\n# print(\"\\n--- Technique 2: Creating Text Embedding Features ---\")\n\n# # --- Step 2a: Prepare the Text Corpus ---\n# print(\"Step 2a: Preparing text corpus from offer metadata...\")\n# offer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\n# # Combine text fields, handling missing values\n# offer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\n# offer_texts = offer_meta_df[['id3', 'full_text']].copy()\n# print(\"Text corpus prepared.\")\n\n# # --- Step 2b: Generate Embeddings ---\n# print(\"Step 2b: Generating text embeddings (this may take a moment)...\")\n# # Use a lightweight but powerful pre-trained model\n# model = SentenceTransformer('all-MiniLM-L6-v2')\n# # Generate the embeddings\n# embeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\n# print(f\"Embeddings generated with shape: {embeddings.shape}\")\n\n# # Create a DataFrame from the embeddings\n# embedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\n# text_embedding_features = pd.concat([offer_texts[['id3']], embedding_df], axis=1)\n# print(\"Text embedding features created successfully.\")\n\n\n# # =============================================================================\n# # STAGE 3: MERGE NEW FEATURES AND SAVE\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v10) ---\")\n\n# def enrich_dataframe(df, co_click_features, text_embedding_features):\n#     \"\"\"A reusable function to enrich a dataframe with new features.\"\"\"\n#     df['id3'] = df['id3'].astype(str)\n#     co_click_features['id3'] = co_click_features['id3'].astype(str)\n#     text_embedding_features['id3'] = text_embedding_features['id3'].astype(str)\n    \n#     # Merge co-click features\n#     df = pd.merge(df, co_click_features, on='id3', how='left')\n#     # Merge text embedding features\n#     df = pd.merge(df, text_embedding_features, on='id3', how='left')\n    \n#     return df\n\n# # --- Enrich both the training and test dataframes ---\n# print(\"Enriching training data...\")\n# df_train_final = enrich_dataframe(df_train, co_click_features, text_embedding_features)\n# print(\"Enriching test data...\")\n# df_test_final = enrich_dataframe(df_test, co_click_features, text_embedding_features)\n\n# # --- Save the final DataFrames ---\n# try:\n#     train_output_path = f'{WORKING_DIR}train_data_v16.parquet'\n#     test_output_path = f'{WORKING_DIR}test_data_v16.parquet'\n    \n#     df_train_final.to_parquet(train_output_path)\n#     df_test_final.to_parquet(test_output_path)\n    \n#     print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n#     print(f\"Successfully saved final test data to: {test_output_path}\")\n# except Exception as e:\n#     print(f\"\\nError saving the final files: {e}\")\n\n# print(\"\\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:21:41.906874Z","iopub.execute_input":"2025-07-20T14:21:41.907577Z","iopub.status.idle":"2025-07-20T14:23:08.914152Z","shell.execute_reply.started":"2025-07-20T14:21:41.907543Z","shell.execute_reply":"2025-07-20T14:23:08.913210Z"}},"outputs":[{"name":"stdout","text":"Step 1c: Creating final co-click features...\nCo-click features created successfully.\n\n--- Technique 2: Creating Text Embedding Features ---\nStep 2a: Preparing text corpus from offer metadata...\nText corpus prepared.\nStep 2b: Generating text embeddings (this may take a moment)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c99345d5edd45d0849689f8d080d400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f02e3597f034c7b96798b71cd9a8fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99cf9fc10e064bcf9b7fbd78fb704dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfe314a4bd746ba828d0c80e22f1a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c92f10699a94bdfa8f2c7b9e13d4d3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9faa12e7661e4c6dadcfbcfc104df782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34470221253e4a64b3b31c5caceff72a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f00e5c94f549878c6b09751409298a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb51d0789aa6474aab19b600cce7e7a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3de03cd09d4132bb3797985dcaecab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175fd54f03184c03b5c77779f7f91646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9caa6d6dcca4b9eb93c2ec6f9affa82"}},"metadata":{}},{"name":"stdout","text":"Embeddings generated with shape: (4164, 384)\nText embedding features created successfully.\n\n--- STAGE 3: Merging New Features and Saving Final Datasets (v10) ---\nEnriching training data...\nEnriching test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v16.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v16.parquet\n\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# df_train_final.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:24:10.055774Z","iopub.execute_input":"2025-07-20T14:24:10.056182Z","iopub.status.idle":"2025-07-20T14:24:10.061416Z","shell.execute_reply.started":"2025-07-20T14:24:10.056156Z","shell.execute_reply":"2025-07-20T14:24:10.060790Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(770164, 788)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# import gc\n\n# # List of large DataFrames and variables created in the previous script\n# variables_to_delete = [\n#     'df_train', 'df_test', 'offer_meta_df', 'event_dd', \n#     'event_dd_sample', 'session_offers', 'merged_sessions', \n#     'co_occurrence', 'co_occurrence_counts', 'event_df_pd', \n#     'offer_ctr', 'most_frequent_partner', 'co_click_features',\n#     'offer_texts', 'embeddings', 'embedding_df', \n#     'text_embedding_features', 'df_train_final', 'df_test_final'\n# ]\n\n# print(\"--- Clearing RAM ---\")\n# for var_name in variables_to_delete:\n#     # Check if the variable exists in the notebook's memory before trying to delete it\n#     if var_name in globals():\n#         print(f\"Deleting: {var_name}\")\n#         del globals()[var_name]\n\n# # Call the garbage collector to release the memory\n# gc.collect()\n# print(\"RAM cleared successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:29.465627Z","iopub.execute_input":"2025-07-20T14:40:29.466299Z","iopub.status.idle":"2025-07-20T14:40:32.494368Z","shell.execute_reply.started":"2025-07-20T14:40:29.466264Z","shell.execute_reply":"2025-07-20T14:40:32.493537Z"}},"outputs":[{"name":"stdout","text":"--- Clearing RAM ---\nDeleting: df_train\nDeleting: df_test\nDeleting: offer_meta_df\nDeleting: offer_ctr\nDeleting: most_frequent_partner\nDeleting: co_click_features\nDeleting: offer_texts\nDeleting: embeddings\nDeleting: df_train_final\nDeleting: df_test_final\nRAM cleared successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# # =============================================================================\n# # SECTION 0: SETUP AND IMPORTS\n# # =============================================================================\n# print(\"--- Section 0: Setting up the environment ---\")\n# # Install the necessary library for text embeddings\n# !pip install -q sentence-transformers\n\n# import pandas as pd\n# import numpy as np\n# import dask.dataframe as dd\n# import warnings\n# import gc\n# import os\n\n# from sentence_transformers import SentenceTransformer\n# from sklearn.metrics.pairwise import cosine_similarity\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 1: LOAD DATA\n# # =============================================================================\n# print(\"\\n--- Section 1: Loading Data ---\")\n# try:\n#     # Load your latest training and test sets\n#     df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n#     df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n#     # Load supplementary data for feature creation\n#     offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n#     event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    \n#     print(\"Successfully loaded all required data.\")\n# except FileNotFoundError:\n#     print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n#     exit()\n\n# # =============================================================================\n# # TECHNIQUE 1: ADVANCED CO-CLICK FEATURES\n# # =============================================================================\n# print(\"\\n--- Technique 1: Creating Advanced Co-Click Features ---\")\n\n# # --- Step 1a: Create Customer Sessions on a Sample ---\n# print(\"Step 1a: Creating customer sessions on a sample of the data...\")\n# event_dd_sample = event_dd.sample(frac=0.2, random_state=42)\n# event_dd_sample['impression_time'] = dd.to_datetime(event_dd_sample['id4'], errors='coerce')\n# event_dd_sample = event_dd_sample.sort_values(['id2', 'impression_time'])\n\n# def calculate_time_diff(df):\n#     df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n#     return df\n\n# meta = event_dd_sample._meta.copy()\n# meta['time_diff'] = pd.Series(dtype='float64')\n# event_dd_sample = event_dd_sample.map_partitions(calculate_time_diff, meta=meta)\n\n# event_dd_sample['new_session'] = (event_dd_sample['time_diff'] > 1800).fillna(True)\n# event_dd_sample['session_id'] = event_dd_sample.groupby('id2')['new_session'].cumsum()\n# print(\"Customer sessions created.\")\n\n\n# # --- Step 1b: Calculate Co-occurrence ---\n# print(\"Step 1b: Calculating offer co-occurrence within sessions...\")\n# session_offers_dd = event_dd_sample[['session_id', 'id2', 'id3']]\n# merged_sessions_dd = dd.merge(session_offers_dd, session_offers_dd, on=['session_id', 'id2'])\n# co_occurrence_dd = merged_sessions_dd[merged_sessions_dd['id3_x'] != merged_sessions_dd['id3_y']]\n# co_occurrence_counts = co_occurrence_dd.groupby(['id3_x', 'id3_y']).size().compute().reset_index(name='count')\n# print(\"Co-occurrence calculated successfully.\")\n\n\n# # --- Step 1c: Create Advanced Co-Click Features ---\n# print(\"Step 1c: Creating final co-click features...\")\n# event_dd['clicked'] = (~event_dd['id7'].isnull()).astype(int)\n# offer_ctr = event_dd.groupby('id3')['clicked'].mean().compute().to_dict()\n\n# # Get the CTR of each partner offer\n# co_occurrence_counts['partner_ctr'] = co_occurrence_counts['id3_y'].map(offer_ctr)\n# co_occurrence_counts = co_occurrence_counts.sort_values(['id3_x', 'count'], ascending=[True, False])\n\n# # Find the top 5 partners for each offer\n# top_5_partners = co_occurrence_counts.groupby('id3_x').head(5)\n\n# # Calculate aggregate stats for the top 5 partners' CTRs\n# co_click_agg = top_5_partners.groupby('id3_x')['partner_ctr'].agg(['mean', 'max', 'std']).reset_index()\n# co_click_features = co_click_agg.rename(columns={\n#     'id3_x': 'id3',\n#     'mean': 'top5_partner_mean_ctr',\n#     'max': 'top5_partner_max_ctr',\n#     'std': 'top5_partner_std_ctr'\n# })\n# print(\"Advanced co-click features created successfully.\")\n\n# # Clean up memory\n# del event_dd, event_dd_sample, co_occurrence_counts\n# gc.collect()\n\n# # =============================================================================\n# # TECHNIQUE 2: TEXT SIMILARITY FEATURES\n# # =============================================================================\n# print(\"\\n--- Technique 2: Creating Text Similarity Features ---\")\n\n# # --- Step 2a: Prepare the Text Corpus and Generate Embeddings ---\n# print(\"Step 2a: Preparing text corpus and generating embeddings...\")\n# offer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\n# offer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\n# offer_texts = offer_meta_df[['id3', 'id8', 'full_text']].copy()\n\n# model = SentenceTransformer('all-MiniLM-L6-v2')\n# embeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\n# embedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\n# offer_embeddings_df = pd.concat([offer_texts[['id3', 'id8']], embedding_df], axis=1)\n# print(\"Text embeddings generated.\")\n\n# # --- Step 2b: Create Industry DNA and Calculate Similarity ---\n# print(\"Step 2b: Calculating offer-industry text similarity...\")\n# # Calculate the average embedding vector for each industry\n# industry_dna = offer_embeddings_df.groupby('id8').mean(numeric_only=True).reset_index()\n# industry_dna = industry_dna.rename(columns={col: f'industry_{col}' for col in industry_dna.columns if 'emb' in col})\n\n# # Merge the industry DNA back to each offer\n# offer_embeddings_df = pd.merge(offer_embeddings_df, industry_dna, on='id8', how='left')\n\n# # Calculate the cosine similarity between each offer's embedding and its industry's average embedding\n# emb_cols = [f'text_emb_{i}' for i in range(embeddings.shape[1])]\n# industry_emb_cols = [f'industry_text_emb_{i}' for i in range(embeddings.shape[1])]\n\n# # This is a memory-intensive step, so we process in chunks if needed\n# # For now, we attempt it directly as the dataframe is moderately sized\n# offer_embeddings_df['offer_industry_text_similarity'] = [\n#     cosine_similarity(row[emb_cols].values.reshape(1, -1), row[industry_emb_cols].values.reshape(1, -1))[0][0]\n#     if not row[industry_emb_cols].isnull().any() else np.nan\n#     for index, row in offer_embeddings_df.iterrows()\n# ]\n# text_similarity_features = offer_embeddings_df[['id3', 'offer_industry_text_similarity']]\n# print(\"Text similarity features created successfully.\")\n\n\n# # =============================================================================\n# # STAGE 3: MERGE NEW FEATURES AND SAVE\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v17) ---\")\n\n# def enrich_dataframe(df, co_click_features, text_similarity_features):\n#     \"\"\"A reusable function to enrich a dataframe with new features.\"\"\"\n#     df['id3'] = df['id3'].astype(str)\n#     co_click_features['id3'] = co_click_features['id3'].astype(str)\n#     text_similarity_features['id3'] = text_similarity_features['id3'].astype(str)\n    \n#     # Merge co-click features\n#     df = pd.merge(df, co_click_features, on='id3', how='left')\n#     # Merge text similarity features\n#     df = pd.merge(df, text_similarity_features, on='id3', how='left')\n    \n#     return df\n\n# # --- Enrich both the training and test dataframes ---\n# print(\"Enriching training data...\")\n# df_train_final = enrich_dataframe(df_train, co_click_features, text_similarity_features)\n# print(\"Enriching test data...\")\n# df_test_final = enrich_dataframe(df_test, co_click_features, text_similarity_features)\n\n# # --- Save the final DataFrames ---\n# try:\n#     train_output_path = f'{WORKING_DIR}train_data_v18.parquet'\n#     test_output_path = f'{WORKING_DIR}test_data_v18.parquet'\n    \n#     df_train_final.to_parquet(train_output_path)\n#     df_test_final.to_parquet(test_output_path)\n    \n#     print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n#     print(f\"Successfully saved final test data to: {test_output_path}\")\n# except Exception as e:\n#     print(f\"\\nError saving the final files: {e}\")\n\n# print(\"\\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:47:27.253980Z","iopub.execute_input":"2025-07-20T16:47:27.254571Z","iopub.status.idle":"2025-07-20T16:52:11.888021Z","shell.execute_reply.started":"2025-07-20T16:47:27.254546Z","shell.execute_reply":"2025-07-20T16:52:11.887114Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-07-20 16:48:57.174338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753030137.348404      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753030137.396031      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Advanced Co-Click Features ---\nStep 1a: Creating customer sessions on a sample of the data...\nCustomer sessions created.\nStep 1b: Calculating offer co-occurrence within sessions...\nCo-occurrence calculated successfully.\nStep 1c: Creating final co-click features...\nAdvanced co-click features created successfully.\n\n--- Technique 2: Creating Text Similarity Features ---\nStep 2a: Preparing text corpus and generating embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c12cd61fb23048e0812954294808ad23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69ea86edd1e413da9130e43562785c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf60185ed8c0499cbc48931165ff6f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef11b224116a4e529a45baaa1e7615c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115642c847db42ebb45136bee283336a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"251465aa9e1c478fa2803e765cd415b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d7da5a658b49f1a900ef8feb9ef6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc6cb498e7b44509b9f367368ccbc2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98559d142ea04556b9725c4507fff6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fd83c5fdb44f7f9f57ddd5c8dff70e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4670b9e8a246d8ba19fa454939d959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80d066006ee44af8a781ac7877c28b6"}},"metadata":{}},{"name":"stdout","text":"Text embeddings generated.\nStep 2b: Calculating offer-industry text similarity...\nText similarity features created successfully.\n\n--- STAGE 3: Merging New Features and Saving Final Datasets (v17) ---\nEnriching training data...\nEnriching test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v18.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v18.parquet\n\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df_train_final.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import xgboost as xgb\n# from sklearn.model_selection import GroupKFold\n# from sklearn.metrics import roc_auc_score\n# import warnings\n# import gc\n# import os\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 0: DEFINE THE MAP@7 EVALUATION METRIC\n# # =============================================================================\n# def map_at_k(y_true, y_pred, group_ids, k=7):\n#     \"\"\"\n#     Calculates the Mean Average Precision at k.\n    \n#     Args:\n#         y_true (array-like): The true relevance labels (0 or 1).\n#         y_pred (array-like): The predicted ranking scores.\n#         group_ids (array-like): The group/query IDs for each sample.\n#         k (int): The cutoff for the precision calculation.\n        \n#     Returns:\n#         float: The Mean Average Precision @ k score.\n#     \"\"\"\n#     df = pd.DataFrame({'group': group_ids, 'y_true': y_true, 'y_pred': y_pred})\n    \n#     average_precisions = []\n#     for group in df['group'].unique():\n#         group_df = df[df['group'] == group].sort_values('y_pred', ascending=False).reset_index(drop=True)\n        \n#         # Get the top k predictions\n#         group_df = group_df.head(k)\n        \n#         if group_df['y_true'].sum() == 0:\n#             average_precisions.append(0)\n#             continue\n            \n#         relevant_hits = 0\n#         precision_at_i = []\n#         for i, row in group_df.iterrows():\n#             if row['y_true'] == 1:\n#                 relevant_hits += 1\n#                 precision_at_i.append(relevant_hits / (i + 1))\n        \n#         if not precision_at_i:\n#             average_precisions.append(0)\n#         else:\n#             average_precisions.append(np.mean(precision_at_i))\n            \n#     return np.mean(average_precisions)\n\n\n# # =============================================================================\n# # STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# # =============================================================================\n# print(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\n# try:\n#     # Load your final, pre-cleaned training and test data\n#     df = pd.read_parquet(f'{WORKING_DIR}train_data_v18.parquet')\n#     df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v18.parquet')\n#     print(\"Successfully loaded final training (v92) and test (v92) data.\")\n# except FileNotFoundError:\n#     print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n#     exit()\n\n# # --- Memory Reduction Function ---\n# def reduce_mem_usage(df):\n#     \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n#     start_mem = df.memory_usage().sum() / 1024**2\n#     print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n    \n#     for col in df.columns:\n#         col_type = df[col].dtype\n#         if str(col_type)[:3] == 'int' or str(col_type)[:5] == 'float':\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)\n#             else:\n#                 if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)\n    \n#     end_mem = df.memory_usage().sum() / 1024**2\n#     print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n#     print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n#     return df\n\n# # --- Prepare Feature and Target Names ---\n# target_col = 'y'\n# # All columns are features except for identifiers and the target\n# feature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n# # Identify all categorical features to be one-hot encoded\n# categorical_features = [col for col in df.columns if 'cluster' in col or col == 'offer_type_code' or col == 'offer_type_x_weekend']\n\n# # --- Preprocessing ---\n# print(\"Preprocessing data for training...\")\n# # Handle one-hot encoding for categorical features\n# df = pd.get_dummies(df, columns=categorical_features, dummy_na=True)\n# df_test = pd.get_dummies(df_test, columns=categorical_features, dummy_na=True)\n\n# # Update feature list after one-hot encoding\n# for cat_col in categorical_features:\n#     if cat_col in feature_cols:\n#         feature_cols.remove(cat_col)\n# dummy_cols = [col for col in df.columns if any(cat_col in col for cat_col in categorical_features)]\n# feature_cols.extend(dummy_cols)\n# # Remove any duplicates that might have been added\n# feature_cols = list(dict.fromkeys(feature_cols)) \n\n# # Convert all feature columns to numeric before imputation\n# print(\"Converting all feature columns to numeric types...\")\n# for col in feature_cols:\n#     if col in df.columns:\n#         df[col] = pd.to_numeric(df[col], errors='coerce')\n#     if col in df_test.columns:\n#         df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n\n# # Final imputation for training data\n# df[feature_cols] = df[feature_cols].fillna(-999)\n\n# # MODIFIED: Move memory reduction to AFTER all preprocessing\n# print(\"\\nOptimizing memory usage for training data...\")\n# df = reduce_mem_usage(df)\n# print(\"\\nOptimizing memory usage for test data...\")\n# df_test = reduce_mem_usage(df_test)\n\n# # --- Create Groups for XGBRanker ---\n# # A \"query\" or \"group\" is a single ranking task (all offers for one customer on one day)\n# df['group_id'] = df['id2'].astype(str) + '_' + df['id5'].astype(str)\n# df = df.sort_values('group_id').reset_index(drop=True)\n# group_sizes = df.groupby('group_id')['id1'].count().to_numpy()\n\n# # Define features (X) and target (y)\n# X = df[feature_cols]\n# y = df[target_col].astype(int)\n# print(f\"Prepared training data with {len(feature_cols)} features and {len(group_sizes)} groups.\")\n\n\n# # =============================================================================\n# # STAGE 2: TRAINING FINAL ENSEMBLE WITH XGBRANKER\n# # =============================================================================\n# print(\"\\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\")\n\n# # Define XGBRanker parameters. Note the 'rank:ndcg' objective.\n# final_params = {\n#     'objective': 'rank:ndcg', # Learning-to-rank objective\n#     'eval_metric': 'ndcg@7',  # Evaluate using a metric similar to the competition\n#     'use_label_encoder': False, 'seed': 42,\n#     'tree_method': 'gpu_hist', 'gpu_id': 0,\n#     'n_estimators': 2000, \n#     'learning_rate': 0.05, \n#     'max_depth': 8, \n#     'subsample': 0.8, \n#     'colsample_bytree': 0.8,\n#     'max_bin': 128 # MODIFIED: Add max_bin to reduce memory usage during training\n# }\n\n# N_SPLITS = 3 # Using 5 folds for a balance of stability and speed\n# gkf_train = GroupKFold(n_splits=N_SPLITS)\n# oof_predictions = np.zeros(len(df))\n# oof_map_scores = []\n\n# print(f\"Starting final training with {N_SPLITS}-Fold GroupKFold on GPU...\")\n# # For XGBRanker, we must use the customer ID for the GroupKFold split\n# groups_for_split = df['id2']\n\n# for fold, (train_idx, val_idx) in enumerate(gkf_train.split(X, y, groups=groups_for_split)):\n#     print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n#     X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n#     X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n    \n#     # Get the group sizes for the training and validation sets\n#     train_groups = df.iloc[train_idx].groupby('group_id')['id1'].count().to_numpy()\n#     val_groups = df.iloc[val_idx].groupby('group_id')['id1'].count().to_numpy()\n    \n#     model = xgb.XGBRanker(**final_params)\n#     model.fit(X_train_fold, y_train_fold, group=train_groups,\n#               eval_set=[(X_val_fold, y_val_fold)], eval_group=[val_groups],\n#               early_stopping_rounds=50, verbose=False)\n    \n#     # XGBRanker outputs scores, not probabilities\n#     val_preds = model.predict(X_val_fold)\n#     oof_predictions[val_idx] = val_preds\n\n#     # --- Calculate and store MAP@7 for this fold ---\n#     fold_map_score = map_at_k(y_val_fold, val_preds, df.iloc[val_idx]['group_id'], k=7)\n#     oof_map_scores.append(fold_map_score)\n#     print(f\"Fold {fold+1} MAP@7 Score: {fold_map_score:.5f}\")\n\n#     model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold+1}.json'\n#     model.save_model(model_path)\n#     print(f\"Model for fold {fold+1} saved to {model_path}\")\n#     del X_train_fold, y_train_fold, X_val_fold, y_val_fold, model\n#     gc.collect()\n\n# # --- Evaluate Overall Performance ---\n# print(f\"\\n--- Overall Cross-Validation Results ---\")\n# print(f\"Mean Out-of-Fold (OOF) MAP@7 Score: {np.mean(oof_map_scores):.5f}\")\n# print(\"This score is the most reliable estimate of your final leaderboard performance.\")\n\n\n# # =============================================================================\n# # STAGE 3: PREDICTION AND SUBMISSION FILE CREATION\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Generating Final Predictions ---\")\n# # Prepare test data\n# submission_ids = df_test[['id1', 'id2', 'id3', 'id5']].copy()\n# # Align test columns with the training columns\n# X_test = df_test.reindex(columns=X.columns, fill_value=0)\n# # Final imputation for test data\n# X_test = X_test.fillna(-999)\n\n# # Load models and predict\n# test_predictions = np.zeros(len(X_test))\n# for fold in range(1, N_SPLITS + 1):\n#     print(f\"Predicting with Fold {fold}/{N_SPLITS}...\")\n#     model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold}.json'\n#     model = xgb.XGBRanker()\n#     model.load_model(model_path)\n#     # Predict outputs ranking scores\n#     test_predictions += model.predict(X_test) / N_SPLITS\n\n# # Create submission file\n# print(\"\\nCreating submission file...\")\n# submission_df = submission_ids.copy()\n# submission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\n# cleaned_predictions = np.nan_to_num(test_predictions, nan=-999) # Use a low score for NaNs\n# submission_df['pred'] = cleaned_predictions\n\n# submission_path = f'{WORKING_DIR}r2_submission_final_ranker18.csv'\n# submission_df.to_csv(submission_path, index=False)\n# print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n# print(\"\\nFirst 5 rows of the submission file:\")\n# print(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:52:11.889540Z","iopub.execute_input":"2025-07-20T16:52:11.889919Z","iopub.status.idle":"2025-07-20T17:18:35.917508Z","shell.execute_reply.started":"2025-07-20T16:52:11.889895Z","shell.execute_reply":"2025-07-20T17:18:35.915585Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\nSuccessfully loaded final training (v92) and test (v92) data.\nPreprocessing data for training...\nConverting all feature columns to numeric types...\n\nOptimizing memory usage for training data...\nMemory usage of dataframe is 2367.98 MB\nMemory usage after optimization is: 1217.04 MB\nDecreased by 48.6%\n\nOptimizing memory usage for test data...\nMemory usage of dataframe is 1132.30 MB\nMemory usage after optimization is: 600.84 MB\nDecreased by 46.9%\nPrepared training data with 439 features and 52468 groups.\n\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\nStarting final training with 3-Fold GroupKFold on GPU...\n--- Fold 1/3 ---\nFold 1 MAP@7 Score: 0.06282\nModel for fold 1 saved to /kaggle/working/final_ranker_model_fold_1.json\n--- Fold 2/3 ---\nFold 2 MAP@7 Score: 0.06610\nModel for fold 2 saved to /kaggle/working/final_ranker_model_fold_2.json\n--- Fold 3/3 ---\nFold 3 MAP@7 Score: 0.06171\nModel for fold 3 saved to /kaggle/working/final_ranker_model_fold_3.json\n\n--- Overall Cross-Validation Results ---\nMean Out-of-Fold (OOF) MAP@7 Score: 0.06354\nThis score is the most reliable estimate of your final leaderboard performance.\n\n--- STAGE 3: Generating Final Predictions ---\nPredicting with Fold 1/3...\nPredicting with Fold 2/3...\nPredicting with Fold 3/3...\n\nCreating submission file...\n\nSubmission file successfully saved to: /kaggle/working/r2_submission_final_ranker18.csv\n\nFirst 5 rows of the submission file:\n                                               id1      id2     id3  \\\n0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n\n          id5      pred  \n0  11-04-2023 -1.411453  \n1  11-04-2023 -1.487017  \n2  11-05-2023  0.144096  \n3  11-04-2023 -2.434480  \n4  11-05-2023 -2.545596  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.preprocessing import MinMaxScaler\n# import warnings\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 1: LOAD THE SUBMISSION FILE\n# # =============================================================================\n# print(\"--- STAGE 1: Loading the submission file ---\")\n# try:\n#     submission_path = f'{WORKING_DIR}r2_submission_final_ranker18.csv'\n#     df_sub = pd.read_csv(submission_path)\n#     print(\"Successfully loaded submission file.\")\n#     print(\"\\nOriginal 'pred' column stats:\")\n#     print(df_sub['pred'].describe())\n# except FileNotFoundError:\n#     print(f\"Error: {submission_path} not found.\")\n#     print(\"Please ensure you have run the prediction script successfully.\")\n#     exit()\n\n# # =============================================================================\n# # STAGE 2: NORMALIZE THE PREDICTION COLUMN\n# # =============================================================================\n# print(\"\\n--- STAGE 2: Normalizing the 'pred' column ---\")\n\n# # Initialize the Min-Max Scaler to scale values between 0 and 1\n# scaler = MinMaxScaler()\n\n# # The scaler expects a 2D array, so we need to reshape the column\n# # We fit and transform in one step\n# df_sub['pred_normalized'] = scaler.fit_transform(df_sub[['pred']])\n\n# print(\"\\n'pred' column has been normalized.\")\n# print(\"\\nNew 'pred_normalized' column stats:\")\n# print(df_sub['pred_normalized'].describe())\n\n\n# # =============================================================================\n# # STAGE 3: CREATE AND SAVE THE NORMALIZED SUBMISSION FILE\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Saving the normalized submission file ---\")\n\n# # Create the final dataframe, dropping the old prediction column\n# df_final_sub = df_sub[['id1', 'id2', 'id3', 'id5']].copy()\n# df_final_sub['pred'] = df_sub['pred_normalized']\n\n# # Define the output path\n# output_path = f'{WORKING_DIR}r2_submission_final_ranker_normalized18.csv'\n\n# try:\n#     df_final_sub.to_csv(output_path, index=False)\n#     print(f\"\\nSuccessfully saved normalized submission file to: {output_path}\")\n#     print(\"\\nFirst 5 rows of the new submission file:\")\n#     print(df_final_sub.head())\n# except Exception as e:\n#     print(f\"\\nError saving the file: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # =============================================================================\n# # SECTION 0: SETUP AND IMPORTS\n# # =============================================================================\n# print(\"--- Section 0: Setting up the environment ---\")\n# # Install the necessary libraries for graph and sequential embeddings\n# !pip install -q node2vec gensim\n\n# import pandas as pd\n# import numpy as np\n# import dask.dataframe as dd\n# import warnings\n# import gc\n# import os\n# import networkx as nx\n# from node2vec import Node2Vec\n# from gensim.models import Word2Vec\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n\n# # Suppress warnings for cleaner output\n# warnings.filterwarnings('ignore')\n\n# # --- Define Kaggle File Paths ---\n# INPUT_DIR = '/kaggle/input/amexkkgp/'\n# WORKING_DIR = '/kaggle/working/'\n\n# # =============================================================================\n# # STAGE 1: LOAD DATA\n# # =============================================================================\n# print(\"\\n--- Section 1: Loading Data ---\")\n# try:\n#     # Load your latest training and test sets\n#     df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n#     df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n#     # Use Dask for the large event log\n#     event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet', columns=['id2', 'id3', 'id4'])\n    \n#     print(\"Successfully loaded all required data.\")\n# except FileNotFoundError:\n#     print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n#     exit()\n\n# # =============================================================================\n# # TECHNIQUE 1: GRAPH EMBEDDINGS (NODEVEC) WITH PCA\n# # =============================================================================\n# print(\"\\n--- Technique 1: Creating Graph Embedding Features ---\")\n\n# # --- Step 1a: Build the Interaction Graph on a Sample ---\n# print(\"Step 1a: Building the customer-offer interaction graph on a sample...\")\n# # Use a large sample of the event data to make the process much faster\n# event_dd_sample = event_dd.sample(frac=0.3, random_state=42)\n# edge_list = event_dd_sample[['id2', 'id3']].compute()\n# # Create a graph from the edge list\n# G = nx.from_pandas_edgelist(edge_list, 'id2', 'id3')\n# print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n\n# # --- Step 1b: Train the Node2Vec Model ---\n# print(\"Step 1b: Training the Node2Vec model...\")\n# node2vec = Node2Vec(G, dimensions=32, walk_length=20, num_walks=10, workers=4, quiet=True)\n# # min_count=5 acts as a form of pruning for robustness.\n# model_n2v = node2vec.fit(window=5, min_count=5, batch_words=4)\n# print(\"Node2Vec model trained successfully.\")\n\n# # --- Step 1c: Extract Offer Embeddings ---\n# print(\"Step 1c: Extracting offer embeddings...\")\n# offer_ids = df_train['id3'].unique().tolist() + df_test['id3'].unique().tolist()\n# offer_ids = list(set(offer_ids)) # Get unique offer IDs across train and test\n\n# graph_embeddings = []\n# for offer_id in offer_ids:\n#     try:\n#         vector = model_n2v.wv[offer_id]\n#         graph_embeddings.append([offer_id] + vector.tolist())\n#     except KeyError:\n#         continue\n\n# graph_embedding_features = pd.DataFrame(graph_embeddings, columns=['id3'] + [f'graph_emb_{i}' for i in range(32)])\n# print(\"Graph embedding features created successfully.\")\n\n# # Clean up memory\n# del G, edge_list, model_n2v, node2vec, event_dd_sample\n# gc.collect()\n\n# # =============================================================================\n# # TECHNIQUE 2: SEQUENTIAL EMBEDDINGS (WORD2VEC) WITH PCA\n# # =============================================================================\n# print(\"\\n--- Technique 2: Creating Sequential Embedding Features ---\")\n\n# # --- Step 2a: Create Customer Sessions ---\n# print(\"Step 2a: Creating customer sessions...\")\n# event_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\n\n# # MODIFIED: Use a more robust Dask method to create sessions by sorting first.\n# print(\"Sorting events by customer and time...\")\n# event_dd = event_dd.sort_values(['id2', 'impression_time'])\n\n# # Define a function to calculate time differences within partitions\n# def calculate_time_diff(df):\n#     df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n#     return df\n\n# # Apply the function to each partition using map_partitions\n# meta = event_dd._meta.copy()\n# meta['time_diff'] = pd.Series(dtype='float64')\n# event_dd = event_dd.map_partitions(calculate_time_diff, meta=meta)\n\n# # A new session starts if the time difference is > 30 minutes (1800 seconds)\n# event_dd['new_session'] = (event_dd['time_diff'] > 1800).fillna(True)\n# event_dd['session_id'] = event_dd.groupby('id2')['new_session'].cumsum()\n\n# # Create the list of sessions (sequences of offers)\n# print(\"Aggregating offers into sessions...\")\n# sessions = event_dd.groupby(['id2', 'session_id'])['id3'].apply(list, meta=('id3', 'object')).compute().tolist()\n# print(f\"Created {len(sessions)} customer sessions.\")\n\n# # --- Step 2b: Train the Word2Vec Model ---\n# print(\"Step 2b: Training the Word2Vec model...\")\n# # Train the model on the sessions. min_count=5 acts as pruning.\n# model_w2v = Word2Vec(sentences=sessions, vector_size=32, window=5, min_count=5, workers=4)\n# print(\"Word2Vec model trained successfully.\")\n\n# # --- Step 2c: Extract Offer Embeddings ---\n# print(\"Step 2c: Extracting sequential embeddings...\")\n# sequential_embeddings = []\n# for offer_id in offer_ids:\n#     try:\n#         vector = model_w2v.wv[offer_id]\n#         sequential_embeddings.append([offer_id] + vector.tolist())\n#     except KeyError:\n#         continue\n\n# sequential_embedding_features = pd.DataFrame(sequential_embeddings, columns=['id3'] + [f'seq_emb_{i}' for i in range(32)])\n# print(\"Sequential embedding features created successfully.\")\n\n# # Clean up memory\n# del sessions, model_w2v, event_dd\n# gc.collect()\n\n# # =============================================================================\n# # STAGE 3: APPLY PCA AND MERGE FEATURES\n# # =============================================================================\n# print(\"\\n--- STAGE 3: Applying PCA and Merging Final Features ---\")\n\n# def apply_pca_to_embeddings(df_features, prefix):\n#     \"\"\"Applies PCA to a dataframe of embedding features.\"\"\"\n#     ids = df_features[['id3']]\n#     embeddings = df_features.drop(columns=['id3'])\n    \n#     # Scale the data before PCA\n#     scaler = StandardScaler()\n#     embeddings_scaled = scaler.fit_transform(embeddings)\n    \n#     # Use PCA to capture 95% of the variance\n#     pca = PCA(n_components=0.95)\n#     embeddings_pca = pca.fit_transform(embeddings_scaled)\n#     print(f\"PCA for '{prefix}' completed. New shape: {embeddings_pca.shape}\")\n    \n#     # Create a DataFrame from the PCA components\n#     pca_df = pd.DataFrame(embeddings_pca, columns=[f'{prefix}_pca_{i}' for i in range(embeddings_pca.shape[1])])\n#     return pd.concat([ids, pca_df], axis=1)\n\n# # --- Apply PCA to both sets of embeddings ---\n# graph_pca_features = apply_pca_to_embeddings(graph_embedding_features, 'graph')\n# sequential_pca_features = apply_pca_to_embeddings(sequential_embedding_features, 'seq')\n\n# def enrich_dataframe(df, graph_pca_features, sequential_pca_features):\n#     \"\"\"A reusable function to enrich a dataframe with new PCA features.\"\"\"\n#     df['id3'] = df['id3'].astype(str)\n#     graph_pca_features['id3'] = graph_pca_features['id3'].astype(str)\n#     sequential_pca_features['id3'] = sequential_pca_features['id3'].astype(str)\n    \n#     # Merge graph PCA features\n#     df = pd.merge(df, graph_pca_features, on='id3', how='left')\n#     # Merge sequential PCA features\n#     df = pd.merge(df, sequential_pca_features, on='id3', how='left')\n    \n#     return df\n\n# # --- Enrich both the training and test dataframes ---\n# print(\"Enriching training data...\")\n# df_train_final = enrich_dataframe(df_train, graph_pca_features, sequential_pca_features)\n# print(\"Enriching test data...\")\n# df_test_final = enrich_dataframe(df_test, graph_pca_features, sequential_pca_features)\n\n# # --- Save the final DataFrames ---\n# try:\n#     train_output_path = f'{WORKING_DIR}train_data_v18.parquet'\n#     test_output_path = f'{WORKING_DIR}test_data_v18.parquet'\n    \n#     df_train_final.to_parquet(train_output_path)\n#     df_test_final.to_parquet(test_output_path)\n    \n#     print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n#     print(f\"Successfully saved final test data to: {test_output_path}\")\n# except Exception as e:\n#     print(f\"\\nError saving the final files: {e}\")\n\n# print(\"\\n--- ADVANCED EMBEDDING FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-20T15:58:44.885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary library for text embeddings\n!pip install -q sentence-transformers\n\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load your latest training and holdout test sets with entity clusters\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_entity_clusters.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_entity_clusters.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Train shape: {df_train.shape}, Holdout Test shape: {df_holdout_test.shape}\")\nexcept FileNotFoundError as e:\n    print(f\"Error: Required data not found: {e}\")\n    print(\"Please ensure the entity_clustering.py script ran successfully.\")\n    exit()\n\n# =============================================================================\n# TECHNIQUE 1: ADVANCED CO-CLICK FEATURES\n# =============================================================================\nprint(\"\\n--- Technique 1: Creating Advanced Co-Click Features ---\")\n\n# --- Step 1a: Create Customer Sessions on a Sample ---\nprint(\"Step 1a: Creating customer sessions on a sample of the data...\")\n# Use a sample of the event data to make the process manageable\nevent_dd_sample = event_dd.sample(frac=0.2, random_state=42)\nevent_dd_sample['impression_time'] = dd.to_datetime(event_dd_sample['id4'], errors='coerce')\n\n# Sort by customer and time to correctly identify sessions\nevent_dd_sample = event_dd_sample.sort_values(['id2', 'impression_time'])\n\n# Define a function to calculate time differences within partitions\ndef calculate_time_diff(df):\n    df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n    return df\n\n# Apply the function using map_partitions for robust Dask execution\nmeta = event_dd_sample._meta.copy()\nmeta['time_diff'] = pd.Series(dtype='float64')\nevent_dd_sample = event_dd_sample.map_partitions(calculate_time_diff, meta=meta)\n\n# A new session starts if the time difference is > 30 minutes (1800 seconds)\nevent_dd_sample['new_session'] = (event_dd_sample['time_diff'] > 1800).fillna(True)\nevent_dd_sample['session_id'] = event_dd_sample.groupby('id2')['new_session'].cumsum()\nprint(\"Customer sessions created.\")\n\n\n# --- Step 1b: Calculate Co-occurrence ---\nprint(\"Step 1b: Calculating offer co-occurrence within sessions...\")\nsession_offers_dd = event_dd_sample[['session_id', 'id2', 'id3']]\nmerged_sessions_dd = dd.merge(session_offers_dd, session_offers_dd, on=['session_id', 'id2'])\nco_occurrence_dd = merged_sessions_dd[merged_sessions_dd['id3_x'] != merged_sessions_dd['id3_y']]\nco_occurrence_counts = co_occurrence_dd.groupby(['id3_x', 'id3_y']).size().compute().reset_index(name='count')\nprint(\"Co-occurrence calculated successfully.\")\n\n\n# --- Step 1c: Create Advanced Co-Click Features ---\nprint(\"Step 1c: Creating final co-click features...\")\n# Use the full event log to get the most accurate historical CTRs\nevent_dd['clicked'] = (~event_dd['id7'].isnull()).astype(int)\noffer_ctr = event_dd.groupby('id3')['clicked'].mean().compute().to_dict()\n\n# Get the CTR of each partner offer\nco_occurrence_counts['partner_ctr'] = co_occurrence_counts['id3_y'].map(offer_ctr)\nco_occurrence_counts = co_occurrence_counts.sort_values(['id3_x', 'count'], ascending=[True, False])\n\n# Find the top 5 partners for each offer\ntop_5_partners = co_occurrence_counts.groupby('id3_x').head(5)\n\n# Calculate aggregate stats for the top 5 partners' CTRs\nco_click_agg = top_5_partners.groupby('id3_x')['partner_ctr'].agg(['mean', 'max', 'std']).reset_index()\nco_click_features = co_click_agg.rename(columns={\n    'id3_x': 'id3',\n    'mean': 'top5_partner_mean_ctr',\n    'max': 'top5_partner_max_ctr',\n    'std': 'top5_partner_std_ctr'\n})\nprint(\"Advanced co-click features created successfully.\")\n\n# Clean up memory\ndel event_dd, event_dd_sample, co_occurrence_counts, merged_sessions_dd, co_occurrence_dd\ngc.collect()\n\n# =============================================================================\n# TECHNIQUE 2: TEXT SIMILARITY FEATURES\n# =============================================================================\nprint(\"\\n--- Technique 2: Creating Text Similarity Features ---\")\n\n# --- Step 2a: Prepare the Text Corpus and Generate Embeddings ---\nprint(\"Step 2a: Preparing text corpus and generating embeddings...\")\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\noffer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\noffer_texts = offer_meta_df[['id3', 'id8', 'full_text']].copy()\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\nembedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\noffer_embeddings_df = pd.concat([offer_texts[['id3', 'id8']], embedding_df], axis=1)\nprint(\"Text embeddings generated.\")\n\n# --- Step 2b: Create Industry DNA and Calculate Similarity ---\nprint(\"Step 2b: Calculating offer-industry text similarity...\")\n# Calculate the average embedding vector for each industry\nindustry_dna = offer_embeddings_df.groupby('id8').mean(numeric_only=True).reset_index()\nindustry_dna = industry_dna.rename(columns={col: f'industry_{col}' for col in industry_dna.columns if 'emb' in col})\n\n# Merge the industry DNA back to each offer\noffer_embeddings_df = pd.merge(offer_embeddings_df, industry_dna, on='id8', how='left')\n\n# Calculate the cosine similarity between each offer's embedding and its industry's average embedding\nemb_cols = [f'text_emb_{i}' for i in range(embeddings.shape[1])]\nindustry_emb_cols = [f'industry_{col}' for col in emb_cols] # Correctly renamed columns\n\n# Fill NaNs in industry vectors for offers in industries with no text data\noffer_embeddings_df[industry_emb_cols] = offer_embeddings_df[industry_emb_cols].fillna(0)\n\n# Vectorized cosine similarity calculation for performance\noffer_vecs = offer_embeddings_df[emb_cols].values\nindustry_vecs = offer_embeddings_df[industry_emb_cols].values\n# Calculate dot product and norms\ndot_product = np.sum(offer_vecs * industry_vecs, axis=1)\noffer_norm = np.linalg.norm(offer_vecs, axis=1)\nindustry_norm = np.linalg.norm(industry_vecs, axis=1)\n# Calculate cosine similarity, handling potential division by zero\ncosine_sim = dot_product / (offer_norm * industry_norm + 1e-6)\n\noffer_embeddings_df['offer_industry_text_similarity'] = cosine_sim\ntext_similarity_features = offer_embeddings_df[['id3', 'offer_industry_text_similarity']]\nprint(\"Text similarity features created successfully.\")\n\n\n# =============================================================================\n# STAGE 3: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets ---\")\n\ndef enrich_dataframe(df, co_click_features, text_similarity_features):\n    \"\"\"A reusable function to enrich a dataframe with new features.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    co_click_features['id3'] = co_click_features['id3'].astype(str)\n    text_similarity_features['id3'] = text_similarity_features['id3'].astype(str)\n    \n    # Merge co-click features\n    df = pd.merge(df, co_click_features, on='id3', how='left')\n    # Merge text similarity features\n    df = pd.merge(df, text_similarity_features, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train.copy(), co_click_features, text_similarity_features)\nprint(\"Enriching holdout-test data...\")\ndf_holdout_test_final = enrich_dataframe(df_holdout_test.copy(), co_click_features, text_similarity_features)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_final_features.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_final_features.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_holdout_test_final.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\n    print(f\"Final Train shape: {df_train_final.shape}, Final Holdout Test shape: {df_holdout_test_final.shape}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:36:19.772686Z","iopub.execute_input":"2025-08-31T17:36:19.773168Z","iopub.status.idle":"2025-08-31T17:38:23.859525Z","shell.execute_reply.started":"2025-08-31T17:36:19.773143Z","shell.execute_reply":"2025-08-31T17:38:23.858622Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n","output_type":"stream"},{"name":"stderr","text":"2025-08-31 17:36:38.053019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756661798.240855     179 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756661798.293780     179 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\nTrain shape: (166729, 395), Holdout Test shape: (55577, 395)\n\n--- Technique 1: Creating Advanced Co-Click Features ---\nStep 1a: Creating customer sessions on a sample of the data...\nCustomer sessions created.\nStep 1b: Calculating offer co-occurrence within sessions...\nCo-occurrence calculated successfully.\nStep 1c: Creating final co-click features...\nAdvanced co-click features created successfully.\n\n--- Technique 2: Creating Text Similarity Features ---\nStep 2a: Preparing text corpus and generating embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a08adbbf6b43e79d40335557f06adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db80bc56184440e69b8827fa4603d141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2954fee8f6c4590b04c09a39bbd8325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b12efc833ad483a834a31f506d5ebf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a798e59c3fda4be98f622729ec22480e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08660d467fc645dd8a618169fce81db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fcb488073a46e1a90481180c9451ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65e4e0d870541328991f507f08b3158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69fa79bbd708471db662fb3b95196bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efca2b02f687418eb84f93c7d01e582f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30821bd5efff428c851c8eb726fbe109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f086a7d9887461b8d70e749c27ac9d8"}},"metadata":{}},{"name":"stdout","text":"Text embeddings generated.\nStep 2b: Calculating offer-industry text similarity...\nText similarity features created successfully.\n\n--- STAGE 3: Merging New Features and Saving Final Datasets ---\nEnriching training data...\nEnriching holdout-test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_final_features.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_final_features.parquet\nFinal Train shape: (166729, 399), Final Holdout Test shape: (55577, 399)\n\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary library for text embeddings\n!pip install -q sentence-transformers\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm.auto import tqdm\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\ntqdm.pandas()\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load the datasets you just created in the previous step\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_final_features.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_final_features.parquet')\n    \n    # Load supplementary data needed for embeddings\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Train shape: {df_train.shape}, Holdout Test shape: {df_holdout_test.shape}\")\nexcept FileNotFoundError as e:\n    print(f\"Error: Required data not found: {e}\")\n    print(\"Please ensure the final_embedding_features.py script ran successfully.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: GENERATE TEXT EMBEDDINGS (needed for similarity calculation)\n# =============================================================================\nprint(\"\\n--- Stage 2: Generating Text Embeddings for Similarity Calculation ---\")\n\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\noffer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\noffer_texts = offer_meta_df[['id3', 'full_text']].copy()\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\nembedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\noffer_embeddings_df = pd.concat([offer_texts[['id3']], embedding_df], axis=1)\nprint(\"Text embeddings generated for all offers.\")\n\n# =============================================================================\n# STAGE 3: SIMILARITY-BASED SEQUENTIAL FEATURES (RNN-like Logic)\n# =============================================================================\nprint(\"\\n--- Stage 3: Creating Similarity-Based Sequential Features ---\")\n\ndef create_sequential_features(df, offer_embeddings_df):\n    \"\"\"\n    Creates handcrafted features based on the similarity of a target offer\n    to a customer's recent click history within the provided dataframe.\n    \"\"\"\n    print(f\"Preparing data for sequential feature engineering on a dataframe of shape {df.shape}...\")\n    # Ensure time is in the correct format\n    df['impression_time'] = pd.to_datetime(df['id4'], errors='coerce')\n    \n    # Create a lookup dictionary for offer embeddings for fast access\n    emb_cols = [col for col in offer_embeddings_df if 'text_emb' in col]\n    offer_embeddings_lookup = offer_embeddings_df.set_index('id3')[emb_cols]\n\n    # Get a dataframe of only the click events to build histories\n    clicks = df[df['y'] == 1].copy()\n    clicks = clicks.sort_values('impression_time')\n    \n    # For each customer, create a list of their clicked offers and timestamps\n    click_history = clicks.groupby('id2').agg({\n        'id3': list,\n        'impression_time': list\n    }).rename(columns={'id3': 'click_history_offers', 'impression_time': 'click_history_times'}).reset_index()\n\n    # Merge this history back to the main dataframe\n    df = pd.merge(df, click_history, on='id2', how='left')\n    \n    def calculate_similarity_features(row, N=5):\n        # Get current offer details\n        current_offer_id = row['id3']\n        current_time = row['impression_time']\n        \n        # Check if history exists\n        if not isinstance(row['click_history_offers'], list):\n            return pd.Series([np.nan] * 4, index=['max_sim_recent_5', 'mean_sim_recent_5', 'std_sim_recent_5', 'time_weighted_mean_sim_recent_5'])\n            \n        # Combine history into a dataframe\n        history_df = pd.DataFrame({\n            'offer': row['click_history_offers'],\n            'time': row['click_history_times']\n        })\n        \n        # Filter for clicks that happened *before* the current impression\n        recent_history = history_df[history_df['time'] < current_time].tail(N)\n        \n        if recent_history.empty:\n            return pd.Series([np.nan] * 4, index=['max_sim_recent_5', 'mean_sim_recent_5', 'std_sim_recent_5', 'time_weighted_mean_sim_recent_5'])\n\n        # Get embeddings for current offer and recent clicks\n        try:\n            current_vec = offer_embeddings_lookup.loc[current_offer_id].values.reshape(1, -1)\n            recent_vecs = offer_embeddings_lookup.loc[recent_history['offer']].values\n        except KeyError:\n             return pd.Series([np.nan] * 4, index=['max_sim_recent_5', 'mean_sim_recent_5', 'std_sim_recent_5', 'time_weighted_mean_sim_recent_5'])\n\n        # Calculate similarities and time differences\n        sims = cosine_similarity(current_vec, recent_vecs)[0]\n        time_diffs_days = (current_time - recent_history['time']).dt.total_seconds() / (24 * 3600)\n        \n        # Time decay factor: more recent clicks get higher weight (e.g., exponential decay)\n        time_weights = np.exp(-time_diffs_days)\n\n        # Calculate final features\n        max_sim = np.max(sims)\n        mean_sim = np.mean(sims)\n        std_sim = np.std(sims) if len(sims) > 1 else 0 # Std dev is 0 if only one item\n        time_weighted_mean_sim = np.sum(sims * time_weights) / np.sum(time_weights) if np.sum(time_weights) > 0 else np.nan\n        \n        return pd.Series([max_sim, mean_sim, std_sim, time_weighted_mean_sim], index=['max_sim_recent_5', 'mean_sim_recent_5', 'std_sim_recent_5', 'time_weighted_mean_sim_recent_5'])\n\n    print(f\"Applying similarity logic to {len(df)} rows. This may take a while...\")\n    # Apply the function to each row\n    sequential_features = df.progress_apply(calculate_similarity_features, axis=1)\n    \n    # Combine the new features with the original dataframe\n    df = pd.concat([df, sequential_features], axis=1)\n    \n    # Clean up temporary columns\n    df = df.drop(columns=['click_history_offers', 'click_history_times', 'impression_time'])\n    \n    return df\n\n# --- Create sequential features for both train and test data ---\ndf_train_final = create_sequential_features(df_train, offer_embeddings_df)\ndf_holdout_test_final = create_sequential_features(df_holdout_test, offer_embeddings_df)\nprint(\"Similarity-based sequential features created successfully.\")\n\n# =============================================================================\n# STAGE 4: SAVE THE FINAL, FULLY-FEATURED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Final, Fully-Featured Datasets ---\")\n\ntry:\n    train_output_path = f'{WORKING_DIR}train_fully_featured.parquet'\n    holdout_test_output_path = f'{WORKING_DIR}holdout_test_fully_featured.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_holdout_test_final.to_parquet(holdout_test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final holdout-test data to: {holdout_test_output_path}\")\n    print(f\"Final Train shape: {df_train_final.shape}, Final Holdout Test shape: {df_holdout_test_final.shape}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- FULL FEATURE ENGINEERING PIPELINE COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:41:09.262276Z","iopub.execute_input":"2025-08-31T17:41:09.263328Z","iopub.status.idle":"2025-08-31T17:45:55.245377Z","shell.execute_reply.started":"2025-08-31T17:41:09.263290Z","shell.execute_reply":"2025-08-31T17:45:55.244458Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\nTrain shape: (166729, 399), Holdout Test shape: (55577, 399)\n\n--- Stage 2: Generating Text Embeddings for Similarity Calculation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3887dde3b1ab4b03854c32c32957e96c"}},"metadata":{}},{"name":"stdout","text":"Text embeddings generated for all offers.\n\n--- Stage 3: Creating Similarity-Based Sequential Features ---\nPreparing data for sequential feature engineering on a dataframe of shape (166729, 399)...\nApplying similarity logic to 166729 rows. This may take a while...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/166729 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b5fb2f6ffb4b389e5947f4e2b0d02c"}},"metadata":{}},{"name":"stdout","text":"Preparing data for sequential feature engineering on a dataframe of shape (55577, 399)...\nApplying similarity logic to 55577 rows. This may take a while...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/55577 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4181a5c4bd455a9df273d2e395bfc7"}},"metadata":{}},{"name":"stdout","text":"Similarity-based sequential features created successfully.\n\n--- STAGE 4: Saving Final, Fully-Featured Datasets ---\n\nSuccessfully saved final training data to: /kaggle/working/train_fully_featured.parquet\nSuccessfully saved final holdout-test data to: /kaggle/working/holdout_test_fully_featured.parquet\nFinal Train shape: (166729, 403), Final Holdout Test shape: (55577, 403)\n\n--- FULL FEATURE ENGINEERING PIPELINE COMPLETE ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary library for CatBoost\n!pip install -q catboost\n\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRanker, Pool\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\n# Assumes the output of the previous script is in the working directory\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 0: DEFINE THE MAP@7 EVALUATION METRIC\n# =============================================================================\ndef map_at_k(y_true, y_pred, group_ids, k=7):\n    \"\"\"\n    Calculates the Mean Average Precision at k.\n    \"\"\"\n    df = pd.DataFrame({'group': group_ids, 'y_true': y_true, 'y_pred': y_pred})\n    \n    average_precisions = []\n    for group in df['group'].unique():\n        group_df = df[df['group'] == group].sort_values('y_pred', ascending=False).reset_index(drop=True)\n        \n        # Get the top k predictions\n        group_df = group_df.head(k)\n        \n        if group_df['y_true'].sum() == 0:\n            average_precisions.append(0)\n            continue\n            \n        relevant_hits = 0\n        precision_at_i = []\n        for i, row in group_df.iterrows():\n            if row['y_true'] == 1:\n                relevant_hits += 1\n                precision_at_i.append(relevant_hits / (i + 1))\n        \n        if not precision_at_i:\n            average_precisions.append(0)\n        else:\n            average_precisions.append(np.mean(precision_at_i))\n            \n    return np.mean(average_precisions)\n\n\n# =============================================================================\n# STAGE 1: DATA LOADING AND PREPARATION FOR CATBOOSTRANKER\n# =============================================================================\nprint(\"--- STAGE 1: Loading and Preparing Data for CatBoostRanker ---\")\ntry:\n    # Load your final, pre-cleaned training and test data from the previous step\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_fully_featured.parquet')\n    df_holdout_test = pd.read_parquet(f'{WORKING_DIR}holdout_test_fully_featured.parquet')\n    print(\"Successfully loaded final training and holdout-test data.\")\nexcept FileNotFoundError:\n    print(\"Error: Fully featured data not found. Please run the final feature engineering script first.\")\n    exit()\n\n# --- Memory Reduction Function ---\ndef reduce_mem_usage(df):\n    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)[:3] == 'int' or str(col_type)[:5] == 'float':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n    return df\n\n# --- Prepare Feature and Target Names ---\ntarget_col = 'y'\n# All columns are features except for identifiers and the target\nfeature_cols = [col for col in df_train.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n# Identify all categorical features for CatBoost's internal handling\ncategorical_features = [col for col in df_train.columns if 'cluster' in col or col == 'offer_type_code' or col == 'offer_type_x_weekend']\n\n# --- Preprocessing ---\nprint(\"Preprocessing data for training...\")\n# Convert feature columns to numeric, let CatBoost handle categoricals as objects\nfor col in feature_cols:\n    if col not in categorical_features:\n        df_train[col] = pd.to_numeric(df_train[col], errors='coerce')\n        df_holdout_test[col] = pd.to_numeric(df_holdout_test[col], errors='coerce')\n\n# Convert categorical columns to string type for CatBoost\nfor col in categorical_features:\n    df_train[col] = df_train[col].astype(str).fillna('missing')\n    df_holdout_test[col] = df_holdout_test[col].astype(str).fillna('missing')\n\n# Final imputation for numeric features\nnumeric_features = [col for col in feature_cols if col not in categorical_features]\ndf_train[numeric_features] = df_train[numeric_features].fillna(-999)\ndf_holdout_test[numeric_features] = df_holdout_test[numeric_features].fillna(-999)\n\n\n# Optimize memory usage\nprint(\"\\nOptimizing memory usage for training data...\")\ndf_train = reduce_mem_usage(df_train)\nprint(\"\\nOptimizing memory usage for holdout-test data...\")\ndf_holdout_test = reduce_mem_usage(df_holdout_test)\n\n# --- Create Groups for CatBoostRanker ---\n# A \"query\" or \"group\" is a single ranking task (all offers for one customer on one day)\ndf_train['group_id'] = df_train['id2'].astype(str) + '_' + df_train['id5'].astype(str)\ndf_train = df_train.sort_values('group_id').reset_index(drop=True)\n\n# Define features (X) and target (y)\nX_train = df_train[feature_cols]\ny_train = df_train[target_col].astype(int)\nprint(f\"Prepared training data with {len(feature_cols)} features and {df_train['group_id'].nunique()} groups.\")\n\n# Prepare holdout-test set\ndf_holdout_test['group_id'] = df_holdout_test['id2'].astype(str) + '_' + df_holdout_test['id5'].astype(str)\nX_holdout_test = df_holdout_test[feature_cols]\ny_holdout_test = df_holdout_test[target_col].astype(int)\n\n# =============================================================================\n# STAGE 2: TRAINING FINAL ENSEMBLE WITH CATBOOSTRANKER\n# =============================================================================\nprint(\"\\n--- STAGE 2: Training Final Ensemble of CatBoostRanker Models ---\")\n\n# Define CatBoostRanker parameters for a balance of speed and performance\nfinal_params = {\n    'objective': 'YetiRank',\n    'eval_metric': 'MAP:top=7', # CORRECTED: Changed 'k=7' to 'top=7' for correct CatBoost syntax\n    'random_seed': 42,\n    'task_type': 'GPU',\n    'iterations': 1500, # A reasonable number with early stopping\n    'learning_rate': 0.05,\n    'depth': 7, # A slightly shallower tree for speed\n    'l2_leaf_reg': 3,\n    'verbose': 200\n}\n\nN_SPLITS = 3 # Using 3 folds for a good balance of stability and speed\ngkf_train = GroupKFold(n_splits=N_SPLITS)\noof_predictions = np.zeros(len(df_train))\noof_map_scores = []\n\nprint(f\"Starting final training with {N_SPLITS}-Fold GroupKFold on GPU...\")\n# For CatBoostRanker, we must use the customer ID for the GroupKFold split\ngroups_for_split = df_train['id2']\n\nfor fold, (train_idx, val_idx) in enumerate(gkf_train.split(X_train, y_train, groups=groups_for_split)):\n    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n    X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n    \n    # Get the group_id for each set\n    train_group_ids = df_train.iloc[train_idx]['group_id']\n    val_group_ids = df_train.iloc[val_idx]['group_id']\n    \n    # Create CatBoost Pool objects for efficiency\n    train_pool = Pool(data=X_train_fold, label=y_train_fold, group_id=train_group_ids, cat_features=categorical_features)\n    val_pool = Pool(data=X_val_fold, label=y_val_fold, group_id=val_group_ids, cat_features=categorical_features)\n    \n    model = CatBoostRanker(**final_params)\n    model.fit(train_pool,\n              eval_set=val_pool,\n              early_stopping_rounds=50)\n    \n    # CatBoostRanker outputs scores, not probabilities\n    val_preds = model.predict(X_val_fold)\n    oof_predictions[val_idx] = val_preds\n\n    # --- Calculate and store MAP@7 for this fold ---\n    fold_map_score = map_at_k(y_val_fold, val_preds, val_group_ids, k=7)\n    oof_map_scores.append(fold_map_score)\n    print(f\"Fold {fold+1} MAP@7 Score: {fold_map_score:.5f}\")\n\n    model_path = f'{WORKING_DIR}final_catboost_ranker_fold_{fold+1}.cbm'\n    model.save_model(model_path)\n    print(f\"Model for fold {fold+1} saved to {model_path}\")\n    del X_train_fold, y_train_fold, X_val_fold, y_val_fold, model, train_pool, val_pool\n    gc.collect()\n\n# --- Evaluate Overall Performance ---\nprint(f\"\\n--- Overall Cross-Validation Results ---\")\nprint(f\"Mean Out-of-Fold (OOF) MAP@7 Score: {np.mean(oof_map_scores):.5f}\")\nprint(\"This score is a reliable estimate of your model's generalization ability.\")\n\n\n# =============================================================================\n# STAGE 3: FINAL EVALUATION ON HOLDOUT-TEST SET\n# =============================================================================\nprint(\"\\n--- STAGE 3: Final Evaluation on Holdout-Test Set ---\")\n\n# Load models and predict on the holdout test set\nholdout_test_predictions = np.zeros(len(X_holdout_test))\nfor fold in range(1, N_SPLITS + 1):\n    print(f\"Predicting with Fold {fold}/{N_SPLITS}...\")\n    model_path = f'{WORKING_DIR}final_catboost_ranker_fold_{fold}.cbm'\n    model = CatBoostRanker()\n    model.load_model(model_path)\n    # Predict outputs ranking scores\n    holdout_test_predictions += model.predict(X_holdout_test) / N_SPLITS\n\n# --- Calculate final MAP@7 on the holdout set ---\nholdout_group_ids = df_holdout_test['group_id']\nfinal_holdout_map_score = map_at_k(y_holdout_test, holdout_test_predictions, holdout_group_ids, k=7)\nprint(f\"\\n--- FINAL MODEL PERFORMANCE ---\")\nprint(f\"Final MAP@7 Score on Hold-Out Test Set: {final_holdout_map_score:.5f}\")\n\n# =============================================================================\n# STAGE 4: SAVE HOLDOUT PREDICTIONS FOR INSPECTION\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Holdout Predictions for Inspection ---\")\n\n# Create submission-style dataframe for the holdout set\nholdout_preds_df = df_holdout_test[['id1', 'id2', 'id3', 'id5', 'y']].copy()\nholdout_preds_df['id5'] = pd.to_datetime(holdout_preds_df['id5'], errors='coerce').dt.strftime('%m/%d/%Y')\ncleaned_predictions = np.nan_to_num(holdout_test_predictions, nan=-999)\nholdout_preds_df['pred_score'] = cleaned_predictions\n\noutput_path = f'{WORKING_DIR}holdout_predictions.csv'\nholdout_preds_df.to_csv(output_path, index=False)\nprint(f\"\\nHoldout predictions successfully saved to: {output_path}\")\nprint(\"\\nFirst 5 rows of the predictions file:\")\nprint(holdout_preds_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:47:47.072229Z","iopub.execute_input":"2025-08-31T17:47:47.073153Z","iopub.status.idle":"2025-08-31T17:52:33.103135Z","shell.execute_reply.started":"2025-08-31T17:47:47.073117Z","shell.execute_reply":"2025-08-31T17:52:33.102280Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- STAGE 1: Loading and Preparing Data for CatBoostRanker ---\nSuccessfully loaded final training and holdout-test data.\nPreprocessing data for training...\n\nOptimizing memory usage for training data...\nMemory usage of dataframe is 512.00 MB\nMemory usage after optimization is: 263.95 MB\nDecreased by 48.4%\n\nOptimizing memory usage for holdout-test data...\nMemory usage of dataframe is 170.67 MB\nMemory usage after optimization is: 87.98 MB\nDecreased by 48.4%\nPrepared training data with 396 features and 27340 groups.\n\n--- STAGE 2: Training Final Ensemble of CatBoostRanker Models ---\nStarting final training with 3-Fold GroupKFold on GPU...\n--- Fold 1/3 ---\nGroupwise loss function. OneHotMaxSize set to 10\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because PFound, MAP is/are not implemented for GPU\nMetric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\nMetric MAP:top=7 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.1209213\ttest: 0.1211307\tbest: 0.1211307 (0)\ttotal: 14.9s\tremaining: 6h 13m 13s\n200:\tlearn: 0.1329339\ttest: 0.1309675\tbest: 0.1309675 (200)\ttotal: 18.3s\tremaining: 1m 58s\n400:\tlearn: 0.1346908\ttest: 0.1320096\tbest: 0.1320096 (400)\ttotal: 21.7s\tremaining: 59.3s\nbestTest = 0.1323617169\nbestIteration = 495\nShrink model to first 496 iterations.\nFold 1 MAP@7 Score: 0.14210\nModel for fold 1 saved to /kaggle/working/final_catboost_ranker_fold_1.cbm\n--- Fold 2/3 ---\nGroupwise loss function. OneHotMaxSize set to 10\n0:\tlearn: 0.1176238\ttest: 0.1191104\tbest: 0.1191104 (0)\ttotal: 30.2ms\tremaining: 45.3s\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because PFound, MAP is/are not implemented for GPU\nMetric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\nMetric MAP:top=7 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"200:\tlearn: 0.1317187\ttest: 0.1325895\tbest: 0.1325895 (200)\ttotal: 3.38s\tremaining: 21.9s\nbestTest = 0.1331174893\nbestIteration = 254\nShrink model to first 255 iterations.\nFold 2 MAP@7 Score: 0.14426\nModel for fold 2 saved to /kaggle/working/final_catboost_ranker_fold_2.cbm\n--- Fold 3/3 ---\nGroupwise loss function. OneHotMaxSize set to 10\n0:\tlearn: 0.1183993\ttest: 0.1141419\tbest: 0.1141419 (0)\ttotal: 30.1ms\tremaining: 45.1s\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because PFound, MAP is/are not implemented for GPU\nMetric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\nMetric MAP:top=7 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"200:\tlearn: 0.1336136\ttest: 0.1301424\tbest: 0.1301965 (173)\ttotal: 3.43s\tremaining: 22.2s\n400:\tlearn: 0.1353007\ttest: 0.1316308\tbest: 0.1316308 (400)\ttotal: 6.8s\tremaining: 18.6s\nbestTest = 0.132031707\nbestIteration = 527\nShrink model to first 528 iterations.\nFold 3 MAP@7 Score: 0.14160\nModel for fold 3 saved to /kaggle/working/final_catboost_ranker_fold_3.cbm\n\n--- Overall Cross-Validation Results ---\nMean Out-of-Fold (OOF) MAP@7 Score: 0.14265\nThis score is a reliable estimate of your model's generalization ability.\n\n--- STAGE 3: Final Evaluation on Holdout-Test Set ---\nPredicting with Fold 1/3...\nPredicting with Fold 2/3...\nPredicting with Fold 3/3...\n\n--- FINAL MODEL PERFORMANCE ---\nFinal MAP@7 Score on Hold-Out Test Set: 0.15159\n\n--- STAGE 4: Saving Holdout Predictions for Inspection ---\n\nHoldout predictions successfully saved to: /kaggle/working/holdout_predictions.csv\n\nFirst 5 rows of the predictions file:\n                                               id1      id2      id3  \\\n0  1168133_415582_16-23_2023-11-01 22:23:04.000143  1168133   415582   \n1      1442895_22896_16-23_2023-11-01 17:06:28.895  1442895    22896   \n2     1309455_290624_16-23_2023-11-01 05:29:15.354  1309455   290624   \n3    1630478_2302673_16-23_2023-11-01 16:29:07.604  1630478  2302673   \n4   1489952_29416_16-23_2023-11-01 17:02:05.000402  1489952    29416   \n\n          id5  y  pred_score  \n0  11/01/2023  1   -0.035531  \n1  11/01/2023  0   -1.463115  \n2  11/01/2023  0   -1.288484  \n3  11/01/2023  0   -0.920983  \n4  11/01/2023  1    0.858924  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}