{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12516693,"sourceType":"datasetVersion","datasetId":7851172},{"sourceId":12522023,"sourceType":"datasetVersion","datasetId":7904190}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:33:51.595234Z","iopub.execute_input":"2025-07-20T04:33:51.595430Z","iopub.status.idle":"2025-07-20T04:33:51.864217Z","shell.execute_reply.started":"2025-07-20T04:33:51.595414Z","shell.execute_reply":"2025-07-20T04:33:51.863474Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amexkkgp/test_data.parquet\n/kaggle/input/amexkkgp/add_event.parquet\n/kaggle/input/amexkkgp/data_dictionary.csv\n/kaggle/input/amexkkgp/offer_metadata.parquet\n/kaggle/input/amexkkgp/add_trans.parquet\n/kaggle/input/amexkkgp/test_enriched.parquet\n/kaggle/input/amexkkgp/train_enriched.parquet\n/kaggle/input/amexkkgp/train_data.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nprint(\"Loading datasets...\")\n# --- 1. Load Datasets (Using Dask for Large Files) ---\n# Load smaller files with pandas\nstratified_train_df = pd.read_parquet('/kaggle/input/amexkkgp/train_data.parquet')\ntest_df = pd.read_parquet('/kaggle/input/amexkkgp/test_data.parquet')\noffer_df = pd.read_parquet('/kaggle/input/amexkkgp/offer_metadata.parquet')\n\n# Load large event and transaction logs with Dask to prevent memory crashes\ntry:\n    event_df_dask = dd.read_parquet('/kaggle/input/amexkkgp/add_event.parquet')\n    trans_df_dask = dd.read_parquet('/kaggle/input/amexkkgp/add_trans.parquet')\n    print(\"All datasets loaded successfully (using Dask for large files).\")\nexcept Exception as e:\n    print(f\"Error loading supplementary data: {e}\")\n    # Terminate if supplementary data isn't available\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:34:44.750797Z","iopub.execute_input":"2025-07-20T04:34:44.751060Z","iopub.status.idle":"2025-07-20T04:35:06.532288Z","shell.execute_reply.started":"2025-07-20T04:34:44.751037Z","shell.execute_reply":"2025-07-20T04:35:06.531616Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nAll datasets loaded successfully (using Dask for large files).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- 2. Feature Engineering from add_event_df (Offer-Level Only) ---\nprint(\"\\nStarting feature engineering on event data for OFFERS only...\")\n\n# Create a 'clicked' column based on whether id7 is null\nevent_df_dask['clicked'] = (~event_df_dask['id7'].isnull()).astype(int)\n\n# --- Offer-level event features ---\n# Group by offer (id3) and aggregate its historical performance\noffer_event_features_dask = event_df_dask.groupby('id3').agg(\n    offer_total_impressions=('id4', 'count'),\n    offer_total_clicks=('clicked', 'sum')\n)\n# Calculate offer's historical CTR\noffer_event_features_dask['offer_historical_ctr'] = (\n    offer_event_features_dask['offer_total_clicks'] / offer_event_features_dask['offer_total_impressions']\n).fillna(0)\n\n# --- Execute Dask computation ---\nprint(\"Computing aggregated offer features...\")\noffer_event_features = offer_event_features_dask.compute().reset_index()\nprint(\"Offer event features created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:35:19.607548Z","iopub.execute_input":"2025-07-20T04:35:19.608001Z","iopub.status.idle":"2025-07-20T04:35:27.104389Z","shell.execute_reply.started":"2025-07-20T04:35:19.607978Z","shell.execute_reply":"2025-07-20T04:35:27.103747Z"}},"outputs":[{"name":"stdout","text":"\nStarting feature engineering on event data for OFFERS only...\nComputing aggregated offer features...\nOffer event features created.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- 3. NEW: Feature Engineering from add_trans_df (Industry-Level) ---\nprint(\"\\nStarting feature engineering on transaction data for INDUSTRIES...\")\ntrans_df_dask['f367'] = dd.to_numeric(trans_df_dask['f367'], errors='coerce')\n\n# Group by industry (id8) and aggregate transaction behavior\nindustry_trans_features_dask = trans_df_dask.groupby('id8').agg(\n    industry_avg_spend=('f367', 'mean'),\n    industry_total_transactions=('f367', 'count'),\n    industry_unique_products=('f368', dd.Aggregation('nunique', chunk=lambda s: s.nunique(), agg=lambda s: s.nunique()))\n)\nprint(\"Computing aggregated industry features...\")\nindustry_trans_features = industry_trans_features_dask.compute().reset_index()\nprint(\"Industry transaction features created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:35:39.256336Z","iopub.execute_input":"2025-07-20T04:35:39.256785Z","iopub.status.idle":"2025-07-20T04:35:41.326341Z","shell.execute_reply.started":"2025-07-20T04:35:39.256766Z","shell.execute_reply":"2025-07-20T04:35:41.325574Z"}},"outputs":[{"name":"stdout","text":"\nStarting feature engineering on transaction data for INDUSTRIES...\nComputing aggregated industry features...\nIndustry transaction features created.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- 4. Feature Engineering from offer_metadata_df (Pandas) ---\nprint(\"\\nStarting feature engineering on offer metadata...\")\noffer_df['id12'] = pd.to_datetime(offer_df['id12'], errors='coerce')\noffer_df['id13'] = pd.to_datetime(offer_df['id13'], errors='coerce')\noffer_df['offer_duration_days'] = (offer_df['id13'] - offer_df['id12']).dt.days\n\noffer_meta_features = offer_df[['id3', 'f375', 'f376', 'id10', 'id8', 'offer_duration_days']].rename(columns={\n    'f375': 'offer_redemption_freq',\n    'f376': 'offer_discount_rate',\n    'id10': 'offer_type_code' # Renamed to avoid confusion with industry\n})\n# Ensure the merge keys are the correct type\noffer_meta_features['id3'] = offer_meta_features['id3'].astype(str)\noffer_meta_features['id8'] = offer_meta_features['id8'].astype(str)\nindustry_trans_features['id8'] = industry_trans_features['id8'].astype(str)\n\n# --- NEW: Merge industry features into offer metadata ---\noffer_meta_features = pd.merge(offer_meta_features, industry_trans_features, on='id8', how='left')\nprint(\"Offer metadata enriched with industry transaction data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:35:54.032498Z","iopub.execute_input":"2025-07-20T04:35:54.033219Z","iopub.status.idle":"2025-07-20T04:35:54.061649Z","shell.execute_reply.started":"2025-07-20T04:35:54.033167Z","shell.execute_reply":"2025-07-20T04:35:54.061000Z"}},"outputs":[{"name":"stdout","text":"\nStarting feature engineering on offer metadata...\nOffer metadata enriched with industry transaction data.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- 5. Merge All New Features into Main DataFrames ---\nprint(\"\\nMerging all new features into training and test sets...\")\ndef enrich_dataframe(df):\n    \"\"\"Merges all engineered features into a given dataframe.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n\n    # Now merging small, pre-computed pandas DataFrames\n    df = pd.merge(df, offer_event_features, on='id3', how='left')\n    df = pd.merge(df, offer_meta_features, on='id3', how='left')\n\n    return df\n\ntrain_enriched = enrich_dataframe(stratified_train_df)\ntest_enriched = enrich_dataframe(test_df)\n\nprint(f\"Enriched training data shape: {train_enriched.shape}\")\nprint(f\"Enriched test data shape: {test_enriched.shape}\")\n\n\n# --- 6. Save the Enriched Datasets ---\nprint(\"\\nSaving enriched datasets to Parquet files...\")\ntry:\n    train_enriched.to_parquet('/kaggle/working/train_enriched_v10.parquet')\n    test_enriched.to_parquet('/kaggle/working/test_enriched_v10.parquet')\n    print(\"Successfully saved enriched data.\")\nexcept Exception as e:\n    print(f\"Error saving enriched data: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:37:01.291055Z","iopub.execute_input":"2025-07-20T04:37:01.291674Z","iopub.status.idle":"2025-07-20T04:38:17.977462Z","shell.execute_reply.started":"2025-07-20T04:37:01.291646Z","shell.execute_reply":"2025-07-20T04:38:17.976672Z"}},"outputs":[{"name":"stdout","text":"\nMerging all new features into training and test sets...\nEnriched training data shape: (770164, 383)\nEnriched test data shape: (369301, 382)\n\nSaving enriched datasets to Parquet files...\nSuccessfully saved enriched data.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_enriched.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:38:17.979098Z","iopub.execute_input":"2025-07-20T04:38:17.979318Z","iopub.status.idle":"2025-07-20T04:38:17.984388Z","shell.execute_reply.started":"2025-07-20T04:38:17.979302Z","shell.execute_reply":"2025-07-20T04:38:17.983704Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(770164, 383)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_enriched.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:38:17.984961Z","iopub.execute_input":"2025-07-20T04:38:17.985212Z","iopub.status.idle":"2025-07-20T04:38:18.021845Z","shell.execute_reply.started":"2025-07-20T04:38:17.985168Z","shell.execute_reply":"2025-07-20T04:38:18.021243Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                               id1      id2        id3  \\\n0  1366776_189706075_16-23_2023-11-02 22:22:00.042  1366776  189706075   \n1      1366776_89227_16-23_2023-11-01 23:51:24.999  1366776      89227   \n2      1366776_35046_16-23_2023-11-01 00:30:59.797  1366776      35046   \n3    1366776_6275451_16-23_2023-11-02 22:21:32.261  1366776    6275451   \n4      1366776_78053_16-23_2023-11-02 22:21:34.799  1366776      78053   \n\n                       id4         id5  y   f1    f2    f3    f4  ...  \\\n0  2023-11-02 22:22:00.042  2023-11-02  0  1.0  None  None  None  ...   \n1  2023-11-01 23:51:24.999  2023-11-01  0  1.0  None  None  None  ...   \n2  2023-11-01 00:30:59.797  2023-11-01  0  1.0  None  None  None  ...   \n3  2023-11-02 22:21:32.261  2023-11-02  0  1.0  None  None  None  ...   \n4  2023-11-02 22:21:34.799  2023-11-02  0  1.0  None  None  None  ...   \n\n  offer_total_clicks offer_historical_ctr offer_redemption_freq  \\\n0               1092             0.059875                   2.0   \n1                966             0.046487                   2.0   \n2                759             0.041484                   2.0   \n3                771             0.042805                   2.0   \n4                784             0.042544                   2.0   \n\n  offer_discount_rate offer_type_code       id8 offer_duration_days  \\\n0                 2.0               1  57310000                29.0   \n1                 NaN               1  59210000               181.0   \n2                10.0               1  72310000                29.0   \n3                10.0               1  56510500                29.0   \n4                 8.0               1  59991300                29.0   \n\n  industry_avg_spend industry_total_transactions industry_unique_products  \n0         433.638017                     37472.0                      1.0  \n1         142.771510                     24597.0                      1.0  \n2         137.033818                     19409.0                      1.0  \n3         249.131054                     57064.0                      1.0  \n4         167.946222                     14005.0                      1.0  \n\n[5 rows x 383 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id1</th>\n      <th>id2</th>\n      <th>id3</th>\n      <th>id4</th>\n      <th>id5</th>\n      <th>y</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>...</th>\n      <th>offer_total_clicks</th>\n      <th>offer_historical_ctr</th>\n      <th>offer_redemption_freq</th>\n      <th>offer_discount_rate</th>\n      <th>offer_type_code</th>\n      <th>id8</th>\n      <th>offer_duration_days</th>\n      <th>industry_avg_spend</th>\n      <th>industry_total_transactions</th>\n      <th>industry_unique_products</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1366776_189706075_16-23_2023-11-02 22:22:00.042</td>\n      <td>1366776</td>\n      <td>189706075</td>\n      <td>2023-11-02 22:22:00.042</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>1092</td>\n      <td>0.059875</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>57310000</td>\n      <td>29.0</td>\n      <td>433.638017</td>\n      <td>37472.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1366776_89227_16-23_2023-11-01 23:51:24.999</td>\n      <td>1366776</td>\n      <td>89227</td>\n      <td>2023-11-01 23:51:24.999</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>966</td>\n      <td>0.046487</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>59210000</td>\n      <td>181.0</td>\n      <td>142.771510</td>\n      <td>24597.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1366776_35046_16-23_2023-11-01 00:30:59.797</td>\n      <td>1366776</td>\n      <td>35046</td>\n      <td>2023-11-01 00:30:59.797</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>759</td>\n      <td>0.041484</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>72310000</td>\n      <td>29.0</td>\n      <td>137.033818</td>\n      <td>19409.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1366776_6275451_16-23_2023-11-02 22:21:32.261</td>\n      <td>1366776</td>\n      <td>6275451</td>\n      <td>2023-11-02 22:21:32.261</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>771</td>\n      <td>0.042805</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>56510500</td>\n      <td>29.0</td>\n      <td>249.131054</td>\n      <td>57064.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1366776_78053_16-23_2023-11-02 22:21:34.799</td>\n      <td>1366776</td>\n      <td>78053</td>\n      <td>2023-11-02 22:21:34.799</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>784</td>\n      <td>0.042544</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>1</td>\n      <td>59991300</td>\n      <td>29.0</td>\n      <td>167.946222</td>\n      <td>14005.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 383 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Modeling libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA AND DATA DICTIONARY\n# =============================================================================\nprint(\"--- STAGE 1: Loading Data ---\")\ntry:\n    # Load the intelligent sample for fitting the cluster models\n    df_train = pd.read_parquet(f'/kaggle/working/train_enriched_v10.parquet')\n    print(\"Successfully loaded all required data.\")\n    print(f\"Training sample shape: {df_train.shape}\")\n    \n    # --- NEW: Print the last 15 column names ---\n    print(\"\\nLast 15 columns of the training data:\")\n    print(df_train.columns[-15:].tolist())\n    \n    # Load the full, enriched test set to apply the transformations to\n    df_test = pd.read_parquet(f'/kaggle/working/test_enriched_v10.parquet')\n    print(f\"Test data shape: {df_test.shape}\")\n    # Load the data dictionary\n    data_dict = pd.read_csv(f'{INPUT_DIR}data_dictionary.csv')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Training sample shape: {df_train.shape}\")\n    print(f\"Test data shape: {df_test.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please run the sampling and feature engineering scripts first.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: DEFINE THE REUSABLE CLUSTERING FUNCTION\n# =============================================================================\nprint(\"\\n--- STAGE 2: Defining the Clustering Pipeline Function ---\")\n\ndef create_cluster_features(df_train, df_test, data_dict, keywords, cluster_col_name, n_clusters=5):\n    \"\"\"\n    Performs a full PCA + K-Means clustering pipeline on a specified group of features\n    and adds the resulting cluster labels to both the training and test dataframes.\n    \"\"\"\n    print(f\"\\n--- Creating cluster feature: '{cluster_col_name}' ---\")\n    \n    # --- a. Select Features ---\n    print(f\"Selecting features based on keywords: {keywords}...\")\n    try:\n        if keywords == 'all':\n            group_features_in_df = [col for col in df_train.columns if col.startswith('f')]\n        else:\n            feature_name_col = 'masked_column'\n            description_col = 'Description'\n            group_mask = data_dict[description_col].str.contains('|'.join(keywords), case=False, na=False)\n            group_features = data_dict[group_mask][feature_name_col].tolist()\n            group_features_in_df = [col for col in group_features if col in df_train.columns]\n        \n        if not group_features_in_df:\n            print(f\"Warning: No features found for group '{cluster_col_name}'. Skipping.\")\n            return df_train, df_test\n            \n        print(f\"Found {len(group_features_in_df)} features for this group.\")\n        X_train_cluster = df_train[group_features_in_df].copy()\n        X_test_cluster = df_test[group_features_in_df].copy()\n    except Exception as e:\n        print(f\"Error selecting features: {e}. Skipping this cluster.\")\n        return df_train, df_test\n\n    # --- b. Preprocess Data ---\n    print(\"Preprocessing data (impute, scale)...\")\n    for col in X_train_cluster.columns:\n        X_train_cluster[col] = pd.to_numeric(X_train_cluster[col], errors='coerce')\n        X_test_cluster[col] = pd.to_numeric(X_test_cluster[col], errors='coerce')\n\n    imputer = SimpleImputer(strategy='median')\n    scaler = StandardScaler()\n    \n    # Fit on training data, transform both train and test\n    X_train_imputed = imputer.fit_transform(X_train_cluster)\n    X_test_imputed = imputer.transform(X_test_cluster)\n    \n    X_train_scaled = scaler.fit_transform(X_train_imputed)\n    X_test_scaled = scaler.transform(X_test_imputed)\n\n    # --- c. PCA ---\n    print(\"Applying PCA...\")\n    pca = PCA(n_components=0.90) # Keep components explaining 90% of variance\n    pca.fit(X_train_scaled)\n    \n    X_train_pca = pca.transform(X_train_scaled)\n    X_test_pca = pca.transform(X_test_scaled)\n    print(f\"PCA resulted in {pca.n_components_} components.\")\n\n    # --- d. K-Means Clustering ---\n    print(\"Applying K-Means...\")\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n    \n    # Fit on training data, predict for both train and test\n    df_train[cluster_col_name] = kmeans.fit_predict(X_train_pca)\n    df_test[cluster_col_name] = kmeans.predict(X_test_pca)\n    \n    print(f\"Successfully created and added '{cluster_col_name}' feature.\")\n    \n    return df_train, df_test\n\n\n# =============================================================================\n# STAGE 3: DEFINE FEATURE GROUPS AND RUN THE PIPELINE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Running the Clustering Pipeline for Each Feature Group ---\")\n\n# --- Define Keyword Groups ---\n# Group 1: Customer Spending Behavior\nspending_keywords = ['debit amount', 'debit transaction', 'spend in']\ndf_train, df_test = create_cluster_features(df_train, df_test, data_dict, spending_keywords, 'spending_cluster')\n\n# Group 2: Customer Click/Impression History\nhistory_keywords = ['ctr in last', 'clicks in last', 'impressions in last']\ndf_train, df_test = create_cluster_features(df_train, df_test, data_dict, history_keywords, 'history_cluster')\n\n# Group 3: Customer Profile & Loyalty\nprofile_keywords = ['membership level', 'account age', 'interest score', 'miles', 'segments']\ndf_train, df_test = create_cluster_features(df_train, df_test, data_dict, profile_keywords, 'profile_cluster')\n\n# Group 4: Customer Engagement on Amex Platforms\nengagement_keywords = ['time spent', 'pages viewed', 'visited', 'logon']\ndf_train, df_test = create_cluster_features(df_train, df_test, data_dict, engagement_keywords, 'engagement_cluster')\n\n# Group 5: All 'f' Features (Holistic)\ndf_train, df_test = create_cluster_features(df_train, df_test, data_dict, 'all', 'holistic_cluster')\n\n\n# =============================================================================\n# STAGE 4: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Final Datasets with New Cluster Features ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = ['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train.columns])\nprint(\"\\nColumns added to the test data:\")\nprint([col for col in new_cols if col in df_test.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v11.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v11.parquet'\n    \n    df_train.to_parquet(train_output_path)\n    df_test.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- CLUSTER FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:48:32.366780Z","iopub.execute_input":"2025-07-20T04:48:32.367080Z","iopub.status.idle":"2025-07-20T04:53:38.423713Z","shell.execute_reply.started":"2025-07-20T04:48:32.367054Z","shell.execute_reply":"2025-07-20T04:53:38.422731Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Data ---\nSuccessfully loaded all required data.\nTraining sample shape: (770164, 383)\n\nLast 15 columns of the training data:\n['f363', 'f364', 'f365', 'f366', 'offer_total_impressions', 'offer_total_clicks', 'offer_historical_ctr', 'offer_redemption_freq', 'offer_discount_rate', 'offer_type_code', 'id8', 'offer_duration_days', 'industry_avg_spend', 'industry_total_transactions', 'industry_unique_products']\nTest data shape: (369301, 382)\nSuccessfully loaded all required data.\nTraining sample shape: (770164, 383)\nTest data shape: (369301, 382)\n\n--- STAGE 2: Defining the Clustering Pipeline Function ---\n\n--- STAGE 3: Running the Clustering Pipeline for Each Feature Group ---\n\n--- Creating cluster feature: 'spending_cluster' ---\nSelecting features based on keywords: ['debit amount', 'debit transaction', 'spend in']...\nFound 47 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 33 components.\nApplying K-Means...\nSuccessfully created and added 'spending_cluster' feature.\n\n--- Creating cluster feature: 'history_cluster' ---\nSelecting features based on keywords: ['ctr in last', 'clicks in last', 'impressions in last']...\nFound 49 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 28 components.\nApplying K-Means...\nSuccessfully created and added 'history_cluster' feature.\n\n--- Creating cluster feature: 'profile_cluster' ---\nSelecting features based on keywords: ['membership level', 'account age', 'interest score', 'miles', 'segments']...\nFound 18 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 13 components.\nApplying K-Means...\nSuccessfully created and added 'profile_cluster' feature.\n\n--- Creating cluster feature: 'engagement_cluster' ---\nSelecting features based on keywords: ['time spent', 'pages viewed', 'visited', 'logon']...\nFound 45 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 22 components.\nApplying K-Means...\nSuccessfully created and added 'engagement_cluster' feature.\n\n--- Creating cluster feature: 'holistic_cluster' ---\nSelecting features based on keywords: all...\nFound 366 features for this group.\nPreprocessing data (impute, scale)...\nApplying PCA...\nPCA resulted in 167 components.\nApplying K-Means...\nSuccessfully created and added 'holistic_cluster' feature.\n\n--- STAGE 4: Saving Final Datasets with New Cluster Features ---\n\nColumns added to the training data:\n['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\n\nColumns added to the test data:\n['spending_cluster', 'history_cluster', 'profile_cluster', 'engagement_cluster', 'holistic_cluster']\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v11.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v11.parquet\n\n--- CLUSTER FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD LATEST DATASETS\n# =============================================================================\nprint(\"--- STAGE 1: Loading Latest Datasets ---\")\ntry:\n    # Load your latest training and test sets with cluster features\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_data_v11.parquet')\n    df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v11.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Input training data shape: {df_train.shape}\")\n    print(f\"Input test data shape: {df_test.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: CREATE TIME-BASED AND INTERACTION FEATURES\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating Time-Based and Interaction Features ---\")\n\ndef create_advanced_features(df):\n    \"\"\"A reusable function to create time-based and interaction features.\"\"\"\n    \n    # --- Time-Based Features ---\n    df['impression_time'] = pd.to_datetime(df['id4'], errors='coerce')\n    df['hour_of_day'] = df['impression_time'].dt.hour\n    df['day_of_week'] = df['impression_time'].dt.dayofweek # Monday=0, Sunday=6\n    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n    \n    # --- NEW: Cyclical Time Features ---\n    # This helps the model understand the cyclical nature of time (e.g., hour 23 is close to hour 0)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day']/24.0)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day']/24.0)\n    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week']/7.0)\n    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week']/7.0)\n\n    # --- NEW: Interaction Features ---\n    # We create these based on hypotheses from our EDA.\n    # For example, is a certain offer type more effective on the weekend?\n    if 'offer_type_code' in df.columns:\n        df['offer_type_x_weekend'] = df['offer_type_code'].astype(str) + \"_\" + df['is_weekend'].astype(str)\n    \n    # How does an offer's historical popularity compare to the average for its type?\n    if 'offer_historical_ctr' in df.columns and 'offer_type_code' in df.columns:\n        avg_ctr_by_type = df.groupby('offer_type_code')['offer_historical_ctr'].transform('mean')\n        df['offer_ctr_vs_type_avg'] = df['offer_historical_ctr'] / (avg_ctr_by_type + 1e-6) # Add epsilon to avoid division by zero\n\n    # This column is no longer needed after feature extraction\n    df = df.drop(columns=['impression_time'])\n    \n    return df\n\n# --- Add features to both the training and test dataframes ---\nprint(\"Enriching training data with new features...\")\ndf_train_final = create_advanced_features(df_train)\nprint(\"Enriching test data with new features...\")\ndf_test_final = create_advanced_features(df_test)\n\n\n# =============================================================================\n# STAGE 3: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Saving Final Datasets with All New Features ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = [\n    'hour_of_day', 'day_of_week', 'is_weekend',\n    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n    'offer_type_x_weekend', 'offer_ctr_vs_type_avg'\n]\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train_final.columns])\nprint(\"\\nColumns added to the test data:\")\nprint([col for col in new_cols if col in df_test_final.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v12.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v12.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- FINAL FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:59:18.294941Z","iopub.execute_input":"2025-07-20T04:59:18.295787Z","iopub.status.idle":"2025-07-20T05:00:19.507368Z","shell.execute_reply.started":"2025-07-20T04:59:18.295755Z","shell.execute_reply":"2025-07-20T05:00:19.506597Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Latest Datasets ---\nSuccessfully loaded all required data.\nInput training data shape: (770164, 388)\nInput test data shape: (369301, 387)\n\n--- STAGE 2: Creating Time-Based and Interaction Features ---\nEnriching training data with new features...\nEnriching test data with new features...\n\n--- STAGE 3: Saving Final Datasets with All New Features ---\n\nColumns added to the training data:\n['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'offer_type_x_weekend', 'offer_ctr_vs_type_avg']\n\nColumns added to the test data:\n['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'offer_type_x_weekend', 'offer_ctr_vs_type_avg']\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v12.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v12.parquet\n\n--- FINAL FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\n# INPUT_DIR1 = '/kaggle/input/amex-v7/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD ALL NECESSARY DATA\n# =============================================================================\nprint(\"--- STAGE 1: Loading All Necessary Data ---\")\ntry:\n    # Load your v7 datasets as the starting point\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_data_v12.parquet')\n    df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v12.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    trans_dd = dd.read_parquet(f'{INPUT_DIR}add_trans.parquet')\n    \n    print(\"Successfully loaded all required data.\")\n    print(f\"Input training data shape: {df_train.shape}\")\n    print(f\"Input test data shape: {df_test.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure v7 files exist and supplementary data is in the input directory.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: CREATE NEW ADVANCED FEATURES\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating New Advanced Features ---\")\n\n# --- Feature 1: Time-to-Click ---\nprint(\"Creating 'Time-to-Click' feature...\")\nevent_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\nevent_dd['click_time'] = dd.to_datetime(event_dd['id7'], errors='coerce')\n# event_dd['time_to_click_seconds'] = (event_dd['click_time'] - event_dd['impression_time']).dt.total_seconds()\n# avg_time_to_click = event_dd.groupby('id3')['time_to_click_seconds'].mean().compute().reset_index()\nprint(\"'Time-to-Click' feature created.\")\n\n# --- Feature 2 & 3: Debit/Credit and Transaction Time Patterns ---\nprint(\"Creating industry-level transaction pattern features...\")\ntrans_dd['f371_hour'] = dd.to_datetime(trans_dd['f371'], format='%H:%M:%S', errors='coerce').dt.hour\ndebit_credit_counts = trans_dd.groupby(['id8', 'f369']).size().compute().unstack(fill_value=0)\nif 'C' not in debit_credit_counts.columns: debit_credit_counts['C'] = 0\nif 'D' not in debit_credit_counts.columns: debit_credit_counts['D'] = 0\ndebit_credit_counts.columns = ['credit_trans_count', 'debit_trans_count']\navg_trans_hour = trans_dd.groupby('id8')['f371_hour'].mean().compute().reset_index(name='avg_trans_hour')\n\n# MODIFIED: Added .reset_index() to ensure 'id8' is a column for a correct merge\nindustry_pattern_features = pd.merge(debit_credit_counts.reset_index(), avg_trans_hour, on='id8', how='outer')\nprint(\"Industry pattern features created.\")\n\n# --- Feature 4: Offer Body Length ---\nprint(\"Creating 'Offer Body Length' feature...\")\noffer_meta_df['offer_body_length'] = offer_meta_df['f378'].str.len()\noffer_body_feature = offer_meta_df[['id3', 'offer_body_length']]\nprint(\"'Offer Body Length' feature created.\")\n\n# Clean up memory\ndel event_dd, trans_dd\ngc.collect()\n\n# =============================================================================\n# STAGE 3: MERGE ALL NEW FEATURES INTO MAIN DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging All New Features ---\")\n\ndef enrich_dataframe(df, avg_time_to_click, offer_meta_df, industry_pattern_features, offer_body_feature):\n    \"\"\"A reusable function to enrich a dataframe with all new features.\"\"\"\n    \n    # Ensure merge keys are the correct type\n    df['id3'] = df['id3'].astype(str)\n    avg_time_to_click['id3'] = avg_time_to_click['id3'].astype(str)\n    offer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\n    offer_meta_df['id8'] = offer_meta_df['id8'].astype(str)\n    industry_pattern_features['id8'] = industry_pattern_features['id8'].astype(str)\n    offer_body_feature['id3'] = offer_body_feature['id3'].astype(str)\n\n    # Merge Time-to-Click\n    df = pd.merge(df, avg_time_to_click, on='id3', how='left')\n    \n    # Merge Industry Patterns (via offer_meta_df)\n    offer_meta_with_patterns = pd.merge(offer_meta_df, industry_pattern_features, on='id8', how='left')\n    df = pd.merge(df, offer_meta_with_patterns[['id3', 'credit_trans_count', 'debit_trans_count', 'avg_trans_hour']], on='id3', how='left')\n\n    # Merge Offer Body Length\n    df = pd.merge(df, offer_body_feature, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, avg_time_to_click, offer_meta_df, industry_pattern_features, offer_body_feature)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_dataframe(df_test, avg_time_to_click, offer_meta_df, industry_pattern_features, offer_body_feature)\n\n# =============================================================================\n# STAGE 4: SAVE THE FINAL ENRICHED DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 4: Saving Final Datasets (v8) ---\")\n\n# --- Verify the new columns were added ---\nnew_cols = [\n    'time_to_click_seconds', 'credit_trans_count', 'debit_trans_count',\n    'avg_trans_hour', 'offer_body_length'\n]\nprint(\"\\nColumns added to the training data:\")\nprint([col for col in new_cols if col in df_train_final.columns])\nprint(\"\\nColumns added to the test data:\")\nprint([col for col in new_cols if col in df_test_final.columns])\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v13.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v13.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- FINAL FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:02:56.172356Z","iopub.execute_input":"2025-07-20T05:02:56.173347Z","iopub.status.idle":"2025-07-20T05:04:49.334316Z","shell.execute_reply.started":"2025-07-20T05:02:56.173309Z","shell.execute_reply":"2025-07-20T05:04:49.333553Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading All Necessary Data ---\nSuccessfully loaded all required data.\nInput training data shape: (770164, 397)\nInput test data shape: (369301, 396)\n\n--- STAGE 2: Creating New Advanced Features ---\nCreating 'Time-to-Click' feature...\n'Time-to-Click' feature created.\nCreating industry-level transaction pattern features...\nIndustry pattern features created.\nCreating 'Offer Body Length' feature...\n'Offer Body Length' feature created.\n\n--- STAGE 3: Merging All New Features ---\nEnriching training data...\nEnriching test data...\n\n--- STAGE 4: Saving Final Datasets (v8) ---\n\nColumns added to the training data:\n['time_to_click_seconds', 'credit_trans_count', 'debit_trans_count', 'avg_trans_hour', 'offer_body_length']\n\nColumns added to the test data:\n['time_to_click_seconds', 'credit_trans_count', 'debit_trans_count', 'avg_trans_hour', 'offer_body_length']\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v13.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v13.parquet\n\n--- FINAL FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD THE LATEST TRAINING DATA AND IDENTIFY ZERO-VARIANCE COLUMNS\n# =============================================================================\nprint(\"--- STAGE 1: Analyzing Training Data for Zero-Variance Features ---\")\ntry:\n    # Load your latest training set\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_data_v13.parquet')\n    print(\"Successfully loaded train_data_v81.parquet.\")\n    print(f\"Original training data shape: {df_train.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: train_data_v81.parquet not found. Please ensure the previous feature engineering script ran successfully.\")\n    exit()\n\n# --- Identify columns with zero variance based on the new, stricter condition ---\n# A column is now considered zero-variance only if it has one unique value AND no missing values.\ncols_to_drop = []\nfor col in df_train.columns:\n    # Skip identifier columns and the target\n    if col in ['id1', 'id2', 'id3', 'id4', 'id5', 'y']:\n        continue\n    \n    # The new condition: exactly one unique value AND no NaNs.\n    if df_train[col].nunique() == 1 and df_train[col].isnull().sum() == 0:\n        cols_to_drop.append(col)\n\nprint(f\"\\nFound {len(cols_to_drop)} columns with zero variance to remove based on the new condition.\")\nif cols_to_drop:\n    print(\"Columns to be removed:\", cols_to_drop)\n\n# --- Drop the identified columns ---\ndf_train_v9 = df_train.drop(columns=cols_to_drop)\nprint(f\"\\nNew training data shape after removing zero-variance columns: {df_train_v9.shape}\")\n\n\n# =============================================================================\n# STAGE 2: APPLY THE SAME CLEANING TO THE TEST DATA\n# =============================================================================\nprint(\"\\n--- STAGE 2: Applying the Same Cleaning to the Test Data ---\")\ntry:\n    # Load your latest test set\n    df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v13.parquet')\n    print(\"Successfully loaded test_data_v81.parquet.\")\n    print(f\"Original test data shape: {df_test.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: test_data_v81.parquet not found.\")\n    exit()\n\n# --- Drop the same columns from the test set for consistency ---\n# We need to find which of the columns to drop actually exist in the test set\ncols_to_drop_in_test = [col for col in cols_to_drop if col in df_test.columns]\ndf_test_v9 = df_test.drop(columns=cols_to_drop_in_test)\nprint(f\"\\nRemoved {len(cols_to_drop_in_test)} columns from the test set.\")\nprint(f\"New test data shape: {df_test_v9.shape}\")\n\n\n# =============================================================================\n# STAGE 3: SAVE THE FINAL (v9) DATASETS\n# =============================================================================\nprint(\"\\n--- STAGE 3: Saving Final v9 Datasets ---\")\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v14.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v14.parquet'\n    \n    df_train_v9.to_parquet(train_output_path)\n    df_test_v9.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- FINAL CLEANING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:05:29.492205Z","iopub.execute_input":"2025-07-20T05:05:29.493128Z","iopub.status.idle":"2025-07-20T05:06:35.281427Z","shell.execute_reply.started":"2025-07-20T05:05:29.493096Z","shell.execute_reply":"2025-07-20T05:06:35.280622Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Analyzing Training Data for Zero-Variance Features ---\nSuccessfully loaded train_data_v81.parquet.\nOriginal training data shape: (770164, 402)\n\nFound 1 columns with zero variance to remove based on the new condition.\nColumns to be removed: ['is_weekend']\n\nNew training data shape after removing zero-variance columns: (770164, 401)\n\n--- STAGE 2: Applying the Same Cleaning to the Test Data ---\nSuccessfully loaded test_data_v81.parquet.\nOriginal test data shape: (369301, 401)\n\nRemoved 1 columns from the test set.\nNew test data shape: (369301, 400)\n\n--- STAGE 3: Saving Final v9 Datasets ---\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v14.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v14.parquet\n\n--- FINAL CLEANING COMPLETE ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport gc\nimport os\n\n# Modeling libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"--- STAGE 1: Loading Data ---\")\ntry:\n    # Load your latest training and test sets\n    df_train = pd.read_parquet(f'{WORKING_DIR}train_data_v14.parquet')\n    df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v14.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_df = pd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    trans_df = pd.read_parquet(f'{INPUT_DIR}add_trans.parquet')\n    \n    print(\"Successfully loaded all required data.\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: OFFER CLUSTERING\n# =============================================================================\nprint(\"\\n--- STAGE 2: Creating Offer Clusters ---\")\n\n# --- a. Create features for each offer ---\nprint(\"Aggregating historical features for each offer...\")\nevent_df['impression_time'] = pd.to_datetime(event_df['id4'], errors='coerce')\nevent_df['hour_of_day'] = event_df['impression_time'].dt.hour\nevent_df['is_weekend'] = (event_df['impression_time'].dt.dayofweek >= 5).astype(int)\nevent_df['clicked'] = event_df['id7'].notna().astype(int)\n\n# Aggregate stats for each offer\noffer_features = event_df.groupby('id3').agg(\n    offer_historical_ctr=('clicked', 'mean'),\n    offer_total_impressions=('id4', 'count'),\n    avg_impression_hour=('hour_of_day', 'mean'),\n    weekend_impression_ratio=('is_weekend', 'mean')\n).reset_index()\n\n# Merge with static offer metadata\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\noffer_features['id3'] = offer_features['id3'].astype(str)\noffer_features = pd.merge(offer_features, offer_meta_df[['id3', 'f375', 'f376']], on='id3', how='left')\noffer_features = offer_features.rename(columns={'f375': 'offer_redemption_freq', 'f376': 'offer_discount_rate'})\n\n# --- b. Preprocess and Cluster Offers ---\nprint(\"Preprocessing and clustering offers...\")\noffer_feature_cols = [col for col in offer_features.columns if col != 'id3']\nX_offer = offer_features[offer_feature_cols].copy()\n\nimputer = SimpleImputer(strategy='median')\nX_offer_imputed = imputer.fit_transform(X_offer)\nscaler = StandardScaler()\nX_offer_scaled = scaler.fit_transform(X_offer_imputed)\n\npca = PCA(n_components=0.95) # Explain 95% of variance\nX_offer_pca = pca.fit_transform(X_offer_scaled)\nprint(f\"Offer PCA resulted in {pca.n_components_} components.\")\n\nkmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')\noffer_features['offer_cluster'] = kmeans.fit_predict(X_offer_pca)\nprint(\"Offer clusters created successfully.\")\n\n\n# =============================================================================\n# STAGE 3: INDUSTRY CLUSTERING\n# =============================================================================\nprint(\"\\n--- STAGE 3: Creating Industry Clusters ---\")\n\n# --- a. Create features for each industry ---\nprint(\"Aggregating historical features for each industry...\")\ntrans_df['f367'] = pd.to_numeric(trans_df['f367'], errors='coerce')\nindustry_features = trans_df.groupby('id8').agg(\n    industry_avg_spend=('f367', 'mean'),\n    industry_std_spend=('f367', 'std'),\n    industry_total_transactions=('f367', 'count'),\n    industry_unique_products=('f368', 'nunique')\n).reset_index()\n\n# --- b. Preprocess and Cluster Industries ---\nprint(\"Preprocessing and clustering industries...\")\nindustry_feature_cols = [col for col in industry_features.columns if col != 'id8']\nX_industry = industry_features[industry_feature_cols].copy()\n\nimputer = SimpleImputer(strategy='median')\nX_industry_imputed = imputer.fit_transform(X_industry)\nscaler = StandardScaler()\nX_industry_scaled = scaler.fit_transform(X_industry_imputed)\n\npca = PCA(n_components=0.95)\nX_industry_pca = pca.fit_transform(X_industry_scaled)\nprint(f\"Industry PCA resulted in {pca.n_components_} components.\")\n\nkmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')\nindustry_features['industry_cluster'] = kmeans.fit_predict(X_industry_pca)\nprint(\"Industry clusters created successfully.\")\n\n\n# =============================================================================\n# STAGE 4: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 4: Merging New Cluster Features and Saving ---\")\n\n# --- Link industry clusters to offers ---\noffer_meta_df['id8'] = offer_meta_df['id8'].astype(str)\nindustry_features['id8'] = industry_features['id8'].astype(str)\noffer_meta_with_clusters = pd.merge(offer_meta_df, industry_features[['id8', 'industry_cluster']], on='id8', how='left')\n\n# --- Merge all new features into train and test sets ---\ndef enrich_with_entity_clusters(df, offer_features, offer_meta_with_clusters):\n    df['id3'] = df['id3'].astype(str)\n    # Merge offer clusters\n    df = pd.merge(df, offer_features[['id3', 'offer_cluster']], on='id3', how='left')\n    # Merge industry clusters (via offer metadata)\n    df = pd.merge(df, offer_meta_with_clusters[['id3', 'industry_cluster']], on='id3', how='left')\n    return df\n\nprint(\"Enriching training data...\")\ndf_train_final = enrich_with_entity_clusters(df_train, offer_features, offer_meta_with_clusters)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_with_entity_clusters(df_test, offer_features, offer_meta_with_clusters)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v15.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v15.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ENTITY CLUSTER FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:07:39.146152Z","iopub.execute_input":"2025-07-20T05:07:39.146928Z","iopub.status.idle":"2025-07-20T05:09:25.315072Z","shell.execute_reply.started":"2025-07-20T05:07:39.146899Z","shell.execute_reply":"2025-07-20T05:09:25.314096Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- STAGE 2: Creating Offer Clusters ---\nAggregating historical features for each offer...\nPreprocessing and clustering offers...\nOffer PCA resulted in 6 components.\nOffer clusters created successfully.\n\n--- STAGE 3: Creating Industry Clusters ---\nAggregating historical features for each industry...\nPreprocessing and clustering industries...\nIndustry PCA resulted in 4 components.\nIndustry clusters created successfully.\n\n--- STAGE 4: Merging New Cluster Features and Saving ---\nEnriching training data...\nEnriching test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v15.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v15.parquet\n\n--- ENTITY CLUSTER FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_train_final.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:10:05.739628Z","iopub.execute_input":"2025-07-20T05:10:05.739945Z","iopub.status.idle":"2025-07-20T05:10:05.745927Z","shell.execute_reply.started":"2025-07-20T05:10:05.739923Z","shell.execute_reply":"2025-07-20T05:10:05.745206Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(770164, 403)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_train_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:10:14.647700Z","iopub.execute_input":"2025-07-20T05:10:14.648319Z","iopub.status.idle":"2025-07-20T05:10:14.670438Z","shell.execute_reply.started":"2025-07-20T05:10:14.648292Z","shell.execute_reply":"2025-07-20T05:10:14.669862Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                               id1      id2        id3  \\\n0  1366776_189706075_16-23_2023-11-02 22:22:00.042  1366776  189706075   \n1      1366776_89227_16-23_2023-11-01 23:51:24.999  1366776      89227   \n2      1366776_35046_16-23_2023-11-01 00:30:59.797  1366776      35046   \n3    1366776_6275451_16-23_2023-11-02 22:21:32.261  1366776    6275451   \n4      1366776_78053_16-23_2023-11-02 22:21:34.799  1366776      78053   \n\n                       id4         id5  y   f1    f2    f3    f4  ...  \\\n0  2023-11-02 22:22:00.042  2023-11-02  0  1.0  None  None  None  ...   \n1  2023-11-01 23:51:24.999  2023-11-01  0  1.0  None  None  None  ...   \n2  2023-11-01 00:30:59.797  2023-11-01  0  1.0  None  None  None  ...   \n3  2023-11-02 22:21:32.261  2023-11-02  0  1.0  None  None  None  ...   \n4  2023-11-02 22:21:34.799  2023-11-02  0  1.0  None  None  None  ...   \n\n    day_cos offer_type_x_weekend offer_ctr_vs_type_avg time_to_click_seconds  \\\n0 -0.900969                  1_0              2.115142             40.884705   \n1 -0.222521                  1_0              1.642199             55.625598   \n2 -0.222521                  1_0              1.465480             60.164669   \n3 -0.900969                  1_0              1.512121             65.115491   \n4 -0.900969                  1_0              1.502907             62.968459   \n\n  credit_trans_count debit_trans_count avg_trans_hour offer_body_length  \\\n0            34916.0            2556.0       8.491300              31.0   \n1            24392.0             205.0       7.362727              37.0   \n2            19232.0             177.0      10.023546              31.0   \n3            49247.0            7817.0      11.758745              31.0   \n4            13298.0             707.0       9.883684              31.0   \n\n  offer_cluster industry_cluster  \n0             4              1.0  \n1             4              1.0  \n2             4              1.0  \n3             4              1.0  \n4             4              1.0  \n\n[5 rows x 403 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id1</th>\n      <th>id2</th>\n      <th>id3</th>\n      <th>id4</th>\n      <th>id5</th>\n      <th>y</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>...</th>\n      <th>day_cos</th>\n      <th>offer_type_x_weekend</th>\n      <th>offer_ctr_vs_type_avg</th>\n      <th>time_to_click_seconds</th>\n      <th>credit_trans_count</th>\n      <th>debit_trans_count</th>\n      <th>avg_trans_hour</th>\n      <th>offer_body_length</th>\n      <th>offer_cluster</th>\n      <th>industry_cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1366776_189706075_16-23_2023-11-02 22:22:00.042</td>\n      <td>1366776</td>\n      <td>189706075</td>\n      <td>2023-11-02 22:22:00.042</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>-0.900969</td>\n      <td>1_0</td>\n      <td>2.115142</td>\n      <td>40.884705</td>\n      <td>34916.0</td>\n      <td>2556.0</td>\n      <td>8.491300</td>\n      <td>31.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1366776_89227_16-23_2023-11-01 23:51:24.999</td>\n      <td>1366776</td>\n      <td>89227</td>\n      <td>2023-11-01 23:51:24.999</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>-0.222521</td>\n      <td>1_0</td>\n      <td>1.642199</td>\n      <td>55.625598</td>\n      <td>24392.0</td>\n      <td>205.0</td>\n      <td>7.362727</td>\n      <td>37.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1366776_35046_16-23_2023-11-01 00:30:59.797</td>\n      <td>1366776</td>\n      <td>35046</td>\n      <td>2023-11-01 00:30:59.797</td>\n      <td>2023-11-01</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>-0.222521</td>\n      <td>1_0</td>\n      <td>1.465480</td>\n      <td>60.164669</td>\n      <td>19232.0</td>\n      <td>177.0</td>\n      <td>10.023546</td>\n      <td>31.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1366776_6275451_16-23_2023-11-02 22:21:32.261</td>\n      <td>1366776</td>\n      <td>6275451</td>\n      <td>2023-11-02 22:21:32.261</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>-0.900969</td>\n      <td>1_0</td>\n      <td>1.512121</td>\n      <td>65.115491</td>\n      <td>49247.0</td>\n      <td>7817.0</td>\n      <td>11.758745</td>\n      <td>31.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1366776_78053_16-23_2023-11-02 22:21:34.799</td>\n      <td>1366776</td>\n      <td>78053</td>\n      <td>2023-11-02 22:21:34.799</td>\n      <td>2023-11-02</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>-0.900969</td>\n      <td>1_0</td>\n      <td>1.502907</td>\n      <td>62.968459</td>\n      <td>13298.0</td>\n      <td>707.0</td>\n      <td>9.883684</td>\n      <td>31.0</td>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 403 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import gc\n\n# List of common large objects we created in previous steps\n# Add any other large variables you might have created\nvariables_to_delete = [\n    'df', 'df_test', 'X', 'y', 'groups', 'df_train', 'df_test_refined',\n    'df_sample', 'X_cluster', 'X_scaled', 'X_pca', 'df_cluster', 'df_train_v9', 'df_test_v9',\n    'rfm_df', 'trans_dd', 'event_dd', 'offer_meta_df', 'data_dict', 'df_train_final', 'df_test_final'\n]\n\nprint(\"--- Clearing RAM before training ---\")\nfor var_name in variables_to_delete:\n    if var_name in globals():\n        print(f\"Deleting: {var_name}\")\n        del globals()[var_name]\n\n# Call the garbage collector to release the memory\ngc.collect()\nprint(\"RAM cleared successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T05:29:15.350598Z","iopub.execute_input":"2025-07-20T05:29:15.351586Z","iopub.status.idle":"2025-07-20T05:29:16.951472Z","shell.execute_reply.started":"2025-07-20T05:29:15.351543Z","shell.execute_reply":"2025-07-20T05:29:16.950785Z"}},"outputs":[{"name":"stdout","text":"--- Clearing RAM before training ---\nDeleting: df_train_v9\nDeleting: df_test_v9\nDeleting: df_train_final\nDeleting: df_test_final\nRAM cleared successfully.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n# =============================================================================\n# STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# =============================================================================\nprint(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\ntry:\n    # Load your final, pre-cleaned training and test data\n    df = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n    df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    print(\"Successfully loaded final training and test data.\")\nexcept FileNotFoundError:\n    print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:04:25.268831Z","iopub.execute_input":"2025-07-20T06:04:25.269403Z","iopub.status.idle":"2025-07-20T06:04:44.967987Z","shell.execute_reply.started":"2025-07-20T06:04:25.269375Z","shell.execute_reply":"2025-07-20T06:04:44.967215Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\nSuccessfully loaded final training and test data.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = df.drop(columns=['time_to_click_seconds'], errors='ignore')\ndf_test = df_test.drop(columns=['time_to_click_seconds'], errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:11:40.082245Z","iopub.execute_input":"2025-07-20T06:11:40.082860Z","iopub.status.idle":"2025-07-20T06:11:45.157227Z","shell.execute_reply.started":"2025-07-20T06:11:40.082834Z","shell.execute_reply":"2025-07-20T06:11:45.156603Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 0: DEFINE THE MAP@7 EVALUATION METRIC\n# =============================================================================\ndef map_at_k(y_true, y_pred, group_ids, k=7):\n    \"\"\"\n    Calculates the Mean Average Precision at k.\n    \n    Args:\n        y_true (array-like): The true relevance labels (0 or 1).\n        y_pred (array-like): The predicted ranking scores.\n        group_ids (array-like): The group/query IDs for each sample.\n        k (int): The cutoff for the precision calculation.\n        \n    Returns:\n        float: The Mean Average Precision @ k score.\n    \"\"\"\n    df = pd.DataFrame({'group': group_ids, 'y_true': y_true, 'y_pred': y_pred})\n    \n    average_precisions = []\n    for group in df['group'].unique():\n        group_df = df[df['group'] == group].sort_values('y_pred', ascending=False).reset_index(drop=True)\n        \n        # Get the top k predictions\n        group_df = group_df.head(k)\n        \n        if group_df['y_true'].sum() == 0:\n            average_precisions.append(0)\n            continue\n            \n        relevant_hits = 0\n        precision_at_i = []\n        for i, row in group_df.iterrows():\n            if row['y_true'] == 1:\n                relevant_hits += 1\n                precision_at_i.append(relevant_hits / (i + 1))\n        \n        if not precision_at_i:\n            average_precisions.append(0)\n        else:\n            average_precisions.append(np.mean(precision_at_i))\n            \n    return np.mean(average_precisions)\n\n\n# # =============================================================================\n# # STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# # =============================================================================\n# print(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\n# try:\n#     # Load your final, pre-cleaned training and test data\n#     df = pd.read_parquet(f'{WORKING_DIR}train_data_v92.parquet')\n#     df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v92.parquet')\n#     print(\"Successfully loaded final training (v92) and test (v92) data.\")\n# except FileNotFoundError:\n#     print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n#     exit()\n\n# --- Prepare Feature and Target Names ---\ntarget_col = 'y'\n# All columns are features except for identifiers and the target\nfeature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n# Identify all categorical features to be one-hot encoded\ncategorical_features = [col for col in df.columns if 'cluster' in col or col == 'offer_type_code' or col == 'offer_type_x_weekend']\n\n# --- Preprocessing ---\nprint(\"Preprocessing data for training...\")\n# Handle one-hot encoding for categorical features\ndf = pd.get_dummies(df, columns=categorical_features, dummy_na=True)\ndf_test = pd.get_dummies(df_test, columns=categorical_features, dummy_na=True)\n\n# Update feature list after one-hot encoding\nfor cat_col in categorical_features:\n    if cat_col in feature_cols:\n        feature_cols.remove(cat_col)\ndummy_cols = [col for col in df.columns if any(cat_col in col for cat_col in categorical_features)]\nfeature_cols.extend(dummy_cols)\n# Remove any duplicates that might have been added\nfeature_cols = list(dict.fromkeys(feature_cols)) \n\n# Convert all feature columns to numeric before imputation\nprint(\"Converting all feature columns to numeric types...\")\nfor col in feature_cols:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    if col in df_test.columns:\n        df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n\n# Final imputation for training data\ndf[feature_cols] = df[feature_cols].fillna(-999)\n\n# --- Create Groups for XGBRanker ---\n# A \"query\" or \"group\" is a single ranking task (all offers for one customer on one day)\ndf['group_id'] = df['id2'].astype(str) + '_' + df['id5'].astype(str)\ndf = df.sort_values('group_id').reset_index(drop=True)\ngroup_sizes = df.groupby('group_id')['id1'].count().to_numpy()\n\n# Define features (X) and target (y)\nX = df[feature_cols]\ny = df[target_col].astype(int)\nprint(f\"Prepared training data with {len(feature_cols)} features and {len(group_sizes)} groups.\")\n\n\n# =============================================================================\n# STAGE 2: TRAINING FINAL ENSEMBLE WITH XGBRANKER\n# =============================================================================\nprint(\"\\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\")\n\n# Define XGBRanker parameters. Note the 'rank:ndcg' objective.\nfinal_params = {\n    'objective': 'rank:ndcg', # Learning-to-rank objective\n    'eval_metric': 'ndcg@7',  # Evaluate using a metric similar to the competition\n    'use_label_encoder': False, 'seed': 42,\n    'tree_method': 'gpu_hist', 'gpu_id': 0,\n    'n_estimators': 1000, \n    'learning_rate': 0.05, \n    'max_depth': 7, \n    'subsample': 0.8, \n    'colsample_bytree': 0.8\n}\n\nN_SPLITS = 5 # Using 5 folds for a balance of stability and speed\ngkf_train = GroupKFold(n_splits=N_SPLITS)\noof_predictions = np.zeros(len(df))\noof_map_scores = []\n\nprint(f\"Starting final training with {N_SPLITS}-Fold GroupKFold on GPU...\")\n# For XGBRanker, we must use the customer ID for the GroupKFold split\ngroups_for_split = df['id2']\n\n# MODIFIED: Corrected the variable name from 'for_split' to 'groups_for_split'\nfor fold, (train_idx, val_idx) in enumerate(gkf_train.split(X, y, groups=groups_for_split)):\n    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n    X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n    X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # Get the group sizes for the training and validation sets\n    train_groups = df.iloc[train_idx].groupby('group_id')['id1'].count().to_numpy()\n    val_groups = df.iloc[val_idx].groupby('group_id')['id1'].count().to_numpy()\n    \n    model = xgb.XGBRanker(**final_params)\n    model.fit(X_train_fold, y_train_fold, group=train_groups,\n              eval_set=[(X_val_fold, y_val_fold)], eval_group=[val_groups],\n              early_stopping_rounds=50, verbose=False)\n    \n    # XGBRanker outputs scores, not probabilities\n    val_preds = model.predict(X_val_fold)\n    oof_predictions[val_idx] = val_preds\n\n    # --- Calculate and store MAP@7 for this fold ---\n    fold_map_score = map_at_k(y_val_fold, val_preds, df.iloc[val_idx]['group_id'], k=7)\n    oof_map_scores.append(fold_map_score)\n    print(f\"Fold {fold+1} MAP@7 Score: {fold_map_score:.5f}\")\n\n    model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold+1}.json'\n    model.save_model(model_path)\n    print(f\"Model for fold {fold+1} saved to {model_path}\")\n    del X_train_fold, y_train_fold, X_val_fold, y_val_fold, model\n    gc.collect()\n\n# --- Evaluate Overall Performance ---\nprint(f\"\\n--- Overall Cross-Validation Results ---\")\nprint(f\"Mean Out-of-Fold (OOF) MAP@7 Score: {np.mean(oof_map_scores):.5f}\")\nprint(\"This score is the most reliable estimate of your final leaderboard performance.\")\n\n\n# =============================================================================\n# STAGE 3: PREDICTION AND SUBMISSION FILE CREATION\n# =============================================================================\nprint(\"\\n--- STAGE 3: Generating Final Predictions ---\")\n# Prepare test data\nsubmission_ids = df_test[['id1', 'id2', 'id3', 'id5']].copy()\n# Align test columns with the training columns\nX_test = df_test.reindex(columns=X.columns, fill_value=0)\n# Final imputation for test data\nX_test = X_test.fillna(-999)\n\n# Load models and predict\ntest_predictions = np.zeros(len(X_test))\nfor fold in range(1, N_SPLITS + 1):\n    print(f\"Predicting with Fold {fold}/{N_SPLITS}...\")\n    model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold}.json'\n    model = xgb.XGBRanker()\n    model.load_model(model_path)\n    # Predict outputs ranking scores\n    test_predictions += model.predict(X_test) / N_SPLITS\n\n# Create submission file\nprint(\"\\nCreating submission file...\")\nsubmission_df = submission_ids.copy()\nsubmission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\ncleaned_predictions = np.nan_to_num(test_predictions, nan=-999) # Use a low score for NaNs\nsubmission_df['pred'] = cleaned_predictions\n\nsubmission_path = f'{WORKING_DIR}r2_submission_final_ranker.csv'\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"\\nSubmission file successfully saved to: {submission_path}\")\nprint(\"\\nFirst 5 rows of the submission file:\")\nprint(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:15:41.002604Z","iopub.execute_input":"2025-07-20T06:15:41.002900Z","iopub.status.idle":"2025-07-20T06:32:41.484281Z","shell.execute_reply.started":"2025-07-20T06:15:41.002877Z","shell.execute_reply":"2025-07-20T06:32:41.483457Z"}},"outputs":[{"name":"stdout","text":"\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\nStarting final training with 5-Fold GroupKFold on GPU...\n--- Fold 1/5 ---\nFold 1 MAP@7 Score: 0.06066\nModel for fold 1 saved to /kaggle/working/final_ranker_model_fold_1.json\n--- Fold 2/5 ---\nFold 2 MAP@7 Score: 0.06649\nModel for fold 2 saved to /kaggle/working/final_ranker_model_fold_2.json\n--- Fold 3/5 ---\nFold 3 MAP@7 Score: 0.06375\nModel for fold 3 saved to /kaggle/working/final_ranker_model_fold_3.json\n--- Fold 4/5 ---\nFold 4 MAP@7 Score: 0.06472\nModel for fold 4 saved to /kaggle/working/final_ranker_model_fold_4.json\n--- Fold 5/5 ---\nFold 5 MAP@7 Score: 0.06350\nModel for fold 5 saved to /kaggle/working/final_ranker_model_fold_5.json\n\n--- Overall Cross-Validation Results ---\nMean Out-of-Fold (OOF) MAP@7 Score: 0.06382\nThis score is the most reliable estimate of your final leaderboard performance.\n\n--- STAGE 3: Generating Final Predictions ---\nPredicting with Fold 1/5...\nPredicting with Fold 2/5...\nPredicting with Fold 3/5...\nPredicting with Fold 4/5...\nPredicting with Fold 5/5...\n\nCreating submission file...\n\nSubmission file successfully saved to: /kaggle/working/r2_submission_final_ranker.csv\n\nFirst 5 rows of the submission file:\n                                               id1      id2     id3  \\\n0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n\n          id5      pred  \n0  11-04-2023 -1.690269  \n1  11-04-2023 -1.326085  \n2  11-05-2023  0.407397  \n3  11-04-2023 -2.410089  \n4  11-05-2023 -2.573299  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary libraries for graph and sequential embeddings\n!pip install -q node2vec gensim\n\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\nimport networkx as nx\nfrom node2vec import Node2Vec\nfrom gensim.models import Word2Vec\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load your latest training and test sets\n    df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n    df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n    # Use Dask for the large event log\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet', columns=['id2', 'id3', 'id4'])\n    \n    print(\"Successfully loaded all required data.\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# TECHNIQUE 1: GRAPH EMBEDDINGS (NODEVEC)\n# =============================================================================\nprint(\"\\n--- Technique 1: Creating Graph Embedding Features ---\")\n\n# --- Step 1a: Build the Interaction Graph ---\nprint(\"Step 1a: Building the customer-offer interaction graph...\")\n# Compute the edge list (customer, offer pairs) from the Dask DataFrame\n# This is the most memory-intensive part of this stage\nedge_list = event_dd[['id2', 'id3']].compute()\n# Create a graph from the edge list\nG = nx.from_pandas_edgelist(edge_list, 'id2', 'id3')\nprint(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n\n# --- Step 1b: Train the Node2Vec Model ---\nprint(\"Step 1b: Training the Node2Vec model (this will take a long time)...\")\n# Initialize Node2Vec. Parameters can be tuned.\nnode2vec = Node2Vec(G, dimensions=32, walk_length=20, num_walks=10, workers=4, quiet=True)\n# Train the model\nmodel_n2v = node2vec.fit(window=5, min_count=1, batch_words=4)\nprint(\"Node2Vec model trained successfully.\")\n\n# --- Step 1c: Extract Offer Embeddings ---\nprint(\"Step 1c: Extracting offer embeddings...\")\noffer_ids = df_train['id3'].unique().tolist() + df_test['id3'].unique().tolist()\noffer_ids = list(set(offer_ids)) # Get unique offer IDs across train and test\n\ngraph_embeddings = []\nfor offer_id in offer_ids:\n    try:\n        vector = model_n2v.wv[offer_id]\n        graph_embeddings.append([offer_id] + vector.tolist())\n    except KeyError:\n        # This offer was not in the historical graph\n        continue\n\ngraph_embedding_features = pd.DataFrame(graph_embeddings, columns=['id3'] + [f'graph_emb_{i}' for i in range(32)])\nprint(\"Graph embedding features created successfully.\")\n\n# Clean up memory\ndel G, edge_list, model_n2v, node2vec\ngc.collect()\n\n# =============================================================================\n# TECHNIQUE 2: SEQUENTIAL EMBEDDINGS (WORD2VEC)\n# =============================================================================\nprint(\"\\n--- Technique 2: Creating Sequential Embedding Features ---\")\n\n# --- Step 2a: Create Customer Sessions ---\nprint(\"Step 2a: Creating customer sessions...\")\nevent_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\n\n# MODIFIED: Use a robust, Dask-native method to create time-based sessions\n# Sort by customer and time\nevent_dd = event_dd.set_index('impression_time').sort_index()\nevent_dd = event_dd.reset_index()\nevent_dd = event_dd.set_index('id2').sort_index()\n\n# Calculate time difference between consecutive events for each customer\nevent_dd['time_diff'] = event_dd.groupby('id2')['impression_time'].diff().dt.total_seconds()\n\n# A new session starts if the time difference is > 30 minutes (1800 seconds)\nevent_dd['new_session'] = (event_dd['time_diff'] > 1800).fillna(True)\nevent_dd['session_id'] = event_dd.groupby('id2')['new_session'].cumsum()\n\n# Create the list of sessions (sequences of offers)\nsessions = event_dd.groupby(['id2', 'session_id'])['id3'].apply(list, meta=('id3', 'object')).compute().tolist()\nprint(f\"Created {len(sessions)} customer sessions.\")\n\n# --- Step 2b: Train the Word2Vec Model ---\nprint(\"Step 2b: Training the Word2Vec model...\")\n# Train the model on the sessions. Treats offers as \"words\" and sessions as \"sentences\".\nmodel_w2v = Word2Vec(sentences=sessions, vector_size=32, window=5, min_count=1, workers=4)\nprint(\"Word2Vec model trained successfully.\")\n\n# --- Step 2c: Extract Offer Embeddings ---\nprint(\"Step 2c: Extracting sequential embeddings...\")\nsequential_embeddings = []\nfor offer_id in offer_ids:\n    try:\n        vector = model_w2v.wv[offer_id]\n        sequential_embeddings.append([offer_id] + vector.tolist())\n    except KeyError:\n        continue\n\nsequential_embedding_features = pd.DataFrame(sequential_embeddings, columns=['id3'] + [f'seq_emb_{i}' for i in range(32)])\nprint(\"Sequential embedding features created successfully.\")\n\n# Clean up memory\ndel sessions, model_w2v\ngc.collect()\n\n# =============================================================================\n# STAGE 3: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v11) ---\")\n\ndef enrich_dataframe(df, graph_embedding_features, sequential_embedding_features):\n    \"\"\"A reusable function to enrich a dataframe with new embedding features.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    graph_embedding_features['id3'] = graph_embedding_features['id3'].astype(str)\n    sequential_embedding_features['id3'] = sequential_embedding_features['id3'].astype(str)\n    \n    # Merge graph embedding features\n    df = pd.merge(df, graph_embedding_features, on='id3', how='left')\n    # Merge sequential embedding features\n    df = pd.merge(df, sequential_embedding_features, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, graph_embedding_features, sequential_embedding_features)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_dataframe(df_test, graph_embedding_features, sequential_embedding_features)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v16.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v16.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED EMBEDDING FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T12:42:52.605810Z","iopub.execute_input":"2025-07-20T12:42:52.606100Z","execution_failed":"2025-07-20T14:00:04.747Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Graph Embedding Features ---\nStep 1a: Building the customer-offer interaction graph...\nGraph created with 429118 nodes and 12793562 edges.\nStep 1b: Training the Node2Vec model (this will take a long time)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary library for text embeddings\n!pip install -q sentence-transformers\n\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\nfrom sentence_transformers import SentenceTransformer\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load your latest training and test sets\n    df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n    df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    \n    print(\"Successfully loaded all required data.\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# TECHNIQUE 1: CO-CLICK & SEQUENTIAL FEATURES (MEMORY-EFFICIENT)\n# =============================================================================\nprint(\"\\n--- Technique 1: Creating Co-Click Features ---\")\n\n# --- Step 1a: Create Customer Sessions on a Sample ---\nprint(\"Step 1a: Creating customer sessions on a sample of the data...\")\n# Use a large sample of the event data to make the process much faster\nevent_dd_sample = event_dd.sample(frac=0.15, random_state=42)\nevent_dd_sample['impression_time'] = dd.to_datetime(event_dd_sample['id4'], errors='coerce')\n\n# Use a robust, Dask-native method to create time-based sessions\nprint(\"Sorting events by customer and time...\")\nevent_dd_sample = event_dd_sample.sort_values(['id2', 'impression_time'])\n\ndef calculate_time_diff(df):\n    df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n    return df\n\nmeta = event_dd_sample._meta.copy()\nmeta['time_diff'] = pd.Series(dtype='float64')\nevent_dd_sample = event_dd_sample.map_partitions(calculate_time_diff, meta=meta)\n\nevent_dd_sample['new_session'] = (event_dd_sample['time_diff'] > 1800).fillna(True)\nevent_dd_sample['session_id'] = event_dd_sample.groupby('id2')['new_session'].cumsum()\nprint(\"Customer sessions created.\")\n\n\n# --- Step 1b: Calculate Co-occurrence (Fully with Dask) ---\nprint(\"Step 1b: Calculating offer co-occurrence within sessions using Dask...\")\n# MODIFIED: Perform the entire co-occurrence calculation within Dask's lazy framework\nsession_offers_dd = event_dd_sample[['session_id', 'id2', 'id3']]\n# Perform a Dask self-merge\nmerged_sessions_dd = dd.merge(session_offers_dd, session_offers_dd, on=['session_id', 'id2'])\n# Filter out self-pairs\nco_occurrence_dd = merged_sessions_dd[merged_sessions_dd['id3_x'] != merged_sessions_dd['id3_y']]\n# Count how many times each pair appears and compute the final small result\nco_occurrence_counts = co_occurrence_dd.groupby(['id3_x', 'id3_y']).size().compute().reset_index(name='count')\nprint(\"Co-occurrence calculated successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:18:38.776722Z","iopub.execute_input":"2025-07-20T14:18:38.777450Z","iopub.status.idle":"2025-07-20T14:20:09.798402Z","shell.execute_reply.started":"2025-07-20T14:18:38.777419Z","shell.execute_reply":"2025-07-20T14:20:09.797337Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n","output_type":"stream"},{"name":"stderr","text":"2025-07-20 14:18:58.503146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753021138.720674     210 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753021138.784423     210 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Co-Click Features ---\nStep 1a: Creating customer sessions on a sample of the data...\nSorting events by customer and time...\nCustomer sessions created.\nStep 1b: Calculating offer co-occurrence within sessions using Dask...\nCo-occurrence calculated successfully.\nStep 1c: Creating final co-click features...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Projection' object has no attribute 'notna'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;31m# (Making sure to convert to/from Expr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"{err}\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Projection' object has no attribute 'notna'\n\nThis often means that you are attempting to use an unsupported API function. Current API coverage is documented here: https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_210/3417018196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 1c: Creating final co-click features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Calculate CTR more efficiently using Dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mevent_dd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clicked'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_dd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0moffer_ctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_dd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clicked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# Raise original error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;31m# Prioritize `FrameBase` attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'notna'"],"ename":"AttributeError","evalue":"'Series' object has no attribute 'notna'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# --- Step 1c: Create Co-Click Features ---\nprint(\"Step 1c: Creating final co-click features...\")\n# MODIFIED: Changed .notna() to the Dask-compatible ~.isnull() to fix the AttributeError\nevent_dd['clicked'] = (~event_dd['id7'].isnull()).astype(int)\noffer_ctr = event_dd.groupby('id3')['clicked'].mean().compute().to_dict()\n\n# For each offer, find its most frequent partner\nco_occurrence_counts = co_occurrence_counts.sort_values('count', ascending=False)\nmost_frequent_partner = co_occurrence_counts.drop_duplicates(subset=['id3_x'], keep='first')\n\n# Get the CTR of that most frequent partner\nmost_frequent_partner['partner_ctr'] = most_frequent_partner['id3_y'].map(offer_ctr)\nco_click_features = most_frequent_partner[['id3_x', 'partner_ctr']].rename(columns={'id3_x': 'id3', 'partner_ctr': 'most_frequent_partner_ctr'})\nprint(\"Co-click features created successfully.\")\n\n# Clean up memory\ndel event_dd, event_dd_sample, co_occurrence_counts\ngc.collect()\n\n# =============================================================================\n# TECHNIQUE 2: TEXT EMBEDDINGS\n# =============================================================================\nprint(\"\\n--- Technique 2: Creating Text Embedding Features ---\")\n\n# --- Step 2a: Prepare the Text Corpus ---\nprint(\"Step 2a: Preparing text corpus from offer metadata...\")\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\n# Combine text fields, handling missing values\noffer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\noffer_texts = offer_meta_df[['id3', 'full_text']].copy()\nprint(\"Text corpus prepared.\")\n\n# --- Step 2b: Generate Embeddings ---\nprint(\"Step 2b: Generating text embeddings (this may take a moment)...\")\n# Use a lightweight but powerful pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n# Generate the embeddings\nembeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\nprint(f\"Embeddings generated with shape: {embeddings.shape}\")\n\n# Create a DataFrame from the embeddings\nembedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\ntext_embedding_features = pd.concat([offer_texts[['id3']], embedding_df], axis=1)\nprint(\"Text embedding features created successfully.\")\n\n\n# =============================================================================\n# STAGE 3: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v10) ---\")\n\ndef enrich_dataframe(df, co_click_features, text_embedding_features):\n    \"\"\"A reusable function to enrich a dataframe with new features.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    co_click_features['id3'] = co_click_features['id3'].astype(str)\n    text_embedding_features['id3'] = text_embedding_features['id3'].astype(str)\n    \n    # Merge co-click features\n    df = pd.merge(df, co_click_features, on='id3', how='left')\n    # Merge text embedding features\n    df = pd.merge(df, text_embedding_features, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, co_click_features, text_embedding_features)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_dataframe(df_test, co_click_features, text_embedding_features)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v16.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v16.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:21:41.906874Z","iopub.execute_input":"2025-07-20T14:21:41.907577Z","iopub.status.idle":"2025-07-20T14:23:08.914152Z","shell.execute_reply.started":"2025-07-20T14:21:41.907543Z","shell.execute_reply":"2025-07-20T14:23:08.913210Z"}},"outputs":[{"name":"stdout","text":"Step 1c: Creating final co-click features...\nCo-click features created successfully.\n\n--- Technique 2: Creating Text Embedding Features ---\nStep 2a: Preparing text corpus from offer metadata...\nText corpus prepared.\nStep 2b: Generating text embeddings (this may take a moment)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c99345d5edd45d0849689f8d080d400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f02e3597f034c7b96798b71cd9a8fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99cf9fc10e064bcf9b7fbd78fb704dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfe314a4bd746ba828d0c80e22f1a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c92f10699a94bdfa8f2c7b9e13d4d3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9faa12e7661e4c6dadcfbcfc104df782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34470221253e4a64b3b31c5caceff72a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f00e5c94f549878c6b09751409298a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb51d0789aa6474aab19b600cce7e7a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3de03cd09d4132bb3797985dcaecab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175fd54f03184c03b5c77779f7f91646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9caa6d6dcca4b9eb93c2ec6f9affa82"}},"metadata":{}},{"name":"stdout","text":"Embeddings generated with shape: (4164, 384)\nText embedding features created successfully.\n\n--- STAGE 3: Merging New Features and Saving Final Datasets (v10) ---\nEnriching training data...\nEnriching test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v16.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v16.parquet\n\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df_train_final.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:24:10.055774Z","iopub.execute_input":"2025-07-20T14:24:10.056182Z","iopub.status.idle":"2025-07-20T14:24:10.061416Z","shell.execute_reply.started":"2025-07-20T14:24:10.056156Z","shell.execute_reply":"2025-07-20T14:24:10.060790Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(770164, 788)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import gc\n\n# List of large DataFrames and variables created in the previous script\nvariables_to_delete = [\n    'df_train', 'df_test', 'offer_meta_df', 'event_dd', \n    'event_dd_sample', 'session_offers', 'merged_sessions', \n    'co_occurrence', 'co_occurrence_counts', 'event_df_pd', \n    'offer_ctr', 'most_frequent_partner', 'co_click_features',\n    'offer_texts', 'embeddings', 'embedding_df', \n    'text_embedding_features', 'df_train_final', 'df_test_final'\n]\n\nprint(\"--- Clearing RAM ---\")\nfor var_name in variables_to_delete:\n    # Check if the variable exists in the notebook's memory before trying to delete it\n    if var_name in globals():\n        print(f\"Deleting: {var_name}\")\n        del globals()[var_name]\n\n# Call the garbage collector to release the memory\ngc.collect()\nprint(\"RAM cleared successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T14:40:29.465627Z","iopub.execute_input":"2025-07-20T14:40:29.466299Z","iopub.status.idle":"2025-07-20T14:40:32.494368Z","shell.execute_reply.started":"2025-07-20T14:40:29.466264Z","shell.execute_reply":"2025-07-20T14:40:32.493537Z"}},"outputs":[{"name":"stdout","text":"--- Clearing RAM ---\nDeleting: df_train\nDeleting: df_test\nDeleting: offer_meta_df\nDeleting: offer_ctr\nDeleting: most_frequent_partner\nDeleting: co_click_features\nDeleting: offer_texts\nDeleting: embeddings\nDeleting: df_train_final\nDeleting: df_test_final\nRAM cleared successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary library for text embeddings\n!pip install -q sentence-transformers\n\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load your latest training and test sets\n    df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n    df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n    # Load supplementary data for feature creation\n    offer_meta_df = pd.read_parquet(f'{INPUT_DIR}offer_metadata.parquet')\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet')\n    \n    print(\"Successfully loaded all required data.\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# TECHNIQUE 1: ADVANCED CO-CLICK FEATURES\n# =============================================================================\nprint(\"\\n--- Technique 1: Creating Advanced Co-Click Features ---\")\n\n# --- Step 1a: Create Customer Sessions on a Sample ---\nprint(\"Step 1a: Creating customer sessions on a sample of the data...\")\nevent_dd_sample = event_dd.sample(frac=0.2, random_state=42)\nevent_dd_sample['impression_time'] = dd.to_datetime(event_dd_sample['id4'], errors='coerce')\nevent_dd_sample = event_dd_sample.sort_values(['id2', 'impression_time'])\n\ndef calculate_time_diff(df):\n    df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n    return df\n\nmeta = event_dd_sample._meta.copy()\nmeta['time_diff'] = pd.Series(dtype='float64')\nevent_dd_sample = event_dd_sample.map_partitions(calculate_time_diff, meta=meta)\n\nevent_dd_sample['new_session'] = (event_dd_sample['time_diff'] > 1800).fillna(True)\nevent_dd_sample['session_id'] = event_dd_sample.groupby('id2')['new_session'].cumsum()\nprint(\"Customer sessions created.\")\n\n\n# --- Step 1b: Calculate Co-occurrence ---\nprint(\"Step 1b: Calculating offer co-occurrence within sessions...\")\nsession_offers_dd = event_dd_sample[['session_id', 'id2', 'id3']]\nmerged_sessions_dd = dd.merge(session_offers_dd, session_offers_dd, on=['session_id', 'id2'])\nco_occurrence_dd = merged_sessions_dd[merged_sessions_dd['id3_x'] != merged_sessions_dd['id3_y']]\nco_occurrence_counts = co_occurrence_dd.groupby(['id3_x', 'id3_y']).size().compute().reset_index(name='count')\nprint(\"Co-occurrence calculated successfully.\")\n\n\n# --- Step 1c: Create Advanced Co-Click Features ---\nprint(\"Step 1c: Creating final co-click features...\")\nevent_dd['clicked'] = (~event_dd['id7'].isnull()).astype(int)\noffer_ctr = event_dd.groupby('id3')['clicked'].mean().compute().to_dict()\n\n# Get the CTR of each partner offer\nco_occurrence_counts['partner_ctr'] = co_occurrence_counts['id3_y'].map(offer_ctr)\nco_occurrence_counts = co_occurrence_counts.sort_values(['id3_x', 'count'], ascending=[True, False])\n\n# Find the top 5 partners for each offer\ntop_5_partners = co_occurrence_counts.groupby('id3_x').head(5)\n\n# Calculate aggregate stats for the top 5 partners' CTRs\nco_click_agg = top_5_partners.groupby('id3_x')['partner_ctr'].agg(['mean', 'max', 'std']).reset_index()\nco_click_features = co_click_agg.rename(columns={\n    'id3_x': 'id3',\n    'mean': 'top5_partner_mean_ctr',\n    'max': 'top5_partner_max_ctr',\n    'std': 'top5_partner_std_ctr'\n})\nprint(\"Advanced co-click features created successfully.\")\n\n# Clean up memory\ndel event_dd, event_dd_sample, co_occurrence_counts\ngc.collect()\n\n# =============================================================================\n# TECHNIQUE 2: TEXT SIMILARITY FEATURES\n# =============================================================================\nprint(\"\\n--- Technique 2: Creating Text Similarity Features ---\")\n\n# --- Step 2a: Prepare the Text Corpus and Generate Embeddings ---\nprint(\"Step 2a: Preparing text corpus and generating embeddings...\")\noffer_meta_df['id3'] = offer_meta_df['id3'].astype(str)\noffer_meta_df['full_text'] = offer_meta_df['id9'].fillna('') + ' ' + offer_meta_df['f378'].fillna('')\noffer_texts = offer_meta_df[['id3', 'id8', 'full_text']].copy()\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(offer_texts['full_text'].tolist(), show_progress_bar=True)\nembedding_df = pd.DataFrame(embeddings, columns=[f'text_emb_{i}' for i in range(embeddings.shape[1])])\noffer_embeddings_df = pd.concat([offer_texts[['id3', 'id8']], embedding_df], axis=1)\nprint(\"Text embeddings generated.\")\n\n# --- Step 2b: Create Industry DNA and Calculate Similarity ---\nprint(\"Step 2b: Calculating offer-industry text similarity...\")\n# Calculate the average embedding vector for each industry\nindustry_dna = offer_embeddings_df.groupby('id8').mean(numeric_only=True).reset_index()\nindustry_dna = industry_dna.rename(columns={col: f'industry_{col}' for col in industry_dna.columns if 'emb' in col})\n\n# Merge the industry DNA back to each offer\noffer_embeddings_df = pd.merge(offer_embeddings_df, industry_dna, on='id8', how='left')\n\n# Calculate the cosine similarity between each offer's embedding and its industry's average embedding\nemb_cols = [f'text_emb_{i}' for i in range(embeddings.shape[1])]\nindustry_emb_cols = [f'industry_text_emb_{i}' for i in range(embeddings.shape[1])]\n\n# This is a memory-intensive step, so we process in chunks if needed\n# For now, we attempt it directly as the dataframe is moderately sized\noffer_embeddings_df['offer_industry_text_similarity'] = [\n    cosine_similarity(row[emb_cols].values.reshape(1, -1), row[industry_emb_cols].values.reshape(1, -1))[0][0]\n    if not row[industry_emb_cols].isnull().any() else np.nan\n    for index, row in offer_embeddings_df.iterrows()\n]\ntext_similarity_features = offer_embeddings_df[['id3', 'offer_industry_text_similarity']]\nprint(\"Text similarity features created successfully.\")\n\n\n# =============================================================================\n# STAGE 3: MERGE NEW FEATURES AND SAVE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Merging New Features and Saving Final Datasets (v17) ---\")\n\ndef enrich_dataframe(df, co_click_features, text_similarity_features):\n    \"\"\"A reusable function to enrich a dataframe with new features.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    co_click_features['id3'] = co_click_features['id3'].astype(str)\n    text_similarity_features['id3'] = text_similarity_features['id3'].astype(str)\n    \n    # Merge co-click features\n    df = pd.merge(df, co_click_features, on='id3', how='left')\n    # Merge text similarity features\n    df = pd.merge(df, text_similarity_features, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, co_click_features, text_similarity_features)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_dataframe(df_test, co_click_features, text_similarity_features)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v18.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v18.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:47:27.253980Z","iopub.execute_input":"2025-07-20T16:47:27.254571Z","iopub.status.idle":"2025-07-20T16:52:11.888021Z","shell.execute_reply.started":"2025-07-20T16:47:27.254546Z","shell.execute_reply":"2025-07-20T16:52:11.887114Z"}},"outputs":[{"name":"stdout","text":"--- Section 0: Setting up the environment ---\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-07-20 16:48:57.174338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753030137.348404      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753030137.396031      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n--- Section 1: Loading Data ---\nSuccessfully loaded all required data.\n\n--- Technique 1: Creating Advanced Co-Click Features ---\nStep 1a: Creating customer sessions on a sample of the data...\nCustomer sessions created.\nStep 1b: Calculating offer co-occurrence within sessions...\nCo-occurrence calculated successfully.\nStep 1c: Creating final co-click features...\nAdvanced co-click features created successfully.\n\n--- Technique 2: Creating Text Similarity Features ---\nStep 2a: Preparing text corpus and generating embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c12cd61fb23048e0812954294808ad23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69ea86edd1e413da9130e43562785c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf60185ed8c0499cbc48931165ff6f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef11b224116a4e529a45baaa1e7615c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115642c847db42ebb45136bee283336a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"251465aa9e1c478fa2803e765cd415b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d7da5a658b49f1a900ef8feb9ef6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc6cb498e7b44509b9f367368ccbc2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98559d142ea04556b9725c4507fff6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fd83c5fdb44f7f9f57ddd5c8dff70e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4670b9e8a246d8ba19fa454939d959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80d066006ee44af8a781ac7877c28b6"}},"metadata":{}},{"name":"stdout","text":"Text embeddings generated.\nStep 2b: Calculating offer-industry text similarity...\nText similarity features created successfully.\n\n--- STAGE 3: Merging New Features and Saving Final Datasets (v17) ---\nEnriching training data...\nEnriching test data...\n\nSuccessfully saved final training data to: /kaggle/working/train_data_v18.parquet\nSuccessfully saved final test data to: /kaggle/working/test_data_v18.parquet\n\n--- ADVANCED FEATURE ENGINEERING COMPLETE ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df_train_final.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport gc\nimport os\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 0: DEFINE THE MAP@7 EVALUATION METRIC\n# =============================================================================\ndef map_at_k(y_true, y_pred, group_ids, k=7):\n    \"\"\"\n    Calculates the Mean Average Precision at k.\n    \n    Args:\n        y_true (array-like): The true relevance labels (0 or 1).\n        y_pred (array-like): The predicted ranking scores.\n        group_ids (array-like): The group/query IDs for each sample.\n        k (int): The cutoff for the precision calculation.\n        \n    Returns:\n        float: The Mean Average Precision @ k score.\n    \"\"\"\n    df = pd.DataFrame({'group': group_ids, 'y_true': y_true, 'y_pred': y_pred})\n    \n    average_precisions = []\n    for group in df['group'].unique():\n        group_df = df[df['group'] == group].sort_values('y_pred', ascending=False).reset_index(drop=True)\n        \n        # Get the top k predictions\n        group_df = group_df.head(k)\n        \n        if group_df['y_true'].sum() == 0:\n            average_precisions.append(0)\n            continue\n            \n        relevant_hits = 0\n        precision_at_i = []\n        for i, row in group_df.iterrows():\n            if row['y_true'] == 1:\n                relevant_hits += 1\n                precision_at_i.append(relevant_hits / (i + 1))\n        \n        if not precision_at_i:\n            average_precisions.append(0)\n        else:\n            average_precisions.append(np.mean(precision_at_i))\n            \n    return np.mean(average_precisions)\n\n\n# =============================================================================\n# STAGE 1: DATA LOADING AND PREPARATION FOR XGBRANKER\n# =============================================================================\nprint(\"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\")\ntry:\n    # Load your final, pre-cleaned training and test data\n    df = pd.read_parquet(f'{WORKING_DIR}train_data_v18.parquet')\n    df_test = pd.read_parquet(f'{WORKING_DIR}test_data_v18.parquet')\n    print(\"Successfully loaded final training (v92) and test (v92) data.\")\nexcept FileNotFoundError:\n    print(\"Error: v92 data not found. Please run the final cleaning script first.\")\n    exit()\n\n# --- Memory Reduction Function ---\ndef reduce_mem_usage(df):\n    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)[:3] == 'int' or str(col_type)[:5] == 'float':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n    return df\n\n# --- Prepare Feature and Target Names ---\ntarget_col = 'y'\n# All columns are features except for identifiers and the target\nfeature_cols = [col for col in df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'id8']]\n# Identify all categorical features to be one-hot encoded\ncategorical_features = [col for col in df.columns if 'cluster' in col or col == 'offer_type_code' or col == 'offer_type_x_weekend']\n\n# --- Preprocessing ---\nprint(\"Preprocessing data for training...\")\n# Handle one-hot encoding for categorical features\ndf = pd.get_dummies(df, columns=categorical_features, dummy_na=True)\ndf_test = pd.get_dummies(df_test, columns=categorical_features, dummy_na=True)\n\n# Update feature list after one-hot encoding\nfor cat_col in categorical_features:\n    if cat_col in feature_cols:\n        feature_cols.remove(cat_col)\ndummy_cols = [col for col in df.columns if any(cat_col in col for cat_col in categorical_features)]\nfeature_cols.extend(dummy_cols)\n# Remove any duplicates that might have been added\nfeature_cols = list(dict.fromkeys(feature_cols)) \n\n# Convert all feature columns to numeric before imputation\nprint(\"Converting all feature columns to numeric types...\")\nfor col in feature_cols:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    if col in df_test.columns:\n        df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n\n# Final imputation for training data\ndf[feature_cols] = df[feature_cols].fillna(-999)\n\n# MODIFIED: Move memory reduction to AFTER all preprocessing\nprint(\"\\nOptimizing memory usage for training data...\")\ndf = reduce_mem_usage(df)\nprint(\"\\nOptimizing memory usage for test data...\")\ndf_test = reduce_mem_usage(df_test)\n\n# --- Create Groups for XGBRanker ---\n# A \"query\" or \"group\" is a single ranking task (all offers for one customer on one day)\ndf['group_id'] = df['id2'].astype(str) + '_' + df['id5'].astype(str)\ndf = df.sort_values('group_id').reset_index(drop=True)\ngroup_sizes = df.groupby('group_id')['id1'].count().to_numpy()\n\n# Define features (X) and target (y)\nX = df[feature_cols]\ny = df[target_col].astype(int)\nprint(f\"Prepared training data with {len(feature_cols)} features and {len(group_sizes)} groups.\")\n\n\n# =============================================================================\n# STAGE 2: TRAINING FINAL ENSEMBLE WITH XGBRANKER\n# =============================================================================\nprint(\"\\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\")\n\n# Define XGBRanker parameters. Note the 'rank:ndcg' objective.\nfinal_params = {\n    'objective': 'rank:ndcg', # Learning-to-rank objective\n    'eval_metric': 'ndcg@7',  # Evaluate using a metric similar to the competition\n    'use_label_encoder': False, 'seed': 42,\n    'tree_method': 'gpu_hist', 'gpu_id': 0,\n    'n_estimators': 2000, \n    'learning_rate': 0.05, \n    'max_depth': 8, \n    'subsample': 0.8, \n    'colsample_bytree': 0.8,\n    'max_bin': 128 # MODIFIED: Add max_bin to reduce memory usage during training\n}\n\nN_SPLITS = 3 # Using 5 folds for a balance of stability and speed\ngkf_train = GroupKFold(n_splits=N_SPLITS)\noof_predictions = np.zeros(len(df))\noof_map_scores = []\n\nprint(f\"Starting final training with {N_SPLITS}-Fold GroupKFold on GPU...\")\n# For XGBRanker, we must use the customer ID for the GroupKFold split\ngroups_for_split = df['id2']\n\nfor fold, (train_idx, val_idx) in enumerate(gkf_train.split(X, y, groups=groups_for_split)):\n    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n    \n    X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n    X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # Get the group sizes for the training and validation sets\n    train_groups = df.iloc[train_idx].groupby('group_id')['id1'].count().to_numpy()\n    val_groups = df.iloc[val_idx].groupby('group_id')['id1'].count().to_numpy()\n    \n    model = xgb.XGBRanker(**final_params)\n    model.fit(X_train_fold, y_train_fold, group=train_groups,\n              eval_set=[(X_val_fold, y_val_fold)], eval_group=[val_groups],\n              early_stopping_rounds=50, verbose=False)\n    \n    # XGBRanker outputs scores, not probabilities\n    val_preds = model.predict(X_val_fold)\n    oof_predictions[val_idx] = val_preds\n\n    # --- Calculate and store MAP@7 for this fold ---\n    fold_map_score = map_at_k(y_val_fold, val_preds, df.iloc[val_idx]['group_id'], k=7)\n    oof_map_scores.append(fold_map_score)\n    print(f\"Fold {fold+1} MAP@7 Score: {fold_map_score:.5f}\")\n\n    model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold+1}.json'\n    model.save_model(model_path)\n    print(f\"Model for fold {fold+1} saved to {model_path}\")\n    del X_train_fold, y_train_fold, X_val_fold, y_val_fold, model\n    gc.collect()\n\n# --- Evaluate Overall Performance ---\nprint(f\"\\n--- Overall Cross-Validation Results ---\")\nprint(f\"Mean Out-of-Fold (OOF) MAP@7 Score: {np.mean(oof_map_scores):.5f}\")\nprint(\"This score is the most reliable estimate of your final leaderboard performance.\")\n\n\n# =============================================================================\n# STAGE 3: PREDICTION AND SUBMISSION FILE CREATION\n# =============================================================================\nprint(\"\\n--- STAGE 3: Generating Final Predictions ---\")\n# Prepare test data\nsubmission_ids = df_test[['id1', 'id2', 'id3', 'id5']].copy()\n# Align test columns with the training columns\nX_test = df_test.reindex(columns=X.columns, fill_value=0)\n# Final imputation for test data\nX_test = X_test.fillna(-999)\n\n# Load models and predict\ntest_predictions = np.zeros(len(X_test))\nfor fold in range(1, N_SPLITS + 1):\n    print(f\"Predicting with Fold {fold}/{N_SPLITS}...\")\n    model_path = f'{WORKING_DIR}final_ranker_model_fold_{fold}.json'\n    model = xgb.XGBRanker()\n    model.load_model(model_path)\n    # Predict outputs ranking scores\n    test_predictions += model.predict(X_test) / N_SPLITS\n\n# Create submission file\nprint(\"\\nCreating submission file...\")\nsubmission_df = submission_ids.copy()\nsubmission_df['id5'] = pd.to_datetime(submission_df['id5'], errors='coerce').dt.strftime('%m-%d-%Y')\ncleaned_predictions = np.nan_to_num(test_predictions, nan=-999) # Use a low score for NaNs\nsubmission_df['pred'] = cleaned_predictions\n\nsubmission_path = f'{WORKING_DIR}r2_submission_final_ranker18.csv'\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"\\nSubmission file successfully saved to: {submission_path}\")\nprint(\"\\nFirst 5 rows of the submission file:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:52:11.889540Z","iopub.execute_input":"2025-07-20T16:52:11.889919Z","iopub.status.idle":"2025-07-20T17:18:35.917508Z","shell.execute_reply.started":"2025-07-20T16:52:11.889895Z","shell.execute_reply":"2025-07-20T17:18:35.915585Z"}},"outputs":[{"name":"stdout","text":"--- STAGE 1: Loading and Preparing Data for XGBRanker ---\nSuccessfully loaded final training (v92) and test (v92) data.\nPreprocessing data for training...\nConverting all feature columns to numeric types...\n\nOptimizing memory usage for training data...\nMemory usage of dataframe is 2367.98 MB\nMemory usage after optimization is: 1217.04 MB\nDecreased by 48.6%\n\nOptimizing memory usage for test data...\nMemory usage of dataframe is 1132.30 MB\nMemory usage after optimization is: 600.84 MB\nDecreased by 46.9%\nPrepared training data with 439 features and 52468 groups.\n\n--- STAGE 2: Training Final Ensemble of XGBRanker Models ---\nStarting final training with 3-Fold GroupKFold on GPU...\n--- Fold 1/3 ---\nFold 1 MAP@7 Score: 0.06282\nModel for fold 1 saved to /kaggle/working/final_ranker_model_fold_1.json\n--- Fold 2/3 ---\nFold 2 MAP@7 Score: 0.06610\nModel for fold 2 saved to /kaggle/working/final_ranker_model_fold_2.json\n--- Fold 3/3 ---\nFold 3 MAP@7 Score: 0.06171\nModel for fold 3 saved to /kaggle/working/final_ranker_model_fold_3.json\n\n--- Overall Cross-Validation Results ---\nMean Out-of-Fold (OOF) MAP@7 Score: 0.06354\nThis score is the most reliable estimate of your final leaderboard performance.\n\n--- STAGE 3: Generating Final Predictions ---\nPredicting with Fold 1/3...\nPredicting with Fold 2/3...\nPredicting with Fold 3/3...\n\nCreating submission file...\n\nSubmission file successfully saved to: /kaggle/working/r2_submission_final_ranker18.csv\n\nFirst 5 rows of the submission file:\n                                               id1      id2     id3  \\\n0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n\n          id5      pred  \n0  11-04-2023 -1.411453  \n1  11-04-2023 -1.487017  \n2  11-05-2023  0.144096  \n3  11-04-2023 -2.434480  \n4  11-05-2023 -2.545596  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD THE SUBMISSION FILE\n# =============================================================================\nprint(\"--- STAGE 1: Loading the submission file ---\")\ntry:\n    submission_path = f'{WORKING_DIR}r2_submission_final_ranker18.csv'\n    df_sub = pd.read_csv(submission_path)\n    print(\"Successfully loaded submission file.\")\n    print(\"\\nOriginal 'pred' column stats:\")\n    print(df_sub['pred'].describe())\nexcept FileNotFoundError:\n    print(f\"Error: {submission_path} not found.\")\n    print(\"Please ensure you have run the prediction script successfully.\")\n    exit()\n\n# =============================================================================\n# STAGE 2: NORMALIZE THE PREDICTION COLUMN\n# =============================================================================\nprint(\"\\n--- STAGE 2: Normalizing the 'pred' column ---\")\n\n# Initialize the Min-Max Scaler to scale values between 0 and 1\nscaler = MinMaxScaler()\n\n# The scaler expects a 2D array, so we need to reshape the column\n# We fit and transform in one step\ndf_sub['pred_normalized'] = scaler.fit_transform(df_sub[['pred']])\n\nprint(\"\\n'pred' column has been normalized.\")\nprint(\"\\nNew 'pred_normalized' column stats:\")\nprint(df_sub['pred_normalized'].describe())\n\n\n# =============================================================================\n# STAGE 3: CREATE AND SAVE THE NORMALIZED SUBMISSION FILE\n# =============================================================================\nprint(\"\\n--- STAGE 3: Saving the normalized submission file ---\")\n\n# Create the final dataframe, dropping the old prediction column\ndf_final_sub = df_sub[['id1', 'id2', 'id3', 'id5']].copy()\ndf_final_sub['pred'] = df_sub['pred_normalized']\n\n# Define the output path\noutput_path = f'{WORKING_DIR}r2_submission_final_ranker_normalized18.csv'\n\ntry:\n    df_final_sub.to_csv(output_path, index=False)\n    print(f\"\\nSuccessfully saved normalized submission file to: {output_path}\")\n    print(\"\\nFirst 5 rows of the new submission file:\")\n    print(df_final_sub.head())\nexcept Exception as e:\n    print(f\"\\nError saving the file: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# SECTION 0: SETUP AND IMPORTS\n# =============================================================================\nprint(\"--- Section 0: Setting up the environment ---\")\n# Install the necessary libraries for graph and sequential embeddings\n!pip install -q node2vec gensim\n\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport warnings\nimport gc\nimport os\nimport networkx as nx\nfrom node2vec import Node2Vec\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Define Kaggle File Paths ---\nINPUT_DIR = '/kaggle/input/amexkkgp/'\nWORKING_DIR = '/kaggle/working/'\n\n# =============================================================================\n# STAGE 1: LOAD DATA\n# =============================================================================\nprint(\"\\n--- Section 1: Loading Data ---\")\ntry:\n    # Load your latest training and test sets\n    df_train = pd.read_parquet(f'/kaggle/input/amex-v15/train_data_v15.parquet')\n    df_test = pd.read_parquet(f'/kaggle/input/amex-v15/test_data_v15.parquet')\n    \n    # Use Dask for the large event log\n    event_dd = dd.read_parquet(f'{INPUT_DIR}add_event.parquet', columns=['id2', 'id3', 'id4'])\n    \n    print(\"Successfully loaded all required data.\")\nexcept FileNotFoundError:\n    print(\"Error: Required data not found. Please ensure all necessary files are in the correct directories.\")\n    exit()\n\n# =============================================================================\n# TECHNIQUE 1: GRAPH EMBEDDINGS (NODEVEC) WITH PCA\n# =============================================================================\nprint(\"\\n--- Technique 1: Creating Graph Embedding Features ---\")\n\n# --- Step 1a: Build the Interaction Graph on a Sample ---\nprint(\"Step 1a: Building the customer-offer interaction graph on a sample...\")\n# Use a large sample of the event data to make the process much faster\nevent_dd_sample = event_dd.sample(frac=0.3, random_state=42)\nedge_list = event_dd_sample[['id2', 'id3']].compute()\n# Create a graph from the edge list\nG = nx.from_pandas_edgelist(edge_list, 'id2', 'id3')\nprint(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n\n# --- Step 1b: Train the Node2Vec Model ---\nprint(\"Step 1b: Training the Node2Vec model...\")\nnode2vec = Node2Vec(G, dimensions=32, walk_length=20, num_walks=10, workers=4, quiet=True)\n# min_count=5 acts as a form of pruning for robustness.\nmodel_n2v = node2vec.fit(window=5, min_count=5, batch_words=4)\nprint(\"Node2Vec model trained successfully.\")\n\n# --- Step 1c: Extract Offer Embeddings ---\nprint(\"Step 1c: Extracting offer embeddings...\")\noffer_ids = df_train['id3'].unique().tolist() + df_test['id3'].unique().tolist()\noffer_ids = list(set(offer_ids)) # Get unique offer IDs across train and test\n\ngraph_embeddings = []\nfor offer_id in offer_ids:\n    try:\n        vector = model_n2v.wv[offer_id]\n        graph_embeddings.append([offer_id] + vector.tolist())\n    except KeyError:\n        continue\n\ngraph_embedding_features = pd.DataFrame(graph_embeddings, columns=['id3'] + [f'graph_emb_{i}' for i in range(32)])\nprint(\"Graph embedding features created successfully.\")\n\n# Clean up memory\ndel G, edge_list, model_n2v, node2vec, event_dd_sample\ngc.collect()\n\n# =============================================================================\n# TECHNIQUE 2: SEQUENTIAL EMBEDDINGS (WORD2VEC) WITH PCA\n# =============================================================================\nprint(\"\\n--- Technique 2: Creating Sequential Embedding Features ---\")\n\n# --- Step 2a: Create Customer Sessions ---\nprint(\"Step 2a: Creating customer sessions...\")\nevent_dd['impression_time'] = dd.to_datetime(event_dd['id4'], errors='coerce')\n\n# MODIFIED: Use a more robust Dask method to create sessions by sorting first.\nprint(\"Sorting events by customer and time...\")\nevent_dd = event_dd.sort_values(['id2', 'impression_time'])\n\n# Define a function to calculate time differences within partitions\ndef calculate_time_diff(df):\n    df['time_diff'] = df.groupby('id2')['impression_time'].diff().dt.total_seconds()\n    return df\n\n# Apply the function to each partition using map_partitions\nmeta = event_dd._meta.copy()\nmeta['time_diff'] = pd.Series(dtype='float64')\nevent_dd = event_dd.map_partitions(calculate_time_diff, meta=meta)\n\n# A new session starts if the time difference is > 30 minutes (1800 seconds)\nevent_dd['new_session'] = (event_dd['time_diff'] > 1800).fillna(True)\nevent_dd['session_id'] = event_dd.groupby('id2')['new_session'].cumsum()\n\n# Create the list of sessions (sequences of offers)\nprint(\"Aggregating offers into sessions...\")\nsessions = event_dd.groupby(['id2', 'session_id'])['id3'].apply(list, meta=('id3', 'object')).compute().tolist()\nprint(f\"Created {len(sessions)} customer sessions.\")\n\n# --- Step 2b: Train the Word2Vec Model ---\nprint(\"Step 2b: Training the Word2Vec model...\")\n# Train the model on the sessions. min_count=5 acts as pruning.\nmodel_w2v = Word2Vec(sentences=sessions, vector_size=32, window=5, min_count=5, workers=4)\nprint(\"Word2Vec model trained successfully.\")\n\n# --- Step 2c: Extract Offer Embeddings ---\nprint(\"Step 2c: Extracting sequential embeddings...\")\nsequential_embeddings = []\nfor offer_id in offer_ids:\n    try:\n        vector = model_w2v.wv[offer_id]\n        sequential_embeddings.append([offer_id] + vector.tolist())\n    except KeyError:\n        continue\n\nsequential_embedding_features = pd.DataFrame(sequential_embeddings, columns=['id3'] + [f'seq_emb_{i}' for i in range(32)])\nprint(\"Sequential embedding features created successfully.\")\n\n# Clean up memory\ndel sessions, model_w2v, event_dd\ngc.collect()\n\n# =============================================================================\n# STAGE 3: APPLY PCA AND MERGE FEATURES\n# =============================================================================\nprint(\"\\n--- STAGE 3: Applying PCA and Merging Final Features ---\")\n\ndef apply_pca_to_embeddings(df_features, prefix):\n    \"\"\"Applies PCA to a dataframe of embedding features.\"\"\"\n    ids = df_features[['id3']]\n    embeddings = df_features.drop(columns=['id3'])\n    \n    # Scale the data before PCA\n    scaler = StandardScaler()\n    embeddings_scaled = scaler.fit_transform(embeddings)\n    \n    # Use PCA to capture 95% of the variance\n    pca = PCA(n_components=0.95)\n    embeddings_pca = pca.fit_transform(embeddings_scaled)\n    print(f\"PCA for '{prefix}' completed. New shape: {embeddings_pca.shape}\")\n    \n    # Create a DataFrame from the PCA components\n    pca_df = pd.DataFrame(embeddings_pca, columns=[f'{prefix}_pca_{i}' for i in range(embeddings_pca.shape[1])])\n    return pd.concat([ids, pca_df], axis=1)\n\n# --- Apply PCA to both sets of embeddings ---\ngraph_pca_features = apply_pca_to_embeddings(graph_embedding_features, 'graph')\nsequential_pca_features = apply_pca_to_embeddings(sequential_embedding_features, 'seq')\n\ndef enrich_dataframe(df, graph_pca_features, sequential_pca_features):\n    \"\"\"A reusable function to enrich a dataframe with new PCA features.\"\"\"\n    df['id3'] = df['id3'].astype(str)\n    graph_pca_features['id3'] = graph_pca_features['id3'].astype(str)\n    sequential_pca_features['id3'] = sequential_pca_features['id3'].astype(str)\n    \n    # Merge graph PCA features\n    df = pd.merge(df, graph_pca_features, on='id3', how='left')\n    # Merge sequential PCA features\n    df = pd.merge(df, sequential_pca_features, on='id3', how='left')\n    \n    return df\n\n# --- Enrich both the training and test dataframes ---\nprint(\"Enriching training data...\")\ndf_train_final = enrich_dataframe(df_train, graph_pca_features, sequential_pca_features)\nprint(\"Enriching test data...\")\ndf_test_final = enrich_dataframe(df_test, graph_pca_features, sequential_pca_features)\n\n# --- Save the final DataFrames ---\ntry:\n    train_output_path = f'{WORKING_DIR}train_data_v18.parquet'\n    test_output_path = f'{WORKING_DIR}test_data_v18.parquet'\n    \n    df_train_final.to_parquet(train_output_path)\n    df_test_final.to_parquet(test_output_path)\n    \n    print(f\"\\nSuccessfully saved final training data to: {train_output_path}\")\n    print(f\"Successfully saved final test data to: {test_output_path}\")\nexcept Exception as e:\n    print(f\"\\nError saving the final files: {e}\")\n\nprint(\"\\n--- ADVANCED EMBEDDING FEATURE ENGINEERING COMPLETE ---\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-20T15:58:44.885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}